\documentclass[12pt]{book}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{hyperref}

\usepackage{color}
\newcommand{\taylor}[1]{{\color{blue} \sf $\spadesuit\spadesuit\spadesuit$ Taylor: [#1]}}
\newcommand{\todo}[1]{{\color{purple} \sf $\spadesuit\spadesuit\spadesuit$ TODO: [#1]}}

\usepackage{enumitem}


\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{graphicx}
\usepackage{soul}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{warning}[theorem]{Warning}




\newcommand{\trdeg}{\operatorname{trdeg}}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}

\renewcommand{\AA}{\mathbb{A}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\Ocal}{\mathcal{O}}

%\newcommand{\sec}{\operatorname{sec}}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Hom}{\operatorname{Hom}}

\newcommand{\LM}{\operatorname{LM}}
\newcommand{\LT}{\operatorname{LT}}
\newcommand{\LC}{\operatorname{LC}}
\newcommand{\Low}{\operatorname{Low}}

\newcommand{\wt}{\operatorname{wt}}
\newcommand{\hol}{\operatorname{Hol}}
\newcommand{\Mer}{\operatorname{Mer}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\llangle}{\langle \langle}
\newcommand{\rrangle}{\rangle \rangle}

\newcommand{\mult}{\operatorname{mult}}
\newcommand{\Res}{\operatorname{Res}}
\newcommand{\res}{\operatorname{res}}

\newcommand{\ord}{\operatorname{ord}}


%\newcommand{\hol}{\operatorname{hol}}
\newcommand{\LocSys}{\operatorname{LocSys}}
\newcommand{\Mod}{\mathsf{Mod}}
\newcommand{\Pic}{\operatorname{Pic}}

\newcommand{\fin}{\operatorname{fin}}

\newcommand{\tr}{\operatorname{tr}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\Mon}{\operatorname{Mon}}
\newcommand{\Sys}{\operatorname{Sys}}
\newcommand{\Conn}{\operatorname{Conn}}
\newcommand{\Repn}{\operatorname{Repn}}
\newcommand{\End}{\operatorname{End}}

\newcommand{\QQbar}{\overline{\mathbb{Q}}}
\newcommand{\Cont}{\operatorname{Cont}}
\newcommand{\Lie}{\operatorname{Lie}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\can}{\operatorname{can}}
\newcommand{\Clf}{\operatorname{Clf}}
\newcommand{\pr}{\operatorname{pr}}



\newcommand{\bra}[1]{\langle #1 \vert }
\newcommand{\ket}[1]{\vert #1 \rangle}
\newcommand{\Jac}{\operatorname{Jac}}
\newcommand{\Char}{\operatorname{Char}}
\newcommand{\an}{\operatorname{an}}
\newcommand{\ldeg}{\operatorname{ldeg}}
\newcommand{\red}{\operatorname{red}}
\newcommand{\rk}{\operatorname{rk}}
\newcommand{\Khat}{\widehat{K}}
\newcommand{\Fbar}{\overline{F}}
\newcommand{\PGL}{\operatorname{PGL}}
\newcommand{\deligne}[1]{\cite[ #1 ,\href{https://labs.thosgood.com/translations/978-3-540-05190-9.pdf}{Hosgood translation}]{Deligne1970}}



\usepackage{listings}
\usepackage{spverbatim}


\usepackage{tabto}
\def\quoteattr#1#2{\setbox0=\hbox{#2}#1\tabto{\dimexpr\linewidth-\wd0}\box0}
%\parskip 1em

\setlength\parskip{1em plus 0.1em minus 0.2em}
\setlength\parindent{0pt}

%opening
\title{An Introduction to the Algebraic Theory of Differential Equations}
\author{Taylor Dupuy }


\begin{document}

\maketitle

\frontmatter

\chapter{FrontMatter}

\section*{Why do these notes exist?}
These started from course notes for a course taught in Fall 2022 at University of Vermont entitled \emph{The Algebraic Theory of Differential Equations}. 
I decided to write these notes because there are a lot different sources for graduate students starting out in Differential Algebra but none of them cover exactly what I would like to cover. 

So what is the Algebraic Theory of Differential Equations? 
For me the starting place are Ritt's books on Differential Algebra. 
I love Ritt's books \cite{Ritt1932} and \cite{Ritt1950}. 
The issue is that they are a bit out of date and have a lot of dependencies within chapters that are not clearly marked so you almost have the read the books linearly. 
They are also missing a lot of the classical theory from the 1800's which motivated the subject. 
The successor to Ritt's books is Kolchin's books \cite{Kolchin1973} which, while mathematically very useful, invokes notation and terminology that gives me nightmares. 
Also, the algebraic geometry there largely ignores the development of scheme theory between 1950 and 1970 by the French school. 
An alternative to these two is Kaplansky's book \cite{Kaplansky1976} which I love but is perhaps too brief. 
We also have \cite{Buium1986} (influenced by Matsuda's book \cite{Matsuda1980}),\cite{Buium1992}, and \cite{Buium1994} which are probably the most influential on my perspective. 
They are about differential field theory, differential algebraic groups, and applications of differential algebra to diophantine geometry.
None of these books are perfect and there is a serious need for a consolidated resource in this subject.

To really understand those books (for example the Poincar\'e-Fuchs theorem on equations of the form $P(t,y,y')=0$ for a polynomial $P$ whose solutions have no movable singularities) one needs to go back to some of the older material which is best covered elsewhere. 

To cover this perspective one would like to talk about hypergeometric functions, the Painlev\'{e} equations, and monodromy more generally. 
For us, monodromy is going to be the starting point.
The main idea is that monodromy problems lead to the notion of classification (which gives hypergeometric functions) and isomonodromy problems (which gives Painlev\'e VI). 
For Painlev\'e there is the classical book of Ince \cite{Ince1944} and a standard text \cite{Iwasaki1991} which is nice but focuses a lot of computations and de-emphasizes global geometry.
For a rigorous treatment of the Riemann-Hilbert problem there are great discussion of Hilbert's 21st problem in \cite{Borel1987} and a more modern algebro geometric version in \cite{Deligne1970}.
Marrying this material with the field theoretic methods of \cite{Buium1986} is something that I want to do and there is some ``folklore theorems'' relating these two approaches that I would like to put in print.
In that spirit, we also want to marry Monodromy with the classical Galois Theory of Linear Differential Equations which goes by the name ``Picard-Vessiot Theory'' by Singer and van der Put \cite{Put2003} and Magid \cite{Magid1994}.


Once one gets into the Painlev\'{e} equations more algebraic geometry surfaces. The Japanese following  Okomoto \cite{Okamoto1987b,Okamoto1987,Okamoto1986, Okamoto1987a} (and many many papers which I'm not going to list following this thread) showed that there exist rational surfaces of ``spaces of initial conditions'' for the Painlev\'{e} which capture a lot of geometry. 
This really has to do with foliations and foliation theory which is also tied into this subject. 

Also, there are so-called Lax Pairs for these Painlev\'{e} equations which leads to a theory of ``algebraic complete integrability''. 
The notion of algebraic complete integrability is discussed in, say, \cite{Beauville1990}, \cite{Adler2004}, but for us what we would like to discuss more is the notion of isospectral deformations, its relationship to Soliton Theory and Algebraic Geometry.
From here one can see that equations like the KP equation makes connections to algebraic geometry through Spectral Curves, Grassmannians, Jacobian Flows, and Krichever modules \cite{Mulase1994} \cite{Segal1998} \cite{Sato1983} \cite{Miwa2000}.

On top of all this there is a general differential Galois Theory beyond linear equations  developed by Umemura \cite{Umemura2011} and a general theory of Riemann-Hilbert  Problems and holonomic D-Modules following Malgrange and Kashiwara \cite{Borel1987}.

I haven't even mentioned differential algebraic geometry (it's associated tussles with dimension theory) and the geometry of foliations \cite{Bryant1991} (or anything by Periera). To make things much harder, much of this material generalizes beyond differential equations , to difference equations \cite{Hrushovski2004}, $p$-derivations \cite{Buium2005}, and other operations \cite{Borger2005}.
These generalizations aren't just for sport and have real consequences outside of differential algebra.

Understandably, I can't cover this all. 
I'm not even going to pretend to try. 
My goal is to survey material.
Because of this, I'm going to need to assume some mathematics at times --- there already exists excellent references for much of the material we need to source.  
We will assume some basic Differential \cite{Ince1944} and Partial Differential Equations \cite{Evans2010}, Commutative Algebra \cite{Atiyah2016}, Galois Theory \cite{Cox2012}, Complex Analysis \cite{Ullrich2008}, Algebraic Topology \cite{Hatcher2002}, Manifold Theory \cite{Lee2013} (mostly complex manifolds which are not covered in loc cit.), and Algebraic Geometry \cite{Vakil2017}. 
At the same time, I'm not crazy. 
I don't want to be writing to nobody. 
Things that I feel are part of a good introduction for well-prepared graduate students I will review. 

In addition to helping graduate students, I want to help  myself.
I have a number of things I would like to understand better. What is a $\tau$-function? What is a space of initial conditions? 
What is a Jacobian flow? What proofs work for differential equations but not for difference equations? 
What do we \emph{really} mean when we say $X$ equation is a limiting case of $Y$ equation?
What are the most fundamental examples to keep in mind and teach students when talking about this material?
What do people mean when they say classical asymptotic methods are ``enriched by $D$-modules and sheaves''?

The subject is vast and I hope we have a fun time exploring it. 
It may be that I don't get anywhere on any of this material and we spend 3 months defining what a differential ideal is. 
We'll see. 
%At the end of all of this there is going to be many course that could be taught using this book.

\iffalse
\section*{A Note After Fall 2022} 
There is a manuscript by Manin \cite{Manin1978} which also attempts to unify differential algebraic and foliation theoretic perspectives on differential equations.

\fi


\section*{Citing These Notes and Version Control}
A link to \today \ version of the .pdf can be found here.
\begin{center}
	\url{https://tdupu.github.io/diff-alg-public/diff-alg.pdf}.
\end{center}
Any version of these notes you would like to recover can be found from tracing the github history. 
You just need to record the dates.

These notes have not been peer reviewed! In fact, I have accepted that these notes are going to be a working document for some time. 
Because of this, I have added the label ``unstable'' to titles of chapters, sections, or subsections when the material is will undergo significant changes.
The general rule of thumb that I'm using is if the majority of material is going to undergo significant changes it deserves and ``unstable'' label. 
That being said, even material which is stable is subject to being organized and reorganized. 

If you find a mistake or have comments about history or citations or really anything that you think will contribute to the material please email taylor.dupuy@gmail.com. Interesting examples are always welcome. 

Here is the bibtex citation for ``\today''. Please add ``\today'' in where it says [TODAY] in the citation. 
\begin{spverbatim}
@Unpublished{Dupuy2022,
author  = {Dupuy, Taylor},
title   = {An Introduction to the Algebraic Theory of Differential Equations},
year    = {2022},
comment = {Course notes from lectures given at the University of Vermont in Fall 2022 ([TODAY] version)},
journal = {preprint},
url     = {https://tdupu.github.io/diff-alg-public/diff-alg.pdf},
}
\end{spverbatim}



\newpage 
\section{Notation (unstable)}
\begin{itemize}
	\item $R^{\Delta}$ (or $R^{\partial}$) the constants of the derivations (or derivation). 
	\item $R\lbrace x \rbrace$ the ring of differential polynomials over $R$.
	\item $K(S)_{\partial} = K(\lbrace S \rbrace)$ the field extension of $K$ $\partial$-generated by $S$.
	\item $\CC\langle t-t_0 \rangle$ convergent power series at $t_0$
	\item $\CC\llangle t-t_0\rrangle$ Laurent series of meromorphic functions (so finite poles).
	\item $R[\Delta]$ the Weyl Algebra associated to a partial differential ring.
\end{itemize}
\newpage

\mainmatter

\tableofcontents


\chapter{Differential Algebra Basics}
I would skip this for now and only come back to this chapter when we need it. 

\section{The Basic Objects}

\subsection{$\Delta$-Rings and $\partial$-Rings}
In this book, unless stated otherwise, all rings are going to be commutative with a multiplicative unit. 
Let $R$ be a commutative ring. 
By a \emph{derivation} on $R$ we map a map of sets $\partial:R\to R$ that satisfied 
 $$ \partial(a+b) = \partial(a) + \partial(b), \qquad \forall a,b\in R,$$
 $$ \partial(ab) = \partial(a) b + a\partial(b), \qquad \forall a, b \in R, $$
 $$ \partial(1) = \partial(0) = 0.$$
Derivations are completely formal here. 
We don't care about limits. 

\begin{exercise}
	Check that all the usual rules hold. For example if $\partial:R \to R$ is a differential ring then 
	\begin{enumerate}
		\item For $n\in \ZZ_{\geq 0}$ and $a \in R$ we have $\partial(a^n) = na^{n-1}\partial(a)$.
		\item For $a \in R$ and $b\in R^{\times}$ we have $\partial(a/b) = (\partial(a)b - a \partial(b))/b^2$. Here $R^{\times}$ denotes the elements which have a multiplicative inverse.
		\item For $f \in R[x]$ and $a \in R$ we have $\partial(f(a)) = f^{\partial}(a) + f'(a)\partial(a)$. If $f(x) = \sum_{i=0}^d b_i x^i$ then $f^{\partial}(x) = \sum_{i=0}^d \partial(b_i) x^i$. 
	\end{enumerate}
\end{exercise}
Note that the one exception for derivative rules holding is the chain rule. 
For an abstract ring $R$ there is not a defined composition of elements $a\circ b$ (although you can compose with polynomials as above).


\begin{definition}
	A \emph{differential ring} or (\emph{$\Delta$-ring}) is a tuple $(R,\Delta)$ where $R$ is a commutative rings with unity and $\Delta = \lbrace \partial_1,\ldots,\partial_m\rbrace$ is a collection of commuting derivations $\partial_i:R \to R$. 
\end{definition}

When $\Delta = \lbrace \partial \rbrace$ then we call $(R,\Delta)$ a \emph{$\partial$-ring} and will use the notation $(R,\partial)$. 
We also call such a ring an ordinary differential ring. 


\begin{example}
	\begin{enumerate}
		\item The ring of polynomials in on variable $(\CC[t],\dfrac{d}{dt})$ 
		\item The ring of rational functions $(\CC(t), \dfrac{d}{dt})$, this is an example of a differential field. 
		In general a \emph{differential field} is a differential ring $(K,\Delta)$ where the underlying ring $K$ is a field. 
		\item The ring of holomorphic functions $\hol(U)$ for some $U\subset \CC^m$ is an example of a $\Delta$-ring, $(\hol(U), \lbrace \dfrac{\partial}{\partial t_1}, \ldots, \dfrac{\partial}{\partial t_m}\rbrace )$. 
		Here we are using $(t_1,\ldots,t_m)$ for the complex variables $t_j = \sigma_j + i \tau_j$ where $\sigma_j,\tau_j \in \RR$. 
		\item We can do the same thing with meromorphic functions $\Mer(U)$. These will give a differential field. 
	\end{enumerate}
\end{example}

\subsection{Morphisms of $\Delta$-Rings and $\partial$-Rings}
Let $(A,\Delta)$ and $(B,\Delta)$ be differential rings where we use $\Delta = \lbrace \partial_1,\ldots,\partial_m\rbrace$ for the derivatives on both $A$ and $B$.
\begin{definition}
A \emph{morphism} of differential rings is a ring homomorphism $f:A\to B$ such that for each $\partial_i \in \Delta$ we have $f(\partial_i(a)) = \partial_i(f(a))$ for each $a\in A$. 
\end{definition}

\subsection{Radicals of Differential Ideals}
Let $K$ be a $\partial$-field and let 
$$K\lbrace x \rbrace = K\lbrace x \rbrace_{\partial} = K[x]_{\partial}=K[x,x',x'',\ldots]$$ 
be the ring of $\partial$-polynomials.

For a subset $A$ of $K\lbrace x \rbrace$ we will let 
$$ [A]=[A]_{\partial} $$
be the $\partial$-ideal generated by $A$. 
It is the smalled $\delta$-ideal containing $A$. 
We will let 
$$ \lbrace A \rbrace = \lbrace A \rbrace_{\partial} = \sqrt{[A]} $$
be the smallest radical ideal containing $A$. 

\begin{lemma}[Radicals of Differential Ideals are Differential Ideals]
	Let $A$ be a differential $\QQ$-algebra. 
	If $I$ is a differential ideal then $\sqrt{I}$ is a differential ideal. 
\end{lemma}
\begin{proof}
	Suppose $a \in \sqrt{I}$. 
	Then there exists some natural number $n$ such that $a^n \in I$. 
	By the Power Lemma~\ref{lem:power-lemma} we have that $(a')^n \in I$. 
	This implies $a' \in \sqrt{I}$. 
\end{proof}

\begin{exercise}
	Show that the intersection of two radical ideals is radical.
\end{exercise}

\begin{exercise}
	Give an example of an ideal $I$ which is radical such that $I^2$ is not radical.
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Monodromy and Hilbert's 21st Problem }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The statement of Hilbert's 21st problem to the 1900 International Congress of Mathematicians (ICM) is as follows. 

\quoteattr{``In the theory of linear differential equations with one independent variable $z$, I wish to indicate an important problem one which very likely Riemann himself may have had in mind. 
	This problem is as follows: To show that there always exists a linear differential equation of the Fuchsian class, with given singular points and monodromic group. 
	The problem requires the production of $n$ functions of the variable $z$, regular\footnote{Hilbert means holomorphic.} throughout the complex $z$-plane except at the given singular points; at these points the functions may become infinite of only finite order, and when $z$ describes circuits about these points the functions shall undergo the prescribed linear substitutions. 
	The existence of such differential equations has been shown to be probable by counting the constants, but the rigorous proof has been obtained up to this time only in the particular case where the fundamental equations of the given substitutions have roots all of absolute magnitude unity. L. Schlesinger (1895) has given this proof, based upon Poincaré's theory of the Fuchsian zeta-functions. 
	The theory of linear differential equations would evidently have a more finished appearance if the problem here sketched could be disposed of by some perfectly general method.''}{Hilbert's 21st Problem}

In this chapter we are going to move towards Hilbert's 21st problem and some of the classical theory of monodromy of solutions of differential equations.


\section{The Monodromy Representation}
In this section we develop some basic tools we need to construct a monodromy representation associated to a linear differential equation on $\PP^1$.

\subsection{Wronskians}

Let $(R,\partial)$ be a differential ring. 
Let $f_1,\ldots,f_n\in R$. 
The \emph{Wronskian} of $f_1,\ldots,f_n$ is 
 $$ W(f_1,\ldots,f_n) = \det \begin{pmatrix}
 f_1 & f_2 & \cdots & f_n \\
 f_1' & f_2' & \cdots & f_n' \\
 \vdots & \vdots & \ddots & \vdots \\
 f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}
 \end{pmatrix}.$$
The Wronskian gives us a test for linear dependence over the constants of a differential field. 
\begin{theorem}
	Let $(K,\partial)$ be a differential field. 
	Let $f_1,\ldots,f_n \in K$. 
	Let $C = K^{\partial}$ be the constants. 
	We have that $f_1,\ldots,f_n$ are linearly dependent over $C$ if and only if $W(f_1,\ldots,f_n)=0$. 
\end{theorem}
\begin{proof}
	Suppose that $f_1,\ldots,f_n$ are linearly dependent over $C$.
	Then there exists $c_1,\ldots,c_n \in C$ not all zero such that 
	 $$ c_1 f_1 + \cdots + c_n f_n =0.$$
	 Taking derivatives gives 
	  \begin{equation} \label{E:element-of-kernel}
	  	\begin{pmatrix}
	  f_1 & f_2 & \cdots & f_n \\
	  f_1' & f_2' & \cdots & f_n' \\
	  \vdots & \vdots & \ddots & \vdots \\
	  f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}
	  \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\  \vdots  \\ c_n \end{pmatrix} =0.
  \end{equation}
	 Since the $c_i \in K$, this means the matrix $B$ such that $\det(B)=W$ is singular and hence $W=\det(B)=0$.
	 
	 Conversely, suppose that $W=0$. 
	 Then there exists some $c_1,\ldots,c_n \in K$ not all zero such that \eqref{E:element-of-kernel} holds. 
	 To prove our result, we need to show that $c_1,\ldots, c_n \in C$. 
	 If some proper subset of $\lbrace f_1,\ldots, f_n \rbrace$ have a non-trivial dependence relation we can replace our set with that subset and hence we can assume without loss of generality that $\lbrace f_2,f_3,\ldots, f_n \rbrace$ are linearly independent over $K$. 
	 We can also suppose that $c_1\neq 0$. 
	 Furthermore we can scale the vector $(c_1,\ldots,c_n)$ by $1/c_1$.
	 Hence we can further assume that $c_1=1$.  
	 
	 Now for $1\leq j \leq n-2$ (note the $n-2$ here) we can take a derivative of 
	  $$ c_1 f_1^{(j)} + \cdots + c_n f_n^{(j)} =0 $$
	 To get 
	 \begin{align*}
	 		   0 =& c_1 f_1^{(j+1)} + \cdots + c_n f_n^{(j+1)} + c_1'  f _1^{(j)} + \cdots + c_n' f_n^{(j)} \\
	 		   =&  c_2'  f _2^{(j)} + \cdots + c_n' f_n^{(j)}.
	\end{align*}
	But $f_2,\ldots,f_n$ are linearly independent. 
	This implies that $c_2'=\cdots=c_n'=0$ which implies that $c_1,\ldots,c_n \in C$ which proves our result. 
\end{proof}

We want to show that the Wronskian satisfies a linear differential equations. 
To do this we need a couple things. 

In what follows one needs to recall the definition of an adjugate matrix and how cofactor expansion works. 
Recall that if $A$ is an invertible $n\times n$ matrix then the \emph{adjugate} is defined by 
$$\adj(A) = \det(A) A^{-1}.$$
This is the best way to remember the formula.  
The adjugate is just what would be the inverse would be had we not inverted the determinant. 
Unlike inverse, tt turns out that every $n\times n$ matrix and we can obtain its formula from cofactor expansion. 
We have 
 $$ \adj(A)_{ji} = (-1)^{i+j} \det(\widetilde{A}_{ij})$$
where $\widetilde{A}_{ij}$ is the matrix obtains by deleting the $i$th row and $j$th column.
This all comes from the formula for the inverse of a matrix using cofactor expansion (sometimes also called ``Laplace's Formula'').

Finally, we need to know what the partial derivative of the determinant is with respect to each of its entries. 
In what follows we are going to consider $X = (x_{ij})$ as an abstract $n\times n$  matrix with entries being variables. 
This means that $\det(X)$ will be viewed as a polynomial in $\ZZ[x_{ij}\colon 1 \leq i,j \leq n ]$. 
\begin{lemma}
	Let $X =(x_{ij})$ be a symbolic matrix. 
	$$ \dfrac{\partial \det(X)}{\partial x_{ij}} = \adj(X)_{ji}. $$
\end{lemma}
\begin{proof}
	The proof is direct. 
	By cofactor expansion we have $\det(X) = \sum_{j=1}^n x_{ij} \adj(X)_{ji}$ hence
	 \begin{align*}
	 	\dfrac{\partial \det(X)}{\partial x_{ij}} = & \dfrac{\partial}{\partial x_{ij}} \left [ \sum_{\ell=1}^n x_{i\ell} \adj(X)_{\ell i}\right] \\
	 	&= \sum_{\ell=1}^n \dfrac{\partial x_{i\ell}}{x_{ij}} \adj(X)_{\ell i} + x_{i\ell} \dfrac{\partial }{\partial x_{ij}} \adj(X)_{\ell i} \\
	 	&= \sum_{\ell=1}^n \delta_{\ell j} \adj(X)_{\ell i} = \adj(X)_{ji}.
	 \end{align*}
 	Note that on the second to last equality we used that $\dfrac{\partial }{\partial x_{ij}} \adj(X)_{\ell i}=0$ since $\adj(X)_{\ell i}$ has no terms with $i$ in the first entry and $\ell$ in the second entry (this is the cofactor expansion formula).
\end{proof}

To apply this we need the formula for the dot product of matrices. 
Sometimes this is called the ``Killing form''.\footnote{Named after Wilhelm Killing 1847--1923}.
If you have never done this exercise in your life you should do it.
\begin{exercise}
	Let $A,B \in M_n(R)$ for a commutative ring $R$. 
	One has 
	$$\Tr(A^TB) = \sum_{1\leq i,j \leq n}A_{ij}B_{ij}.$$ 
\end{exercise}
We can now prove our result.
\begin{theorem}
	Let $A = (a_{ij}) \in M_n(R)$ with $(R,\partial)$ a differential ring. 
	We have 
	 $$ \partial(\det(A)) = \Tr(\adj(A) \partial(A))$$
	where $\partial(A)$ denotes the matrix $\partial(A)= ( \partial(a_{ij}))$.
	Furthermore if $A \in \GL_n(R)$ then 
	 $$\partial(\det(A)) = \Tr( A^{-1} \partial(A)) \det(A).$$
\end{theorem}
\begin{proof}
	Let $X = (x_{ij})$.
	We are going to use the chain rule
	\begin{align*}
		\partial(\det(X)) =& \sum_{1\leq i,j \leq n} \dfrac{\partial \det(X)}{\partial x_{ij}}\partial(x_{ij}) \\
		=& \sum_{1\leq i,j \leq n} \adj(X)_{ji} \partial(x_{ij}) \\
		=& \Tr( \adj(X)\partial(X)).
	\end{align*}
    To get the last formula, if $X$ is invertible we use the previous formula $\adj(X) = \det(X) X^{-1}$.
\end{proof}


\subsection{Stalks and Germs of Holomorphic and Meromorphic Functions}

Recall that for $U' \subset U$ open subset of $\CC^m$ we have injectures $\hol(U) \to \hol(U)$ and $\Mer(U) \to \Mer(U')$ given by restricting the domain of some $f(z)$ to $U'$.
Both of these ring homomorphisms are injective by the analytic continutation principle (which holds in several variables as well as one variable).\footnote{If you have never showmn that analytic continuation works in two variables this is a good exercise.}
The \emph{stalk} at some $t_0 \in \CC$ is 
 $$ \hol_{t_0} = \varinjlim_{U \owns t_0} \hol(U), \quad \Mer_{t_0} \varinjlim_{V \owns t_0} \Mer(U) $$
where the direct limit is taken over open set $U$ containing $t_0$.
Any element of a stalk is called a \emph{germ}. 

It is important to know that there is always a ring homomorphism $\hol(U) \to \hol_{t_0}$ and that any element of $\hol(U)$ is determined by its stalk. 
Same goes for meromorphic functions.

\begin{remark}
	For the uninitiated, we recall that if $I$ is a partially ordered set then a directed system is a collection $((R_i)_{i\in I},(f_{i,j})_{i<j})$ consisting of rings $R_i$ and morphisms $f_{i,j}:R_i \to R_j$ whenever $i<j$. 
	
	The direct limit of the directed system then is the ring 
	 $$ \varprojlim R_i = (\coprod_{i\in I} R_i)/\sim $$
	where $r_i \in R_i$ and $r_j \in R_j$ are declared equivalent when for some $k>i,j$ we have $f_{i,k}(r_i)  = f_{j,k}(r_j)$.
\end{remark}

In the one variable case we for $a\in \CC$ we are going to use the notation 
 $$ \CC\langle t-a \rangle := \hol_a, \quad \CC\llangle t-a \rrangle = \Mer_a $$
And in the several variable case for $(a_1,\ldots,a_m) \in \CC^m$ we will use the notation
 $$ \CC\langle t_1 -a_1,\ldots, t_m - a_m\rangle = \hol_{(a_1,\ldots,a_m)}, \quad \CC\llangle t_1-a_1,\ldots, t_m-a_m \rrangle = \Mer_{(a_1,\ldots,a_m)}.$$
 In other books they use $\CC\lbrace t \rbrace$ for convergent power series but we are going to reserve this symbol for the ring of differential polynomials.

\subsection{Reduction to First Order Systems}

Any system of PDEs is equivalent to a first order system of PDEs.
The idea is that we can always introduce more variables every times we need to take a new derivative so that all of our expressions only involve single derivatives of variables. 
Later, for linear differential equations we will see that we can actually go backwards. 

We illustrate this in the case of linear first order differential equations in one differential indeterminate. 
Here we consider the equation
 $$ y^{(r)} + a_{r-1} y^{(r-1)} + \cdots + a_0 y =0. $$
By introducing ``velocity variables'' $v_j = y^{(j)}$ for $j=0,1,\ldots, r-1$ we get a new system
$$\begin{cases}
	v_0' = v_1, \\
	v_1' = v_2 ,\\
	\ddots \\
	v_{r-1}' = -a_{r-1}v_{r-1} - a_{r-2} v_{r-2} - \cdots - a_0 v_0 .
\end{cases}$$
which then can be written in matrix form 
 $$ V' = AV $$
where 
 $$ V = \begin{pmatrix}v_0 \\
 v_1 \\
 \vdots \\
 v_{r-1} 
 \end{pmatrix}, \qquad A = \begin{pmatrix}
 0 & 1 & 0  & \cdots & 0 \\
 0 & 0 & 1 & \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & 0 & \cdots & 1 \\
 -a_{r-1} & -a_{r-2} & -a_{r-3} & \cdots & -a_0
 \end{pmatrix}.
 $$
Here $A$ is just the transpose of a companion matrix. 
We will often convert between higher order equations and first order equations in this way. 

\begin{remark}
	In the case of linear differential equations there is a way of going backwards using cyclic vectors (\S\ref{S:cyclic-vectors}). 
	That is, most first order linear systems of differential equations  $n$ variables in one derivative can be converted into a order $n$ equation in $n$ dependent variables. 
\end{remark}

\subsection{Linear Systems}
Let $A \in M_n(R)$ where $R$ is a differential ring.  
The system 
\begin{equation}
Y' = A Y
\end{equation}
 is called a \emph{linear system at over $R$} in the indeterminates $Y=(y_1,\ldots,y_n)$ (I am going to allow myself to abusively conflate row and column vectors). 
 The letter $n$ is sometimes called the \emph{rank} of the linear system. 
 
\begin{exercise}
	The solutions of a linear system form an $R^{\partial}$-module. 
\end{exercise}
 
A matrix $\Phi \in \GL_n(R)$ is called a fundamental set of solutions or \emph{fundamental solution} if 
 $$ \Phi' = A \Phi,$$
where the derivatives of $\Phi$ in the expression $\Phi'$ are taken component-wise. 
The idea is that the columns of the matrix $\Phi$ form a basis of solutions over the constants $R^{\partial}$.

Fundamental matrices are unique. 
Any solution of the linear system takes the form $\Phi Z$ for some vector $Z \in (R^{\partial})^{\oplus n}$. 
This menas that if $\widetilde{\Phi}$ is another fundamental matrix there exists some $M \in \GL_n(R^{\partial})$ such that
\begin{equation} \label{E:monodromy-matrix}
  \widetilde{\Phi} = \Phi M.
\end{equation}
In the theory of monodromy, these will become the monodromy matrices and in the Picard-Vessiot theory of linear differential algebraic extensions of differential fields these matrices are going to become the Galois group elements.
This is so important we are going to put it in a theorem environment. 
\begin{theorem}[Existence of ``Monodromy'' Matrices]
	If $\Phi$ and $\widetilde{\Phi}$ are two fundamental matrices of a rank $n$ linear system over a differential ring $(R,\partial)$ then there exists some $M \in \GL_n(R^{\partial})$ such that $\widetilde{\Phi} = \Phi M$.
\end{theorem}

To prove a fundamental set of solutions we are going to use existence and uniqueness together with the following lemma.

\begin{lemma}\label{L:linear-independence}
	Let $K$ be a $\partial$-field. 
	If $Y_1,\ldots,Y_n \in K^n$ are linearly independent over $K$ then they are linearly independent over $C=K^{\partial}$. 
\end{lemma}
\begin{proof}
	We prove this by proving they are linearly dependent over $K$ if and only if they are linearly dependent over $C$.
	If they are linearly dependent over $C$ then clearly they are linearly dependent over $K$. 
	Conversely, suppose that they are linearly dependent over $K$.
	We will prove this by induction so we can suppose that no proper subset is linearly dependent over $K$ otherwise we could apply the inductive hypothesis. 
	The base case is immediate. 
	
	Now we do the inductive step. 
	By clearing denominators we have $Y_1 = \sum_{j=2}^n c_j Y_j$ for some $c_j \in K$.
	We have 
	\begin{align*}
	0=&Y_1'-AY_1 \\
	&= \sum_{j=2}^n c_j' Y_j + \sum_{j=2}^n c_jY_j' - \sum_{j=2}^n c_j AY_j\\
	&= \sum_{j=2}^n c_j' Y_j 
	\end{align*}
	But since $Y_2,\ldots,Y_n$ were assumed to be linearly independent we must have $c_2'=\ldots=c_n'=0$ which proves the $c_j$'s are constants. 
\end{proof}
 
 \subsection{Holomorphic Linear Systems}
A \emph{holomorphic linear system} at $t_0 \in \CC$ is a linear system over $R= \CC\langle t -t_0\rangle$. 
That is, it is a system of linear differential equations 
$$ Y' = AY $$
where the matrix $A$ is holomorphic at $t_0\in \CC$.
 
 We now prove the existence and uniqueness theorem for holomorphic linear systems.
 
 \begin{theorem}[Existence and Uniqueness]
Let $t_0 \in \CC$
Let $A \in M_n(\CC\langle t-t_0 \rangle)$. 
Let $Y_0 \in \CC^n$. 
There exists a unique $Y \in \CC\langle t-t_0 \rangle^{\oplus n}$ such that 
$$\begin{cases}
	Y' = A Y,\\
	Y(t_0) = Y_0.
\end{cases}$$
 \end{theorem}
 There are three ways of doing this.
 I might add some more details later.
 \begin{enumerate}
 	\item Use power series expansions, then prove a convergence result. 
 	\item Big Hammer: Use Cauchy-Kowalevski\footnote{This is the same as Cauchy-Kovaleskaya.
 	Some people spell the Russian name differently. }
   This theorem is morally the same as above just with more complicated PDEs. 
   One shows that there is a power series solution then proves convergence.
    \item Bigger Hammer: Use the existence of differentially closed fields $\widehat{K}$ is the $\partial$-closure of the field $K \subset \CC\llangle t-t_0 \rrangle$ given by $K=\QQ(a_{ij} : 1\leq i,j \leq n )_{\partial}$.
    This is the differential field generated by the coefficients of the matrix $A$.
    The Siedenberg embedding theorem then tells us that $\widehat{K} \subset \CC\llangle t-t_0 \rrangle$, and this gives us a holomorphic solution of $Y'=AY$. 
    By the property of differential closures once we find a solution we can keep adjoining solutions using Blum's axiom. \taylor{explain this further}.
 \end{enumerate}

We now prove the existence of a fundamental matrix.
\begin{lemma}
	Every holomorphic linear system which is holomorphic at $t_0 \in \CC$ admits a fundamental matrix $\Phi(t)$ which is holomorphic at $t_0$.
\end{lemma}
\begin{proof}
By existence and uniqueness we can always find a solution $Y_i \in \CC\langle t-t_0 \rangle^{\oplus n}$ satisfying 
 $$ Y_i' = AY_i, \quad Y_i(t_0) = e_i $$
where $e_i$ is an elementary column vector (it has zeros everywhere except for the $i$th position).
The solutions $Y_1,\ldots,Y_n$ are linearly independent over $K=\CC\langle t-t_0 \rangle^{\oplus n}$ because $e_1,\ldots,e_n$ are linearly independent over $K$. 
Hence by Lemma~\ref{L:linear-independence} we get that the solutions are linearly independent over over $\CC$.
 The matrix $$\Phi = [Y_1 \vert Y_2 \vert \cdots \vert Y_n ]$$ 
 is our fundamental system.
 \end{proof}

\subsection{Restricting the Coefficient Matrix to a Lie Algebra}
It will be conventient in the equation $ Y' = A Y $ to restrict the matrix $A$ to a particular Lie algebra $\Lie(G)$ of some Lie group $G$. 
Here we recall that a Lie group (real or complex) is just a manifold (real of complex) with the structure of group. 
\footnote{There are also algebraic group or group schemes but readers familiar with those already know all of this, so I'm going to not say anything about that as it will take us two far afield.}
The Lie algebra of such a Lie group $\Lie(G)$ can be described either as the group of tangent vector at the identity of $G$ or as the globally invariant vector fields on $G$ (obtained by propagating the tangent vector at the tangent space at the identity to any other point of the manifold by pushforward by multiplication-by-$g$). 
All Lie algebras come with a Lie bracket $(A,B) \mapsto [A,B]$ which is an infinitesimal version of the group multiplication. 
It turns out that this multiplication satisfies the so-called Jacobi identity.
The fundamental property of Lie algebras of Lie groups are that if $G$ is a matrix Lie group then
 $$A\in \Lie(G) \implies e^A \in G.$$
Table~\label{T:lie} gives some common Lie groups with their Lie algebras.
As a sanity check not that if $A^*=-A$ then $(e^A)^* = e^{A^*}=e^{-A} = (e^{A})^{-1})$ so skew-adjoint matrices give rise to unitary matrices after exponentiation.
\begin{table}\label{T:lie}
	\begin{center}
		\begin{tabular}{ccc}
			type & Lie Group & Lie Algebra\\
			\hline complex & $\GL_n(\CC)$ & $M_n(\CC)$ \\
			real & $U_n$ unitary, $U^*=U^{-1}$ & skew-adjoint $A^*=-A$ \\
			complex & $\SL_n(\CC)$, $\det(A)=1$ & trace free $\tr(B)=0$ 
		\end{tabular}
	\end{center}
	\caption{Some common Lie groups and their Lie algebras.}
\end{table}

For the uninitiated we mention that we often axiomatize the notion of a Lie algebra as an $R$-module (or a functor to $R$-modules) which has a Lie bracket and satisfies the Jacobi identity axiom. 
This is useful but not what we mean here. 
The collection of abstract vector fields on a space (scheme, complex manifold, real manifold) satisfy these axioms for example and here the ring $R$ is a ring of functions on a space. 

The main reason we mention Lie groups is because if we restrict our linear equations to have values in a Lie algebra, then the solution will be valued in a Lie group.
\begin{theorem}
	Consider $Y'(t)=A(t)Y(t)$. If $A(t)$ is valued in $\Lie(G)$ then any fundamental matrix will be valued in $G$. 
\end{theorem}
\begin{proof}
	This is a consequence of Theorem~\ref{T:solutions}
\end{proof}



\subsection{Monodromy of Holomorphic Linear Systems}

Consider a holomorphic linear system 
$$Y'=AY, \quad A=A(t) \in M_n(\hol(U)),$$ 
where $U\subset \CC$ a connected open set. 
By the previous section for each $t_0 \in U$ there exists a fundamental matrix $\Phi$ which is holomorphic in a neighborhood of $t_0$.
We are going to want to analytically continue $\Phi$ along every path $\gamma$ starting at $t_0$ and obtain $\Phi^{\gamma}$ which will eventually allow us to cook-up a group homomorphism from the fundamental group of paths starting at $t_0$ to $\GL_n(\CC)$ which measures how much $\Phi$ changed once we take it around the look. 

\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.5]{analytic-continuation.eps}
	\end{center}
\caption{A picture of analytically continuing a local fundamental matrix along a path.}
\end{figure}

The group homomorphism 
 $$ \rho: \pi_1(U,t_0) \to \GL_n(\CC), \quad \rho(\gamma) = M_{\gamma}$$
is called the \emph{monodromy representation}.
We will now explain what $M_{\gamma}\in \GL_n(\CC)$ is  supposing $\Phi_{\gamma}$ exists:
since $\Phi$ and $\Phi_{\gamma}$ are both fundamental matrices at $t_0$ then as in  \eqref{E:monodromy-matrix} there exists some $M_{\gamma}$ such that 
 $$ \Phi_{\gamma} = \Phi M_{\gamma}.$$
That is all.

We need to set our convention for concatenation of paths. 
If $\gamma_1$ and $\gamma_2$ are two paths in $U$ where the endpoint of $\gamma_2$ is the starting point of $\gamma_1$ then we will let $\gamma_2\gamma_1$ denote the path which first performs $\gamma_1$ then performs $\gamma_2$. 

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{fundamental-group-convention.eps}\label{F:fundamental-group-convention}
\end{center}
\caption{The convention we use for composition of paths. Other people use other conventions and it will mess up your formulas.}
\end{figure}

\begin{remark}[WARNING]
	Conventions on concatenation of paths changes from text to text and this will mess with your formulas.
\end{remark}

With our convention for concatenation of paths we have.
 $$ \Phi_{\gamma_2\gamma_1} = (\Phi_{\gamma_1})_{\gamma_2}$$
On one hand we have $\Phi_{\gamma_2\gamma_1} = \Phi M_{\gamma_2\gamma_1}.$
On the other hand we have $ (\Phi_{\gamma_1})_{\gamma_2}= (\Phi M_{\gamma_1})_{\gamma_2} = \Phi_{\gamma_2} (M_{\gamma_1})_{\gamma_2} = \Phi M_{\gamma_2} M_{\gamma_2}.$
This proves that 
 $$ M_{\gamma_2\gamma_1} = M_{\gamma_2}M_{\gamma_1}.$$

Finally, suppose that $\Psi$ is another fundamental matrix at $t_0$.
Then $\Psi = \Phi M$ for some $t_0$ and let $\Psi_{\gamma} = \Psi N_{\gamma}$. 
Then we have 
 $$\Phi M N_{\gamma} =\Psi N_{\gamma}  =\Psi_{\gamma} = \Phi_{\gamma} M = \Phi M_{\gamma} M, $$
which implies 
 $$ N_{\gamma} = M^{-1} M_{\gamma} M .$$
 This proves the representation is independent of the choice of fundamental matrix up to conjugation.

\begin{example}[Babymost Example]
	In the rank one case we have a differential equations 
	 $$ y'(t) = a(t) y(t), \quad a(t) \in \hol(U). $$
	This has a solution $\phi(t) = \exp( \int_{t_0}^t a(s) ds)$ which is also the fundamental matrix. 
	This formula makes sense in a small disc around $t_0$.
	For things to be interesting we need  $ \int_{\gamma} a(s) ds $ to have monodromy.
	
	If $a(t) = 1/t$ this would be the simplest case. 
	This is a little to simple as $\int_{t_0}^t \frac{ds}{s}$ would give a branch of $\log(s)$ which would only change the exponent by $2\pi i$.
	
	If $a(t) = c/t$ for some constant $c$, then things get a little interesting. 
	One then has $y(t) = t^{c} := \exp( c \log(t))$ as a solution. 
	In this case if we let $\gamma_0$ be a loop around the origin and $M_0 = M_{\gamma_0}$ we find that 
	$$M_{0} = \exp{2 \pi i c}.$$
	\qed
\end{example}

For any path $\gamma$ in $U$ starting at $a \in U$ and ending at $b\in U$ and any fundamental matrix $\Phi$ in a neighborhood of $a$ we are going to show that we can analytically continue $\Phi$ along $\gamma$ to get a new fundamental matrix $\Phi^{\gamma}$ which is the analytic continuation of $\Phi$ along gamma.
There are some issue that we need to address.
\begin{enumerate}
\item How do we know that the fundamental matrix doesn't have a natural stopping point where it can't be continued further?
\item How do we know that the continuation $\Phi^{\gamma}$ doesn't degenerate after leading the initial ball $B$ where the power series defining it converged? How do we know solutions don't become linearly dependent?
\end{enumerate}

Let's address the first issue. 
Suppose that $\Phi$ is analytic in some ball $B$ around $a$ and that there is some $a_1$ on the boundary of $B$ where $\Phi$ doesn't extend. 
Well since $a \in U$ we know that there exist some $\Phi_1$ a fundamental matrix which is valid in some neighborhood $B_1$ of $a_1$. 
Then on $B\cap B_1$ we there exists some matrix $M_1 \in \GL_n(\CC)$ such that 
 $$ \Phi_1 = \Phi M_1.$$
By analytic continuation we could actually extend $\Phi$ to $B\cap B_1$ and hence by the sheaf property there exists some unique $\Phi_2$ such defined on $B \cup B_1$ which restricts to $\Phi$ and $\Phi_1$ on there respective domains.

Let's now address the second issue. 
Let $\det(\Phi)=W$. 
We need to show that $W(t)$ is never zero on these continuations. 
We know that 
 $$ W'(t) = \Tr( \Phi^{-1} \Phi') W(t). $$
But since $\Phi' = A\Phi$ we have that $\Phi^{-1} \Phi' = \Phi^{-1}(t) A(t) \Phi(t)$ and since trace is invariant under conjugation our scalar equation becomes 
 $$ W'(t) = \Tr(A(t)) W(t), $$
and we see that 
 $$ W'(t) = \exp(\int_{t_0}^t \Tr(A(s)) ds),$$
where the integral is understood to be a path integral. 
This is never zero which implies that $\Phi_{\gamma}(t)$ always remains a fundamental system of solutions.

Finally, we just want to make the remark that $\Phi_{\gamma}$ only depends on the homotopy class $[\gamma]$ of $\gamma$. 
This is because path integrals are well-defined on homotopy classes.  



\begin{theorem}
For every $U \subset \CC$ and every $A(t) \in \hol(U)$ and every $t_0 \in U$, monodromy of a fundamental set of solutions is well-defined and hence induces a well-defined monodromy representation $ \pi_1(U,t_0) \to \GL_n(\CC),$ given by $[\gamma] \mapsto M_{\gamma}$ where $M_{\gamma}$ is the matrix  $\Phi_{\gamma}=\Phi M_{\gamma}$ for a fundamental matrix $\Phi$.
\end{theorem}

	\begin{example}[Euler Systems]
	In a punctured neighborhood around $0 \in CC$, consider the system 
	$$ Y' = \frac{A}{t} Y. $$
	Consider the function $t^A = \exp(A \log(t))$ for some branch $\log(t)$ and $\exp$ denoting the matrix exponential. We have 
	$$\dfrac{d}{dt}\left[ t^A\right] = \exp(A \log(t) ) A \frac{1}{t} = \frac{A}{t} t^A,$$
	so the matrix $\Phi(t) = t^A$ is a local matrix solution of this equation. 
	Since $\det(e^B) = e^{\Tr(B)}$ for any matrix $B$ we have $\det(\Phi) = t^{\Tr(A)}$ which is never zero and hence $\Phi(t)$ is a fundamental matrix. 
	
	Now let $\gamma$ be a loop in $U$ that encloses the origin.  
	We can compute 
	 $$\Phi_{\gamma}(t) = \exp( A( \log(t) + 2\pi i) = \Phi(t) \exp(2\pi i A) $$
	and hence $M_{\gamma} = \exp(2\pi i A).$
\end{example}

\begin{exercise}
	Every matrix $M \in \GL_n(\CC)$ can appear as the monodromy matrix of some system. (Hint: use the Euler system and show that for every $M \in \GL_n(\CC)$ there exist some $A \in M_n(\CC)$ such that $\exp(2\pi i A) = M$. This needs some ideas like a matrix logarithm or using a Jordan canonical form.)
\end{exercise}


\section[Fuchsian Condition]{Classification of Fuchsian Equations}


\taylor{In hindsight, I would have preferred to follow the logic of \cite[Chapters III,IV]{Borel1987} and peppering in the examples}

The Fuchsian condition is a condition on meromorphic differential equations that we impose that make it so that solutions aren't divergent. 
Maybe this isn't obvious but if one applies the power series technique to innocent looking differential equations they can have formal power series solutions which are completely divergent. 
The next example shows this.
\begin{exercise}
	Consider the equation 
	 $$ t^3 y''(t) + (t^2+t) y'(t) - y(t) =0.$$
	If we expand in a power series we find that for $ y(t) = \sum_{n=0}^{\infty} a_n t^n $
	to be a solution one had the initial value difference equation
	$$\begin{cases}
	a_0 =0, \\
	a_1 = a, \\
	a_n = -(n-1)a_{n-1}.
	\end{cases}$$
	where $a \in \CC$ is arbitrary. 
	One finds that 
	$$ y(t) = a \sum_{n=1}^{\infty} (-1)^{n+1} (n-1)! t^n \in \CC[[t]]\setminus \CC\langle t \rangle$$
	is a divergent power series solution! Note that $\vert a_{n+1} \vert/\vert a_n \vert = n \to \infty$ as $n\to \infty$.
\end{exercise}	

So what is the issue? 
The issue is that when we convert this equation into a first order system of differential equations is has a pole of order bigger than one. 
One can check that if we let $y'(t)=v(t)$ in the above example we see that  $v'(t) = -\frac{t^2+t}{t^3}v(t)+\frac{1}{t^3}y(t)$ and letting $Y(t) = (y(t),v(t))$ we get the first order system 
 $$ Y' =A(t) Y $$
where 
\begin{align*}
 A(t) =& 
\begin{pmatrix}
0 & 1 \\
-\frac{t+1}{t^2} & \frac{1}{t^3}
\end{pmatrix} 
=&
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix} \frac{1}{t^3}
+\begin{pmatrix}
0 & 0 \\
-1 & 0 
\end{pmatrix}\frac{1}{t^2}
+
\begin{pmatrix}
0 & 0 \\
-1 & 0 
\end{pmatrix}\frac{1}{t}
+
\begin{pmatrix}
0 & 1 \\
0 & 0  
\end{pmatrix}
\end{align*}
The matrix expansion of $A(t)$ has a pole of order bigger than one at $t=0$. 

In what follows we are going to let $\PP^1$ denote the projective line (equivalently the Riemann sphere). 
In order to avoid all of the divergent behavior we introduce the notion of a Fuchsian differential equation. 
We will later prove that these differential equations have ``regular singular points''.
\begin{definition}[Fuchsian Differential Equations]
Consider a rank $n$ first order system of differential equations 
\begin{equation}\label{E:first-order-system}
  Y' = A(t) Y. 
 \end{equation}
with $A(t) \in M_n(\hol(\PP^1\setminus T))$ for $T\subset \PP^1$ a finite collection of points. 
We say the system  is \emph{Fuchsian at $t_0 \in T$} (or \emph{regular singular} or \emph{logarithmic}) if $A(t)$ has the form
		 $$ A(t) = \frac{B(t)}{t-t_0},$$
where $B(t)$ is  holomorphic at $t_0$. 
We say the system is \emph{Fuchsian} (or \emph{regular singular} or \emph{logarithmic}) at if it is Fuchsian at every point in $T$. 
\end{definition}
The collection of rank $r$ Fuchsian differentiall equations on $\PP^1\setminus T$ is going to be denoted by 
 $$ \Sys_+^r(\PP^1\setminus T). $$
This is to be put in contrast with $\Sys^r(\PP^1\setminus T)$ which we are using to denote holomorphic systems.
We are going to want to compare these to $\Char_{\GL_r}(\PP^1\setminus T)$ for the Riemann-Hilbert problem.

\begin{remark}[Fuchsian/Regular/Logarithmic]
	\begin{enumerate}
		\item 	The words ``Fuchsian'', ``regular singular'', and ``logarithmic'' all mean the same thing.
		\item 	When a differential equation has a point which is not a regular singular point, people almost always use the term \emph{irregular singular point}.
		\item The systems $\Sys_+^n(X)$ will be generalized to $\Conn_+^n(X)$ integrable logarithmic connections on a vector bundle. 
		We use ``linear system'' to mean connection on a trivial vector bundle.
		We also (following Ogus) use a symbol ``$+$'' every time we are really making a logarithmic construction.
	\end{enumerate}
	

	
	
\end{remark}

There is a simple translation of this criterion into ODEs.
We extend this concept to higher order differential equations in one variable by saying that they at Fuchsian and Fuchsian at a point if there associated first order system is. 

\begin{exercise}[Fuch's Criterion For ODEs In One Variable]\label{E:fuchsian-odes}
	Consider a univariate holomorphic system on $\PP^1\setminus S$. 
	A first order system 
	 $$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t)=0 $$
	is Fuchsian at $t=t_0$ if and only if  the poles of the coefficients are restricted by $\mult_{t=t_0}( a_j(t) ) \geq n-j $.
	This means that the equation takes the form
	 	 $$ y^{(n)} + \frac{b_{n-1}(t)}{t-t_0} y^{(n-1)} + \cdots + \frac{b_{0}(t)}{(t-t_0)^n} =0, $$
	where the $b_j(t)$ are holomorphic at $t=t_0$
\end{exercise}

\begin{exercise}\label{E:fuchsian-parameters}
	Suppose that $S = \lbrace t_1,\ldots,t_m, t_{m+1}=\infty\rbrace$.
	Show that the coefficient 
	 $$a_{n-j}(t)= \frac{b_{n-j}(t)}{\prod_{j=1}^m(t-t_j)^j}, \quad \deg(b_{n-j}) \leq j(m-1). $$
	define a Fuchsian equation an every Fuchsian equation of order $n$ takes this form.
	
	Conclude that the number of coefficients one needs to select is 
	 $$ \sum_{j=1}^{n} j(m-1) = {n \choose 2} (m-1). $$
	(It turns out the coefficents need to satisfy a Fuch's relation (Theorem~\ref{T:fuchs-relation}) which will cut this dimension down by one)
\end{exercise}

The Airy equation is a famous example of a irregular linear differential equation.
This is a primary example when we want to talk about Stokes data which is the generalization of monodromy data (a monodromy reprepresentation) for differential equtions with irregular singular points.
\begin{example}[Airy Equation]
	Consider the Airy equation 
	 $$ y'' = ty.$$
	One can see that this system is regular singular at $t \in \PP^1\setminus \infty$.  
	At $t=\infty \in \PP^1$ we need to change variables $t=1/s$ and we find that $dt = \frac{-1}{s^2}ds$ which means $\frac{d}{dt} = -s^2 \frac{d}{ds}$ and $$\frac{d^2}{dt^2} = s^2 \frac{d}{ds} s^2 \frac{d}{ds}= s^2 (s^2\frac{d}{ds} + 2s)\frac{d}{ds} = s^4\frac{d^2}{ds^2}+2s^3 \frac{d}{ds},$$
	which gives 
	%$$s^4 \frac{d^2y}{ds^2}+ 2s^3 \frac{dy}{ds} = \frac{1}{s} y, $$
	%or 
	 $$ \frac{d^2y}{ds^2}+ \frac{2}{s} \frac{dy}{ds} - \frac{1}{s^5} y=0. $$
	From this we see that there is an irregular singular point at $s=0$.
\end{example}

\begin{remark}[Computations at Infinity]
	There are two ways to compute what $\frac{d^2}{dt^2}$ in the chart at infinity. 
	The first way is to act on an unknown function $f$ by the operator $-s^2 \frac{d}{ds}$ twice and then pretend line you never used the symbol $f=f(s)$ for a computation.
	The second way is to consider the non-commutative ring $\CC[s,\partial]$ subject to the relations $\partial s = s\partial + 1$. 
	This is the a ring of linear differential operators on $\CC[s]$ called the \emph{Weyl algebra}.
	The second way is really equivalent to the first way.
\end{remark}

As stated before, we care about Fuchsian differential equations because they tell us that the solutions are nice. 
By ``nice'' we mean that the singularities are not out of control.
By ``out of control'' we mean, regular singular. 
This means that in every sector $S_{t_0}(\alpha,\beta)$, if we approach the points $t=t_0$ with bounded angle of variation then the solution must have at worst a pole.
Here is a picture of such a sector:

\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.5]{sector.eps}
	\end{center}
	\caption{A sector used in the definition of regular singular. }
\end{figure}

In what follows we will let $S_{t_0}(\alpha,\beta) = \lbrace t \in \CC : \alpha<\arg(t-t_0) <\beta$ where $t_0 \in \CC$, $\alpha,\beta \in [0,2\pi]$ with $\alpha>\beta$ and $\arg$ the branch of the argument taking valued in $[0,2\pi)$.
We will also let $B_R(t_0)$ denote the open disc of radius $R$ centered at $t_0$.
A set of the form $S_{t_0}(\alpha,\beta) \cap B_R(t_0)$ will be called a bounded sector eminating from $t=t_0$, and a bounded sector contained in another bounded sector as an open set will be called a bounded subsector.
\iffalse 
\begin{definition}
Let $S = S_{t_0}(\alpha,\beta) \cap B_R(t_0)$ be a sector of bounded radius eminating from $t_0$ (which is by definition open and doesn't contain $t_0$).
We say that a matrix $\Phi(t) \in M_{m,n}(\hol(S))$ is \emph{regular singular at $t_0$} if and only if for all subsectors $S' \subset S$ of strictly smaller radius and angle there exists some integer $m$ such that 
 $$ \lim_{t \to t_0, t\in S'} \Vert \Phi(t) \Vert  = O( \vert t-t_0 \vert^{-m}).$$
 \end{definition}
\fi

\begin{definition}
	We say that $t=t_0$ is a \emph{regular singular point} if and only if  for every local sector at $t=t_0$ there exists a holomorphic basis of solutions $Y_1(t),\ldots,Y_n(t)$ with $Y_j(t)=(y_{j1}(t),\ldots,y_{jn}(t))^T$ and $\lambda \in \CC$ such that 
	$$ \lim_{t\to t_0} (t-t_0)^{\lambda} y_{ji}(t) =0. $$
	\taylor{Change for Next Round: convert this to a definition for systems}
\end{definition}

That seems like a lot but all this is saying is that as you approach your point in question you don't blow up like an essential singularity.
\begin{theorem}[Fuch's Criterion]
	Solutions of Fuchsian systems only have at worst regular singular points locally. 
\end{theorem}
\begin{proof}[Regularity Estimate Proof]
	The trick in this proof is to use the isomorphic $\CC^n \cong \RR^{2n}$ and make estimates as if the functions were real valued. 
	Here we will view $A(t)$ as a function to $M_{2n}(\RR)$ which is real analytic and a soltion $Y(t)$ as a function to $\RR^{2n}$ which is real analytic. 
	Also we can observe that a solution $Y(t)$ from this real analytic perspective has $\vert Y(t) \vert^2 = Y(t) \cdot Y(t)$. 
	Also, we let $Y'(t)$ denote it's usual real analytic derivative which coincides with its complex analytic one (after again changing the complex analytic one again to real analytic function). 
	We have 
	$$\dfrac{d}{dt}\left[ \ln \vert Y(t) \vert^2 \right] = \dfrac{2 Y'\cdot Y}{\vert Y\vert^2} = 2\frac{ (A Y)\cdot Y}{\vert Y \vert^2}.$$
	 Taking norms and using $\vert V \cdot W \vert \leq \vert V \vert \cdot \vert W \vert$ with $\vert A V \vert \leq \Vert A \Vert \cdot \vert V \vert $ we get  
	  $$ \vert \dfrac{d}{dt}\left[ \ln \vert Y(t) \vert \right] \vert  \leq \Vert A(t) \Vert \leq \frac{C_0}{\vert t \vert}$$ 
	 where in the last line we used the Fuchsian hypothesis.
	 This then gives along a given contour $\gamma(r) = e^{i\theta_0}(r_0-r)$ starting at $\gamma(r_0) = t_0 = e^{i\theta_0}r_0$ and ending at $t=e^{i\theta_0}(r_0-r)$ that $$ \ln\vert Y(t) \vert \leq C_1 + \int_{\gamma}\frac{C_0}{\vert s \vert } d\vert s\vert =  C_1 - \int_{0}^r \frac{C_0}{r-r_0} \vert e^{i\theta_0}dr\vert = C_1 - C_0 \ln \vert r-r_0 \vert $$ 
	 which implies that $\vert Y(t) \vert \leq e^{C_1} \vert t \vert^{-C_0}$.	
\end{proof}

\begin{proof}[Power Series Proof]
	\cite[Chapters III 1.3.1]{Borel1987} \taylor{This actually is a cleaner statement and we are going to replace that above with this discussion in a future version.}
\end{proof}




%%%%%%%%%%%%%%%%%%%%%%%
\section{Hilbert's 21st Problem}
%%%%%%%%%%%%%%%%%%%%%%%
We are now in a position to state Hilbert's 21st problem. 
The Monodromy map associated to every Fuchsian system on $\PP^1$ with poles contained in a finite set $T \subset \PP^1$ a representation of its fundamental group. 
In what follows $\Char_{\GL_n}(\PP^1\setminus T)$ denotes the set of representations of $\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ modulo conjugation and $\Sys_+^n(\PP^1\setminus T)$ denotes the collection of Fuchsian systems.
We have constructed a map
$$ \Sys_+^n(\PP^1\setminus T)  \to \Char_{\GL_n}(\PP^1\setminus T), \quad \mbox{(ODE) $\mapsto$ (Monodromy Rep)}.$$
Hilbert's 21st problem asks if this map of sets is surjective. 
\begin{problem}[Hilbert's 21st Problem]
	Is it the case that every representation $\rho:\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ comes from a Fuchsian differential equation with poles supported on $T$?
\end{problem}
This problem has a rather crazy history. 
The problem was first posed by Hilbert in 1900 during the International Congress of Mathematicians (ICM) during his famous ``Hilbert's Problems'' address.
In 1907 Plemelj\footnote{Nalini Joshi pronounces this ``Plum-ell-i'', I'm not sure how to pronounce this name so perhaps we should copy her.} published a positive answer to the question. 
In 1983, Treibich-Koch published a gap in the proof; it turns out that previous work from Dekkers in 1979 implies that the map is indeed surjective in the rank two case. 
Finally, in 1990, Bolibruch showed that the map is not surjective in rank higher than two disproving the conjecture.

In this section we are going to show that this conjecture is False as stated (Theorem~\ref{T:naive-rhc-false}) for simple dimension reasons.
We will then give Plemelj's construction is given in section \S\ref{S:plemelj}.
It turns out this construction relies on ``apparent singularities'' -- monodromy matrices which are trivial.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Representations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The representations $\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ are rather easy to describe. 
The key observation is that $\PP^1$ minus some points is homotopy equivalent to a bouquet of circles:
$$ \PP^1 \setminus \lbrace t_1,\ldots,t_n\rbrace \approx \underbrace{S^1 \vee \cdots \vee S^1}_{ \mbox{ $(n-1)$-times}}$$
where $\approx$ denotes homotopy equivalence and $\vee$ denotes the wedge product of topological spaces. 
A picture of this homotopy equivalence for $\PP^1 \setminus \lbrace 0,1,\infty\rbrace$ is given in Figure~\ref{F:bouquet}.
\begin{figure}[h]\label{F:bouquet}
	\begin{center}
		\includegraphics[scale=0.5]{bouquet.eps}
	\end{center}
	\caption{The figure shows $\PP^1\setminus \lbrace 0, 1, \infty\rbrace$ being deformed into $S^1 \vee S^1$. 
		The first step is the increase the size of the holes to make it look like a bowling ball. 
		We then wrap one of the holes completely around to get a disc with two interior discs removed. 
		This is then seen to be equivalent to a circle with a line through it. 
		After contracting the middle line one gets the bouquet of circles. 
	}
\end{figure}
The convenient description allows us to see that the fundamental group is just the free group on $(n-1)$ generators
$$\pi_1(\PP^1\setminus\lbrace t_1,\ldots,t_n\rbrace) \cong F_{n-1} \cong \langle \gamma_1,\gamma_2,\ldots,\gamma_{n-1} \rangle,$$ 
the generators then can be taken to be homotopy classes of loops around each of the points $t_1,\ldots, t_{n-1}$. 
The last loop $\gamma_n$ around $t_n$ satisfies the relation 
$$ \gamma_n \cdots \gamma_2 \gamma_1  = 1.$$
You can actually see this loop is trivial if you think about it a little bit. 

Anyway, with this description the representations $\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ are determined by tuples $(M_1,M_2,\ldots,M_{n-1}) \in \GL_n(\CC)^{n-1}$ modulo simultaneous conjugation by an element in $\GL_n(\CC)$.
Here $M_j = \rho(\gamma_j)$. 

To collection of representations of a fundamental group can be given the structure of a variety. 
This variety is the representation variety. 
To get representations up to isomorphism, we need to mod out by conjugation. 
After doing this we get the Character variety. 
\subsubsection{Representation Varieties}
The term ``representation variety'' is non-standard, I think. 
Let $\Pi$ be a finitely presented group. 
This means that 
$$ \Pi = \langle \gamma_1,\ldots,\gamma_{\ell} \colon R_1, \ldots, R_s \rangle$$
where the $R_j$ are relations of the form
$$ \gamma_{i_1}^{a_1} \gamma_{i_2}^{a_2} \cdots \gamma_{i_k}^{a_k} = 1,$$
for some $k\geq 0$, $\lbrace i_1,\ldots, i_k \rbrace \subset \lbrace 1,\ldots, \ell\rbrace$ and $a_1,\ldots, a_k \in \ZZ$. 
A representation of $\Pi\to \GL_n(\CC)$ is determined by $\gamma_i \mapsto M_i$ where $M_i \in \GL_n(\CC)$ satisfy relations of the type 
$$ M_{i_1}^{a_1} M_{i_2}^{a_2} \cdots M_{i_k}^{a_k} = I_n$$
where $I_n$ is the $n\times n$ identity matrix.
Viewing the entries of $M_{i}$ as variables we find get an algebraic variety $\Repn_{\GL_n}(\Pi)(\CC)$ whose points are in bijection with representations $\rho: \Pi \to \GL_n(\CC)$.

Note that the equations are actually defined over $\ZZ$ and the construction is quite general so $\Repn_{\GL_n}(\Pi)(\CC)$ is actually the $\CC$-points of a scheme $\Repn_{\GL_n}(\Pi)$ defined over $\CC$.
In fact,  one can replace any $\GL_n$ with $\SL_n$ or any other algebraic group $G$ to obtain a variety defined over any ring $R$ to get a group scheme over $R$, $\Repn_{G}(\Pi)$. 
In fact if $G_1 \subset G_0$ then we have 
$$ \Repn_{G_1}(\Pi) = (G_1)^{\ell} \cap \Repn_{G_0}(\Pi).$$
The scheme $\Repn_{G_1}(\Pi)$ is actually a fiber product for the following diagram
$$ \begin{tikzcd}
& \Repn_{G_0}(\Pi) \ar[d] \\
G_2^{\ell} \ar[r] & G_0^{\ell}
\end{tikzcd}.$$
We also want to mention that the construction is functorial in $\Pi$.
If $\Pi_0 \to \Pi_1$ is a morphism of finitely presented groups then there is an induced morphism $\Repn_G(\Pi_1) \to \Repn_{G}(\Pi_0)$.
If $w:\Pi_0 \to \Pi_1$ is given by sending a generator of $\Pi_0$ to a word in the generators of $\Pi_1$, $w\colon \gamma_i \mapsto w_i$ then $N=(N_0,\ldots,N_{\ell_1}) \in \Repn_G(\Pi_1)$ are mapped to $(w_1(N), w_2(N),\ldots,w_{\ell_0}(N)) \in \Repn_G(\Pi_0)$.
Since the map $w$ is a group homomorphism the matrices $w_1(N),w_2(N),\ldots,w_{\ell_0}(N)$ must necessarily satisfy the relations for $\Pi_0$.
\begin{definition}
	Given $\Pi$ a finitely presented group with $\ell$ generators and $G$ a group scheme over a ring $R$ the $R$-scheme $\Repn_{G}(\Pi) \subset G^{l}$ is called \emph{representation scheme} (or \emph{representation variety} of $G$ and $\Pi)$). 
	It is cut out by the relations imposed by the relations in the finite presentation.
\end{definition}

When $\Pi=\pi_1(X)$ for some complex manifold these are also called \emph{character varieties}. They are well-defined since the groups are well-defined up to conjugation. 
We use the notation
$$\Repn_G(\Pi) = \Repn_G(X).$$ 
This is again contravariant in $X$. 

\begin{example}
	In the case that $X=\PP^1\setminus \lbrace 0,1,x,\infty\rbrace$ then $\pi_1(X)=F_3$ the free group on three generators and the $\Repn_{\GL_2}(X) \subset \GL_2^4$ defined by the equation $M_0M_1M_2M_{\infty}=I_2$ which is just isomorphic to $\GL_2^3$.
\end{example}


\begin{example}
	Let $X$ be a genus two surface. 
	Then
	$$\pi_1(X) = \langle \alpha_1,\beta_1,\alpha_2,\beta_2 \colon \alpha_1\beta_1\alpha_1^{-1}\beta_1^{-1}\alpha_2\beta_2\alpha_2^{-1}\beta_2^{-1}=1\rangle,$$
	so $\Repn_{\GL_2}(X)\subset \GL_2(\CC)^4$ needs 16 variables and 4 equations. 
	\begin{figure}[h]
		\begin{center}
			\includegraphics[scale=0.33]{genus2.eps}
		\end{center}
		\caption{A picture of the generating cycles on a genus two compact real manifold of dimension two.}
	\end{figure}
\end{example}

\subsubsection{Character Varieties}

t $G$ be a group scheme.
Two representations $\pi_1(X) \to G$ are equivalent if and only if they are conjugate. 
Hence we have an action by $B\in\GL_n$ on $(M_1,\ldots,M_l)$ given by 
$$ B\cdot (M_1,M_2,\ldots,M_l) = (B M_1 B^{-1}, BM_2B^{-1},\ldots, BM_lB^{-1}). $$
We want to quotient by this action.
\begin{definition}
	The algebraic stack 
	$$\Char_{G}(\Pi) := [\Repn_{G}(\Pi)/G]$$ 
	is called the \emph{character stack}.
\end{definition}
When it is represented by a scheme, that scheme is unique up to isomorphism and we call it the \emph{character scheme}. 
In the case that it is a variety we call it the \emph{character variety}. 
We will abusively denote all of these things by $\Char_{G}(\Pi)$.

\begin{remark}
	The point of using an algebraic stack here is not to be a jerk but to state that quotients are a delicate mathematical issue that are often abused and that we need to contend with.
	For the uninitiated we mention that there are several theories available to us. 
	First there is the approach is Geometric Invariant Theory for which there is the famous book \cite{Mumford1994}. 
	The second there is theory of Algebraic Stacks for which there is an entire Stacks Project. 
	One can also use quotients in the category of sheaves too which is described here \cite[\href{https://stacks.math.columbia.edu/tag/07S5}{Tag 07S5}]{stacks-project}.
\end{remark}

We can actually prove that $\Char_G(\Pi)$ is represented by a scheme.
\begin{theorem}
	$\Char_G(\Pi)$ is an algebraic variety. 
\end{theorem}
\begin{proof}
	See Gunning's book \cite[Theorem 27]{Gunning1967}.
	Also see \cite[pg 149]{Iwasaki1991} for futher discussion.
\end{proof}

We can actually compute the dimension of this variety without knowing much. 
\begin{example}[$\GL_2$-Representations of $\PP^1\setminus S$]\label{E:rank2-repns}
	We will look at the case where $G= \GL_2$ and $X = \PP^1\setminus S$ where $S$ is a finite collection of points.
	We claim that $\dim(\Char_{\GL_2}(\PP^1\setminus S))=4(\vert S\vert -1)-3$.
	Here we use that $\pi_1(\PP^1\setminus S) = F_{\vert S \vert-1}$ the free group on $\vert S \vert-1$ letters.
	We have $\overline{G}=\GL_2(\CC)/\lbrace c I_2 \colon c\in \CC^{\times}\rbrace$ acting freely on the open subset $U$ of $\GL_2(\CC)^{\vert S \vert -1}$ corresponding to irreducible representations by Schur's Lemma (the only equivariant conjugation actions must be constants).
	Then we have
	\begin{align*}
	\dim( \Char_{\GL_2}(\PP^1\setminus S) )=& \dim(U) - \dim(\overline{G}) \\
	=& 4\cdot (\vert S \vert -1) -(\dim(\GL_2) -1) \\
	=& 4(\vert S \vert-1) -3,
	\end{align*}
	which proves our result.
\end{example}

As before, we let $\Char_G(X)=\Char_G(\Pi)$ for $\Pi=\pi_1(X)$ when $X$ is connected topological space with fundamental group $\Pi$.

The following example is the situation in the Hypergeometric case. 
\begin{example}[$\SL_2$-Representations of $\PP^1\setminus \lbrace 0,1,\infty\rbrace$]
	In the case of $X=\PP^1\setminus \lbrace 0,1,\infty\rbrace$ and $G=\SL_2$ the character variety is $\AA^3$:
	$$ \Char_{\SL_2}(\PP^1\setminus \lbrace 0,1,\infty \rbrace) \cong \AA^3.$$
	The representation space $\Repn_{\SL_2}(\PP^1\setminus \lbrace 0,1,\infty \rbrace)$ is determined by three matrices in $\SL_2$, $M_0,M_1,M_{\infty}$ satisfying $M_0M_1M_{\infty}=I_2$. 
	Using conjugation we can bring these into standard hypergeometric form with $(\theta_0,\theta_1,\theta_2)$ uniquely determining the monodromy matrices. 
	This corresponds to the fact that the Gauss hypergeometric function has three parameters $(a,b,c)$ see \S\ref{S:hypergeometric-case}.
\end{example}

We have another example for which I won't give a proof. 
You can read more about these spaces in Loray's note \href{https://arxiv.org/pdf/1503.06781.pdf}{here}.
The following example is the situation in the Painlev\'e case.
\begin{example}[$\SL_2$-Representations of $\PP^1\setminus \lbrace 0,1,\infty, x\rbrace$]
	In the case $X = \PP^1\setminus \lbrace 0,1,\infty,x\rbrace$ as before with $G=\SL_2$ then the representation variety $\Repn_{\SL_2}(X)\cong \SL_2^3$ since $\pi_1(\PP^1\setminus \lbrace 0,1,\infty,x\rbrace) \cong F_3$, the free group on three letters (in terms of monodromy matrices there are three $M_0,M_1,M_2,M_{\infty}$ and they satisfy $M_0M_1M_2M_{\infty}=1$ so we think of $M_{\infty}$ being determined by the first three). 
	
	
	It is a fact that the character variety $\Char_{\SL_2}(X)$ is a degree four hypersurface in $\AA^7$ with coordinates $(a,b,c,d,x,y,z)$ given by 
	$$a^2+b^2+c^2+d^2 + x^2+y^2+z^2 -(ab+cd)x-(ad+bc)y-(ac+bd)z + abcd + xyz - 4 = 0.$$
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Failure of Surjectivity of the Naive Riemann-Hilbert Morphism}
%%%%%%%%%%%%%%%%%%%%%%%
Building on what we have already done we can already show that the map

\begin{theorem}[Failure of Naive Hilbert's 21st]\label{T:naive-rhc-false}
	Let $T \subset \PP^1$ be a nonempty finite collection of points with $\vert T \vert \geq 3$.
	The naive Riemann-Hilbert map
	 $$ \Sys_+^2( \PP^1\setminus T) \to \Char_{\GL_2}(\PP^1\setminus T), \quad \mbox{(ODE) $\mapsto$ (Monodromy Rep)},$$
	from Fuchsian systems of rank two to rank two monodromy representations modulo conjugation, is not surjective.
\end{theorem}
\begin{proof}
	We are going to make a dimension count and show that this is impossible. 
	Let $T = \lbrace t_1,\ldots,t_m,\infty\rbrace$.
	First, order 1, rank 2 differential equations are equivalent to order 2 rank 1 differential equations by the theory of cyclic vector (\S\ref{S:cyclic-vectors}). 
	By Exercise~\ref{E:fuchsian-odes} and we know that order two Fuchsian differential equations take the form 
	 $$ y''(t) + a_1(t)y'(t) + a_2(t) =0 $$
	where $a_j(t)=b_j(t)/\prod_j^m(t-t_j)$ where $b_j(t)$ has degree at most $j(m-1)$.
	The coeffiecients of the polynomials $b_1(t),b_2(t)$ then provide a parameter space for the system of differential equations which shows 
	 $$ \dim(\Sys_+^2( \PP^1\setminus T) ) = (m-1) + 2(m-1) = 3m-3.$$
	On the other hand, from example~\ref{E:rank2-repns} we know that 
	 $$ \dim( \Char_{\GL_2}(\PP^1\setminus T)) = 4m-3.$$
	This then gives 
	 $$\dim( \Char_{\GL_2}(\PP^1\setminus T))-\dim(\Sys_+^2( \PP^1\setminus T) ) = m.$$
	Hence for $\vert T\vert = m+1$ bigger than 2 the map can't be surjective.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Painlev\'e VI and Monodromy}
%%%%%%%%%%%%%%%%%%%%%%%
We will look at the monodromy representations of $\PP^1\setminus \lbrace t_1,t_3,t_3,t_4\rbrace$ coming from rank two Fuchsian differential equations with singular locus $\lbrace t_0,t_1,t_3,t_3,t_4\rbrace$ with $t_0$ being an apparent singularity. 
We will show that in this situation:
 $$ \mbox{(dimensions of space of equations)} = 10, $$
  $$ \mbox{(dimensions of space of representations)} = 9, $$
which suggest that we should be able to move through the space of equations in one dimensional families and preserve the monodromy.
Such a deformation is called an isomonodromic deformation.

Let's first count the number of parameters involved in defining a differential equation
 $$ \dfrac{d^2y}{dt^2} + a_1(t) \dfrac{dy}{dt} + a_2(t)y=0,$$ 
with singular points $\lbrace t_0,t_1,t_3,t_3,t_5\rbrace$ such that the singularity at $t_0$ is ``apparent''. 
This means the solution of the ODE is meromorphic at $t_0$. 
Equivalently, loops around $t_0$ make no contribution to the monodromy representation and hence giving a representation 
 $$ \pi_1(\PP^1\setminus \lbrace t_1,t_2,t_3,t_4\rbrace) \to \GL_2(\CC).$$ 
Using the cross-ratio, we only need two parameters to describe the relative positions of the point. 
One can think of this also as sending $\lbrace t_0,t_1,t_2,t_3,t_5\rbrace$ to $\lbrace 0,1,\infty,x,z\rbrace$ using a Moebius transformation. 
In what follows we are going to assume $z$ is the apparent singularity so our associated monodromy representations take the form
 $$ \pi_1(\PP^1\setminus \lbrace 0,1,\infty,x\rbrace) \to \GL_2(\CC).$$
By Exercise~\ref{E:fuchsian-parameters}, the coefficients $a_j(t)$ require $j(m-1)$ parameters where $5=\vert T \vert =m+1$.
So $m=4$.
This gives 
 $$ \mbox{(number of coefficients)} = (1)(m-1) + (2)(m-1) = 3(m-1) = 9.$$
The coefficients are not independent by Fuch's Relation (Theorem~\ref{T:fuchs-relation}).
This cuts down the dimension by one giving us 
$$\mbox{(dimensions of space of equations)} = 2 \mbox{ [for $x$ and $z$]}+(9-1) \mbox{ [for the coeffs]} = 10. $$

We now count the representations.
These are given by $\pi_1(\PP^1\setminus \lbrace 0,1,\infty,x \rbrace) \to \GL_2(\CC)$ which is determined by three matrices in $\GL_2(\CC)$. 
We then mod out by conjugation which is an action of $\PGL_2(\CC)$. 
This gives 
$$ \dim(\Char_{\GL_2}(\PP^1\setminus \lbrace 0,1,\infty,x\rbrace)) = \dim( \GL_2(\CC)^3/\PGL_2(\CC) ) = (4)(3) - (4-1) = 9.$$

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gauge Transformations}
%%%%%%%%%%%%%%%%%%%%%%%
To get a full description of equivalence, it remain to describe the equivalence relation on differential equations.
Let $(R,\partial)$ be a $\partial$-ring and consider the equation
\begin{equation}\label{E:original-equation}
Y' = AY
\end{equation}
where $Y = (y_1,\ldots,y_n)$ and $Y' =(y_1',\ldots,y_n')$ and $A \in M_n(R)$. 
One can change coordinate in this differential equation and suppose that 
\begin{equation}\label{E:gauge-trans-one}
Y = \Phi \widetilde{Y}
\end{equation}
for some $\Phi \in \GL_n(R)$. 
In this situation we get a new equation 
\begin{equation}\label{E:gauge-transformed-equation}
\widetilde{Y}' = \widetilde{A} \widetilde{Y} 
\end{equation}
which is said to be \emph{gauge equivalent} to the previous equation. 
We will now compute what $\widetilde{A}$ is by plugging $Y = \Phi \widetilde{Y}$ into $Y'=AY$. 
We obtain $Y' = (\Phi \widetilde{Y})' = \Phi' \widetilde{Y} + \Phi \widetilde{Y}'$. 
We also obtain $AY = A\Phi \widetilde{Y}$. 
Putting these together gives $\Phi \widetilde{Y}' = A\Phi \widetilde{Y} - \Phi' \widetilde{Y}$ or 
$$ \widetilde{Y}' = \widetilde{A} Y, \qquad \widetilde{A} = \Phi^{-1} A \Phi - \Phi^{-1} \Phi '.$$
Both $Y \mapsto \Phi^{-1} Y$ and $A \mapsto A^{\Phi} := \Phi^{-1} A \Phi + \Phi^{-1} \Phi'$ are called \emph{gauge transformations} and define right group actions of $\GL_n(R)$ on $R^{\oplus n}$ and $M_n(R)$.
The equations \eqref{E:original-equation} and \eqref{E:gauge-transformed-equation} are called \emph{gauge equivalent}.

For holomorphic and meromorphic linear systems we can consider holomorphic and meromorphic gauge transformations. 
These gauge transformations can be local or global. 
What is interesting is that sometimes we can take a meromorphic linear systems and then convert it into a holomorphic linear systems by some meromorphic gauge transformation. 
In the case that we can do this the singularities of the original linear system are called \emph{apparent singularities}. 

\begin{example}
	Consider the linear system 
	\begin{equation}\label{E:apparent-singularity} 
	Y' = \begin{pmatrix}1 & \frac{1}{t^2} -\frac{2}{t} \\ t^2 & 0 \end{pmatrix} Y
	\end{equation}
	which is holomorphic on $\CC\setminus \lbrace 0 \rbrace$. 
	The singularty at $t=0$ is actually just apparent as it is gauge equivalent to the system
	$$ \widetilde{Y}' = \begin{pmatrix} 1 & 1 \\
	1 & 1 
	\end{pmatrix} \widetilde{Y}.$$
	To see this one uses a meromorphic gauge transformation.
	The point here is that the singularities of \eqref{E:apparent-singularity} are just apparent and that they can be removed by using 
	$$ Y =\begin{pmatrix}t^2 & 0 \\ 0 & 1 \end{pmatrix} \widetilde{Y}. $$
	As an exercise one needs to compute 
	$$ \widetilde{A} = \begin{pmatrix} \frac{1}{t^2} & 0 \\
	0 & 1\end{pmatrix} \begin{pmatrix}1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} t^2 & 0 \\ 0 & 1 \end{pmatrix} - \begin{pmatrix} \frac{1}{t^2} & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 2t & 0 \\
	0 & 0 \end{pmatrix}.$$
	which comes from the formula for gauge transformations.
\end{example}

The most important example is the case where $\Phi$ is a fundamental matrix. 
\begin{example}[Equivalence to Trivial Equation]\label{EX:equivalence-to-trivial}
	Consider a linear differential system $Y'=AY$ over a differential ring $(R,\partial)$. 
	Suppose that the system admits a fundamental matrix $\Phi \in \GL_n(R)$. 
	Then setting $Y = \Phi \widetilde{Y}$ we find that 
	$$ A^{\Phi} = \Phi^{-1} A \Phi - \Phi^{-1} \Phi' = \Phi^{-1}( A\Phi - \Phi') =0 $$
	and so that system is gauge equivalent to the trivial system 
	$$ \widetilde{Y}' =0.$$
\end{example}
It is important to observe that there usually aren't \emph{global} fundamental matrices.
This is what prevents us from trivializing all differential equations.

%%%%%%%%%%%%%%%%%%
\section[Classification of Fuchsian Equations]{Classification of Fuchsian Differential Equations on $\PP^1$}
%%%%%%%%%%%%%%%%%%
We are going to show that every Fuchsian differential system on $\PP^1$ with polar locus $S = \lbrace a_1,a_2,\ldots, a_m\rbrace \subset \PP^1\setminus \lbrace \infty \rbrace$ takes the form  
 $$ Y' = A(t) Y, \qquad A(t) = \frac{A_1}{t-a_1} + \cdots + \frac{A_m}{t-a_m},$$
where $A_1,A_2,\ldots, A_m \in M_n(\CC)$ are constant matrices.
To do this we first need to review some facts about residues and Riemann surfaces.


%%%%%%%%%%%%%%%%
\subsection{Some Reminders About Riemann Surfaces}
%%%%%%%%%%%%%%%%
Riemann Surfaces are just topological spaces equipped with a system of holomorphic charts that make them locally isomorphic to open subsets of $\CC$. 
A description of these charts for $\PP^1$ is given in Figure~\ref{F:projective-line}.
This allows us to make sense of what a holomorphic map is and make sense of what computations ``at infinity'' are. 

\begin{figure}[h]\label{F:projective-line}
	\begin{center}
		\includegraphics[scale=0.33]{projective-line.eps}
	\end{center}
	\caption{The projective line $\PP^1$ is isomorphic to the Riemann sphere $S^2$ and is composed of two coordinate charts. 
		The first chart we think of as the ``usual'' copy of $\CC$ (which algebraic geometers upgrade to the affine line $\AA^1$) which has coordinate $t$. 
		Then when we want to set $t=\infty$ we use another copy of $\CC$ with coordinate $s$ where $s=1/t$.
		The point $s=0$ corresponds to the points $t=\infty$ and we use this $s$ coordinate to do all of our computations at infinity. }
\end{figure}

One can upgrade this $\CC$ to $\CC^n$ an get a category of complex manifolds. 
It turns out that the category of compact Riemann surfaces equivalent to the category of smooth projective algebraic curves over $\CC$.
These are curves which are cut out by homogeneous polynomial equations in some complex projective space $\PP^n$. 
The same is not true for higher dimensional compact complex manifolds, there exists compact complex manifolds which aren't projective varieties (see for example \cite[pg 161]{Shafarevich2013}). 
It is true however, a theorem called Chow's theorem, that every compact complex manifold embedded into complex projective space is a projective variety (i.e. it is cut how by homogeneous equations). 

The case of complex projective curves (or equivalently Riemann Surfaces) is especially nice because this category is equivalent to the category of fields $K/\CC(t)$ which are algebraic. 
In the case of connected Riemann surfaces $X$the naturally assocaited field is the field of meromorphic functions on $X$ which we denote by $\Mer(X)$. 
In the case of projective curves $C$ the naturally associated field is the function field $\kappa(C)$.
Miraculously they are isomorphic even though they have drastically different descriptions away from the case of $X=C=\PP^1$. 
This case is rather easy to describe. 

We will prove that $\Mer(\PP^1) = \CC(t)$ which is easily seen to be the fraction field $\CC[t]$ of the polynomial functions on one of its open sets. 
We write $\PP^1 = U_0 \cup U_{\infty}$ and note that some $f \in \Mer(\PP^1)$ has finitely many poles on $U_0$. 
This means there exists a polynomial $g(t)$ such that $f(t)g(t)$ is entire. 
Since $f(t)$ is meromorphic and $g(t)$ is meromorphic the order of vanishing at infinity is finite. 
Here $\ord_{t=\infty} ( f(t) g(t)) = \ord_{s=0}(f(1/s)g(1/s) )$. 
This means that $f(t)g(t) = g(t) \in \CC[t]$. 
Hence $f(t) = h(t)/g(t)$ which proves that every Meromorphic function is rational.

In general $\Mer(X)$ is a finite algebraic extension of $\CC(t)$. 
To give an idea of how different-looking $\Mer(X)$ and $\kappa(X)$ can be consider the case of an elliptic curve $E$. 
As a Riemann surface we like to describe this as $\CC/\Lambda$ for some lattice $\Lambda \subset \CC$. 
In this situation, we have $\Mer(E) = \CC(\wp_{\Lambda}(t), \wp_{\Lambda}'(t))$ where $\wp_{\Lambda}(t)$ is the Weierstrass $\wp$-function associated to the lattice $\Lambda$. 
In the case where we want to present $E$ algebraically, then away from $\infty$ (some curves may have more than one ``point at infinity'' just not the traditional presentations of $\PP^1$ and $E$) we have $E \subset \CC^2$ given by the equation $y^2 = x^3+ax+b$ for some $a,b\in \CC$. 
Here we are using $(x,y)$ for complex coordinates. 
The crazy part is that there is a map $\CC/\Lambda \to E$ given by $x = \wp(t)$ and $y=\wp'(t)$ which gives the isomorphism. 
Here the point $0 \in \CC/\Lambda$ maps to $\infty$ in the projective model of the elliptic curve. 

\subsection{The Residue Theorem For Meromorphic Differential Forms}\label{S:residue-theorem}
We will need the following theorem about the sum of residues being zero later as we try to classify Fuchsian equations.
Here we briefly recall that for any meromorphic differential $\omega$ on a compact Riemann surface $X$ we can find a local parameter $t=t_b$ at $b\in X$ and then write $\omega$ as $f(t)dt$. \footnote{In coordinates on say $\CC$ the local parameter for $b\in  \CC$ is $t_b=t-b$ where $t$ is the usual complex variable. }
We can then develop $f(t)$ in a Laurent series to get 
$$ \omega =  \left(\frac{a_{-n} }{t^n} + \cdots + \frac{a_{-1}}{t} + a_0 + a_1 t + \cdots  \right) dt$$
and define the residue at $b$ by the usual formula 
$$ \res_{t=b}(\omega) = a_{-1}.$$
We will extend this to vector valued differential forms $A(t)dt$  by doing this component by component and taking the residues there.
\begin{theorem}[Residue Theorem]\label{T:residue-theorem}
	Let $\omega$ be a meromorphic differential on a compact Riemann surface $X$. 
	Then $ \sum_{a \in X} \res_{t=a}(\omega) =0.$
\end{theorem}
\begin{proof}
	We give a proof in the case that $X=\PP^1$. 
	A complete proof can be found at \cite[Proposition 6.6]{Schlag2014} and those notes can be found online as of 2022 by a simple Google search.
	
	The basic idea as depicted in figure \ref{F:sum-of-residues} is to take a simple closed contour $\gamma_1$ and its opposite contour $\gamma_2$ and realize that on one hand 
	$$ \int_{\gamma_1}\omega + \int_{\gamma_2} \omega =0 $$
	while on the other hand we have the classic residue theorem from complex analysis for each of these integrals
	$$ \int_{\gamma_1}\omega + \int_{\gamma_2}\omega = 2\pi i \sum_{b \in \PP^1} \res_{t=b}(\omega).$$
	\begin{figure}[h]\label{F:sum-of-residues}
		\begin{center}
			\includegraphics[scale=0.5]{sum-of-residues.eps}
		\end{center}
		\caption{One uses the basic residue theorem on two simple integrals which are opposite of each other to prove the residue theorem on $\PP^1$. }
	\end{figure}
\end{proof}

For $A(t) = (a_{ij}(t)) \in M_n(\CC((t)))$ we will do Laurent series developments entry-by-entry and write 
$$ A(t) = \sum_{j=-\infty}^{\infty} A_j (t-t_0)^j, \qquad A_j \in M_n(\CC).$$
For entries which are truely meromorphic, then for closed curves $\gamma$ we will have  
$$\int_{\gamma} A(t)dt = (\int_{\gamma}a_{ij}(t)dt).$$ 
As above if $A(t)dt$ is a matrix of meromorphic differential forms with poles at $t_1,\ldots,t_m$ on a Riemann surface $X$ with residues $R_j$ for $1\leq j \leq m$ then we get
$$ \sum_{j=1}^m R_j =0.$$

\begin{corollary}
	The sum of the residues of meromorphic matrix valued differential forms is zero.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification of Fuchsian Differential Equations on $\PP^1 \setminus S$}
%%%%%%%%%%%%%%%%%%%%%%%
Fuchsian differential equations on $\PP^1$ have a very simple form.
For a polar locus $S = \lbrace s_1,\ldots,s_m\rbrace$ we will often assume that $S$ takes the form $S =\lbrace 0,1,\infty,s_4,\ldots,s_m \rbrace$ which we can do by using a M\"{o}bius transformation. 
We can also if we want assume that the polar locus not contain $\infty$. 
\begin{theorem}
	Consider a Fuchsian differential equation on $\PP^1$,
	$$ Y' = A(t) Y, \qquad A(t) \in M_n(\CC(t)).$$
	If $A(t)$ has polar locus $S=\lbrace s_1,\ldots,s_m\rbrace \subset \PP^1$ not containing infinity then 
	$$ A(t) = \frac{A_1}{t-s_1} + \cdots + \frac{A_m}{t-s_m} $$
	where $A_j \in M_n(\CC)$ and $A_1+\cdots + A_m=0$.
\end{theorem}
\begin{proof}
	We can suppose without loss of generality that there are no poles at infinity. 
	To prove this one needs to recall that by the Mittag-Leffler theorem \cite[Proposition 2.19]{Schlag2014 } if $f(z)$ is meromorphic on $\CC$ with poles at $s_1,\ldots,s_m$ then there exists $p_j(z) \in \CC[z]$ polynomials such that 
	$$ f(z) - \sum_{j=1}^m p_j( \frac{1}{z-s_j}) $$
	is entire and the degree of $p_j$ is the order of the pole of $f$ at $z=s_j$.
	In our application we have that $A(t)$ has at most a pole at each $s_j$. 
	Hence there exists some matrices $A_1,\ldots, A_m \in M_n(\CC)$ such that the components of  
	$$B(t) = A(t) - \frac{A_1}{t-s_1} - \cdots - \frac{A_m}{t-s_m} $$
	are holomorphic on $\CC$. 
	Also note that $A_j/(t-s_j) = A_js/(1-ss_j)$ is also holomorphic at $s=0$ or $t=\infty$. 
	This means that $B(t)$ is entire and bounded and hence constant. 
	But we know that $\lim_{t\to s_j}B(t) =0$ by construction which means that it must be the constant function zero. 
	
	The second part about the sum of the residues being zero follows from the Residue theorem (\S \ref{S:residue-theorem}) but doing it component by component in the matrix $A(t)dt$.
\end{proof}

The residue matrices are so important we give them a name.
They are called the \emph{local exponents} of the linear differential equation. 
We will see that if $\rho$ is a local exponent of $A(t)/t$ where  $A(t)\in M_n(\CC[[t-t_0]])$ at $t=t_0$ with the property that $\rho+r$ is not an eigenvalue for any integer $r>1$ then the system admits a solution of the form $Y(t) = (t-t_0)^{\rho}Z(t)$ where $Z(t) \in \CC[[t]]^n$. 
If $A(t)$ is holomorphic then $Z(t)$ will be holomorphic.

We conclude this subsection with a classification of equations with one, two, three, and four singular points. 
The case of three singular points will end up leading to the theory of hypergeometric differential equations. 
The case of four singular points ends up leading to the theory of isomonodromic deformations and $P_{VI}$ the 6th Painlev\'{e} equation.

In the following examples the singular locus $S = \lbrace s_1,\ldots,s_m \rbrace \subset \PP^1$ can be taken without loss of generality to be $S=\lbrace 0,1,\infty,s_4,\ldots,s_m\rbrace$ since any three points can map to any other three points by a M\"obius transformation. 
\begin{example}[one singular point]
	Consider a Fuchsian differential equations with $S = \lbrace 0 \rbrace$. 
	Then we have 
	 $$ \dfrac{dY}{dt}= \frac{A_0}{t} Y $$
	for some constant matrix $A_0 \in M_n(\CC)$. 
	We then can use the chart at infinity $\partial_t = -s^2 \partial_s$ to conclude that the equation becomes $-s^2 \dfrac{dY}{ds} = s A_0 Y$ which gives 
	 $$ \dfrac{dY}{ds} = -\frac{A_0}{s}Y,$$
	which is not holomorphic at $\infty$ unless $A_0 = 0$. 
	Hence every such system is equivalent to 
	$$Y'=0.$$
\end{example}

\begin{example}[two singular points]
	Consider a Fuchsian differential equation with polar locus $S = \lbrace 0, \infty \rbrace$. 
	From the previous example we see that it has the form 
	 $$ \frac{dY}{dt} = \frac{A_0}{t} $$
	and that $A_0 = -A_{\infty}$.
\end{example}

The case of three singular points is sometimes called the Gauss case of the hypergeometric case because of its connections to the hypergeometric differential equations.
\begin{example}[three singular points]
	Consider a Fuchsian differential equation with polar locus $S= \lbrace 0, 1,\infty\rbrace$. 
	Such an equation has the form
	 $$ \frac{dY}{dt} = \left ( \frac{A_0}{t} + \frac{A_1}{t-1}\right) Y$$
	where $A_0 + A_1 + A_{\infty} =0$.
	Explicitly after changing coordinates to the chart at infinity (letting $t=1/s$) we find
	 $$ -s^2 \dfrac{dY}{ds} = \left ( \frac{A_0}{1/s} + \frac{A_1}{1/s-1}\right) Y $$
	which implies 
	 $$ \dfrac{dY}{ds} =- \left ( \frac{A_0+A_1}{s} + \frac{A_1}{1-s}\right) Y,$$
	and we can see $A_{\infty} = -A_0-A_1$ explicitly. 
	
	In section \S\ref{S:hypergeometric} we will show every rank two Fuchsian equation with polar locus $S=\lbrace a,b,c\rbrace \subset \PP^1$ can be reduced to the Gauss hypergeometric equation in a single dependent variable 
	$$ t(t-1)y'' +(c-(a+b+1)t)y'-aby =0.$$
\end{example}

The case of four singular points is sometimes called the Painlev\'e case because of its connections to the Painlev\'e equations.
\begin{example}[four singular points]
	Every Fuchsian differential equation with polar locus containing four points now cannot be normalized to a standard set of points.
	We can bring the first three points of $S$ to $0,1,\infty$ but a third point $\lambda\in \CC$ remains. 
	We will have $S=\lbrace 0,1,\infty, \lambda\rbrace$ and the differential equation will take the form 
	 $$ \dfrac{dY}{dt} = \left( \frac{A_0}{t} + \frac{A_1}{t-1} + \frac{A_{\lambda}}{t-\lambda}\right) Y $$
	where $A_0,A_1,A_{\lambda}\in M_n(\CC)$ and we define $A_{\infty}$ by the sum of the residues being zero $A_0+A_1+A_{\lambda}+A_{\infty}=0$. 
	
	A fun game to play here will be to determine the conditions under which we may vary $A_{\lambda}$ as a function of $\lambda$ and preserve the monodromy representation. 
	Note that $\pi_1(\PP^1\setminus \lbrace 0,1,\infty,\lambda_1\rbrace) \cong \pi_1(\PP^1\setminus \lbrace 0,1,\infty,\lambda_2\rbrace)$ for every pair of $\lambda_1$ and $\lambda_2$ so it makes sense to ask for monodromy representations to change. 
	Such deformations are called \emph{isomonodromic}.
	The criterian for deformations to be isomonodromic are given by Schlesinger's equations for matrices which give rise to the Painlev\'e equations. 
\end{example}


\section[Hypergeometric Equations]{Hypergeometric Differential Equations: Fuchsian Differential Equations of rank two on $\PP^1\setminus \lbrace 0,1,\infty\rbrace$}

 \label{S:hypergeometric}

The Gauss hypergeometric equation is the following homogeneous ordinary differential equation
\begin{equation}
   y'' + \frac{(a+b-1)t-c}{t(1-t)} y' + \frac{ab}{t(1-t)} y =0 .
\end{equation}
It solutions are so-called hypergeometric functions and has the remarkable property that any rank two Fuchsian differential equations on $\PP^1$ can be reduced to this equation for some collection of parameters $(a,b,c)$. 
The $(a,b,c)$ really encode eigenvalues of the residue matrices (=local exponents) and the form of the equation is really a consequence of the restriction of these local exponents in part due to Fuch's theorem which says that the sum of the local exponents in this rank 2 case with three singularities must be equal to one. 

\begin{exercise}[Hypergeometric Exponents]\label{E:exponents-for-hypergeometric}
	Show that the local exponents of the hypergeometric differential equation with parameters $(a,b,c)$ fall into the following table:
	$$\begin{tabular}{ccc}
		$0$ & $1$ & $\infty$  \\
	\hline \hline	$0$ & $0$ & $a$ \\
	$1-c$ & $c-a-b$& $b$
	\end{tabular}.	$$
	This is sort of a hard computation now but gets easier once more tools are developed in the rest of the section. 
	I would try it now for 30 minutes, then try it again once you have the indicial equation, then revisit it once more once you have Fuchs' relation.
\end{exercise}

The first mystery is showing that $n\times n$ first order linear systems can actually be reduced to an order $n$ equation in one dependent variable.
This is sort of the opposite of taking an equation of high order and reducing it to an equation a low order but in more variables. 
The key to this is the theory of $D$-modules. 
We will show that linear systems gives rise to a $D$-modules and conversely any $D$-module with a choice of basis gives a linear differential equation. 
Changing the basis will changes the differential equation by a gauge transformation. 
Now knowing that $D$-modules encode linear differential equations we apply Katz's theorem and show that $D$-modules will admit so called cyclic vectors.
In the case of rank two Fuchsian differential systems with three poles this reduces our equation to an order two equation linear equation in one dependent variable.

Finally, once we are in the order two case we need to show that all of our equations are determined by the local exponents and that we can manipulate these exponents by a series of gauge transformations and automorphisms of $\PP^1$ to bring our general equations into the Gauss hypergeometric case.
This involves a basic lemma about how local exponents change under Gauge transformations of the form $Y(t) = t^{\rho} \widetilde{Y}(t)$. 

\subsection{Weyl Algebras and $D$-Modules}\label{S:weyl-algebras}

Let $(R,\Delta)$ be a $\Delta$-algebra. 
\begin{definition}
	A \emph{Weyl algebra} associated to $(R,\Delta)$ is the ring $R[\Delta]$ of linear operators on associated to $(R,\Delta)$. 
	It is the non-commutative ring $R[\partial \colon \partial \in \Delta]$ where one has
	$$ \partial a = a \partial  + \partial(a), \qquad a \in R, \partial \in \Delta.$$
	One also has $\partial_1 \partial_2 = \delta_2 \delta_1$ for $\partial_1,\partial_2 \in \Delta$.
\end{definition}

The idea behind the formula $\partial a = a \partial + \partial(a)$ comes from looking a $a \in R$ when viewed as an element $a \in R[\Delta]$ as the linear operator ``multiplication by $a$''.
In this situation we have 
$$ (\partial a) \cdot f = \partial( af) = \partial(a)f + a \partial(f) = [\partial(a) + a \partial]\cdot f,$$
which justifies the rule. 

\begin{definition}
	A \emph{$D$-module} is a $R[\Delta]$-module. 
	We will simply call these $R[\Delta]$-modules. 
\end{definition}

Authors like to get cutesy with the above definition and it is worth pointing some things out. 
First, many authors define $\mathcal{D} = R[\Delta]$ and then talk about $\mathcal{D}$-modules. See for example Singer and van der Put. 
Some authors only define Weyl-algebras are for polynomial rings and refer to this particular Weyl algebra as \emph{the} Weyl algebra. 
In this case they take $R = \CC[x_1,\ldots,x_m]$ with $\Delta = \lbrace \partial_{x_1},\ldots, \partial_{x_m} \rbrace$ and then only talks about Weyl algebras (as we have defined above) as the only Weyl algebras.
This is useful when searching the literature for propositions about Weyl algebras that you need.
Finally, many authors restrict to the case $\Delta = \lbrace \partial \rbrace$ which will be the case we are interested in mostly and call these $\partial$-modules. 
In this case some authors (like Nick Katz) like to define $D$ as the derivation operator on the module $V$ which satisfies $D(av) = \partial(a) v + a D(v)$ for $v\in V$ and $a \in R$.
I reserve the right to use a mixture of these perspectives (and you should too).

\subsubsection{A Bosonic Fock Space and Weyl Algebras}
A frequently used physical perspective of $D$-modules that occurs is the following. 
Let  
 $$ B=\CC[x_1,x_2,\ldots][\partial_1,\partial_2,\ldots], \quad \partial_j = \dfrac{\partial}{\partial x_j}.$$
This is just a usual Weyl algebra but in countably many variables. 
In some portions of the algebraic theory of differential equations literature \cite{Sato1983, Miwa2000}, one uses the terminology of creation and annihilation operators,
 $$ a_n = \partial_n =\mbox{(annihilation operator)}, \quad a_n^* = x_n = \mbox{ (creation operator) },$$
and observes that these satisfy the rules 
 $$ [a_n,a_m]=0, \quad [a_m^*,a_n^*]=0, \quad [a_m,a_n^*]= \delta_{mn}, \qquad m,n \in \ZZ.$$
From this viewpoint we think of $a_n$  and $a_n^*$ as acting on a space of functions which create and annihilate bosonic particles on the $\ZZ$ and the $D$-module (which is a $B$-module in this case) $\CC[x_1,x_2,\ldots]$ of polynomials in infinitely many indeterminates is called the bosonic Fock space. 
The idea here is that particles along the lattice $\ZZ$ are built-up by acting by these creation and annihilation operators on $1\in \CC[x_1,x_2,\ldots]$ which is called the \emph{vacuum state}.

None of this is really important at the moment and we just say this so that the reader can recognize in the literature that people talking about Bosonic Fock Spaces are really just talking about Weyl algebras. 

\begin{remark}
For the uninitiated we mention that Bosons are particles that mediate the exchange of forces in physics.\footnote{with the exception of the Higgs boson.} 
There are a set of special Bosons for each of the fundamental forces: the photon the $\gamma$-boson for the electromagnetic force; the weak force for three bosons the $W^+$-boson, the $W^-$-boson, and the $Z$-boson -- the $W$-bosons carry a charge and the $Z$-boson does not; the strong force has  six bosons called gluons.

In addition to there there is a Higgs boson which gives particles mass by the so called Higgs Mechanism.
\end{remark}


\subsubsection{Fermionic Fock Spaces}
Recall that for every quadratic space $(V,q)$ we can associated a Clifford algebra $\Clf(V,q)$ where $\Clf(V,q) = T(V)/I_q$ and $I_q$ is generated by $v^2 = q(v)$ and $T(V) = \bigoplus_{n\geq 0} V^{\otimes n}$ is the tensor algbra (so $v^2$ really is $v\otimes v$ but we are dropping the $\otimes$ to make notation simple).
For any such $(V,q)$ there is an associated bilinear form $2B_q(v+w) = q(v+w)-q(v)-q(w)$ and we have the identity for $v,w \in V \subset \Clf(V,q)$ given by 
 $$ \lbrace v,w\rbrace = vw+wv = 2B_q(v,w).$$
When $2$ is invertible in your base ring the data of a bilinear form or quadratic form are equivalent. 

We now perform our construction of the ring of so-called Grassmann numbers.  
Let $V$ be a countably generated vector space with a basis $v_i$ for $i\in \ZZ^*$. 
Let $W$ be a countably generated vector sapce with a basis $w_i$ for $i\in \ZZ^*$. 
We then give a bilinear form $B_q$ on $V\oplus W$ given by
$$ B_q(v_i,v_j)=0, \quad B_q(w_i,w_j)=0, \quad B_q(v_i,w_j) = \begin{cases}
0, & i+j\neq 0 \\
\frac{1}{2}, & i+j=0
\end{cases}.$$
This gives a quadratic space and we have the description as a clifford algebra being given by 
$$ A = \Clf( V\oplus W, q).$$
For some reason, we then use the notation $v_n=\psi_n$ and $w_n=\psi^*_n$ for $n\in \ZZ^*$ and observe the identities:
 $$\lbrace \psi_m,\psi_n \rbrace =0, \quad \lbrace \psi_m^*,\psi_n^* \rbrace =0, \quad \lbrace \psi_{m}^*,\psi_n^*\rbrace = \delta_{n+m,0}, \qquad m,n\in \ZZ^*.$$
Traditionally, the $\psi_n$'s and $\psi_n^*$'s are indexed by $m,n \in \frac{1}{2}+\ZZ$ but we are going to forego this tradition because these are math notes and physics notation is silly. 
Note in particular that $\psi_n^2=0$ and $(\psi_n^*)^2=0$.
Mathematically the ring generated by $\psi_n$ for $n \in \ZZ^*$ is the exterior algebra of a countably generated free module and the same can be said for $\psi_n^*$. 

\begin{remark}
For the uninitiated we mention that Fermions are particles like electrons on which forces act. 
Several behave very similarly to electrons mathematically and these are called leptons (12 in total). 
First every lepton has an anti-particle. 
For the electron this is a positron. 
These mathematically are pretty much identical but with time reversed (yes, weird, but mathematically simple). 
Then there are ``heavier electrons'' called muons and tauons which are like electrons but, well,  heavier.
Then there are the baby versions where are called neutrinos.
There are electron neutrinos, muon neutrinos, and tauon neutrinos.
These also have antiparticles. 

The other six elementary Fermions in the standard model of 2022 are the quarks which are associated with the strong force. These are the cute sounding up, down, strange, charm, top, and bottom quarks.
\end{remark}

\subsection{Linear Systems and $D$-Modules}
There is a procedure for converting between linear differential equations and $D$-modules which will be useful that we will now explain. 
In this subsection we will restrict to the case of a single derivative.

Given a rank $n$ linear system over $(R,\partial)$ given by 
$$ Y' = A Y, \qquad A = (a_{ij}) \in M_n(R), $$
we can define a $D$-module structure on $V=R^{\oplus n}$.
Let $e_1,\ldots,e_n$ be a standard basis for $V$.
Then we define 
$$ D(e_j) = \sum_{i=1}^n a_{ij} e_i.$$
Let $V_0 = (R^{\partial})^{\oplus n}$. 
We now have a $R^{\partial}$-linear operator on $V_0$ and we extend this to all of $V$ by specifying 
$$ D(b v_0) = \partial(b)v_0 + b D(v_0), \quad v_0 \in V_0, b \in R. $$
\begin{exercise}
	Check that this is well defined.
	This means that if $bv_0 = cw_0$ for some other $c \in R$ and $w_0 \in V_0$ then $D(bv_0) = D(cw_0)$. [This is a silly easy problem.]
\end{exercise}

Conversely, given a $D$-module structure on $V = R^{\oplus n}$ one then takes a basis $v_1,\ldots, v_n$ and finds that 
$$ D(v_j) = \sum_{i=1}a_{ij}v_i $$
for some $a_{ij}\in R$. 
This allows us to set up a linear differential equation 
$$ Y' = AY, \qquad A = (a_{ij}) \in M_n(R).$$
One then finds that the linear differential equation associated to the $D$-module is again the $D$-module with $v_1,\ldots,v_n$ identifying with the standard basis vectors. 

If instead we had chosen a different basis one can check that one will obtain a new differential equation
$$ \widetilde{Y}' = \widetilde{A} \widetilde{Y} $$
which is gauge equivalent to the first equation. 
This gives us a procedure for assigning a linear system of rank $n$ over $R$ (up to gauge equivalent) to every $R[\partial]$-module $V$ of finite rank $n$.
The point here is that change of basis of the $D$-module is gives rise to a gauge transformation of the associated linear system.

\begin{exercise}
	Show that indeed a change of coordinates on the $R$-module induces a gauge tranformations of the linear differential equation.
\end{exercise}

\subsection{Cyclic Vectors and Katz's Theorem}\label{S:cyclic-vectors}
In order to convert first order linear systems of rank $n$ into linear differential equations of  order $n$ in a single variable we need the notion of a cyclic vector. 
\begin{definition}
	An $R[\Delta]$-module $V$ is \emph{cyclic} if and only if there exists some $v \in V$ such that $V = R[\Delta]\cdot v$. 
	Such a vector $v \in V$ where $V = R[\Delta]\cdot v$ is called a \emph{cyclic vector}.
\end{definition}

In the case that $\Delta = \lbrace \partial \rbrace$ and $V \cong R^n$ a $R[\partial]$-module a vector $v \in V$ is cyclic if and only if 
$$ v, \partial(v), \ldots, \partial^{n-1}(v) $$
form a basis for $V$. 
This is probably the most important case. 
Before proving such cyclic vectors exist, lets take a moment to realize our goal reducing a first order linear system of rank n to an order n linear differential equation in a single dependent variable. 

Following our procedure we set $v_0 = v$ and $v_i = \partial^i(v)$ which gives use $\partial(v_i) = v_{i+1}$ for $0 \leq i \leq n-2$ and then $\partial(v_{n-1}) = b_0v_0 +b_1v_1 + \cdots + b_{n-1} v_{n-1}$ for $b_i \in R$ and we get the linear systems 
$$ Y' = BY, \qquad B=\begin{pmatrix} 0 & 1 & 0&\cdots & 0 \\
0 & 0 & 1 &\cdots & 0 \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
0& 0 & 0 & \cdots & 1 \\
b_0 & b_1 & b_2 & \cdots & b_{r-1} \\
\end{pmatrix}$$
which gives the linear differential equation 
$$ y^{(n)} = b_0 y+b_1y' + b_2 y''+ \cdots + b_{n-1}y^{(n-1)}.$$

The following Theorem can be found in \cite{Katz1987} (which is just five pages including citations). 
\begin{theorem}[Katz's Theorem]\label{T:katz}
	Let $(R,\partial)$ be a $\partial$-ring with $t\in R$ satisfying $\partial(t)=1$.
	Let $V$ be a free $R$-module of finite rank $n$ which has the structure of a $R[\partial]$-module. 
	Then if $R$ is local, and $(n-1)!$ is invertible in $R$ then $V$ admits a cyclic vector of the form
	$$ v = \sum_{j=0}^{n-1} \frac{(t-a)^j}{j!} \sum_{i=0}^j { j \choose i } D^i(e_{j-i}) $$
	where $a \in R^{\partial}$ and $e_1,\ldots,e_n$ is the standard elementary basis for $R^n$.
\end{theorem}
We will prove this for $R=\CC[[t]]$. 

\begin{lemma}\label{L:katz}
	Let $V=\CC[[t]]^{\oplus n}$ be a $D$-module. 
	\begin{enumerate}
		\item Let $h_0,\ldots,h_{n-1} \in V$ be horizontal (i.e. suppose $\partial(h_j)=0$). 
		Consider  $v=\sum_{j=0}^n \dfrac{t^j}{j!}h_j.$
		The vector $v$ is cyclic. 
		\item For each $v_0\in V$ consider the system 
		\begin{equation}\label{E:deligne-equation}
		\begin{cases}
		v \equiv v_0 \mod tV \\
		\partial(v) =0 
		\end{cases}
		\end{equation}
		The element  $v:= e^{-t\partial}v_0 = \sum_{j\geq 0}(-1)^j \frac{t^j}{j!}\partial^j(v_0)$
		is $t$-adically convergent and is the unique element in $V$ satisfying \eqref{E:deligne-equation}.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Taking derivatives we have 
	\begin{align*}
	v=& h_0 + t h_1 + \frac{t^2}{2!} h_2 + \cdots + \frac{t^{n-1}}{(n-1)!}h_{n-1}\\
	\partial(v) =& h_1 + t h_2 + \frac{t^2}{2!} h_3+ \cdots + \frac{t^{n-2}}{(n-2)!}h_{n-2} \\
	\vdots & \\
	\partial^{n-1}(v) =& h_{n-1}
	\end{align*}
	starting from the bottom of the list and going up one can see linear independence as they each introduce a new $h_j$.
	
	To prove the second part we just compute the derivative of $v$ and expand using the product rule term by term. 
	For uniqueness, suppose that $w$ is another solution. 
	One then has $w=v+tu$ for some $u\in V$. 
	We then get $u+t\partial(u)=0$, by $0 = \partial(w) = \partial(v) + \partial(tu) = \partial(tu)$. 
	We can expand $u$ in a power series to get $u(t) = \sum_j a_j t^j$ and we find that $\partial(u(t)) = u^{\partial}(t) +u'(t)$ which gives $a_0=0$ and then $a_j^{\partial}+(j+1)a_{j+1} +a_{j+1}=0$. 
	This allows us to conclude all of the $a_j=0$ inductively. 
\end{proof}

The proof of the following theorem will use Nakayama's Lemma which can be found in Atiyah-MacDonald \cite[pg 21]{Atiyah2016}.
\begin{proof}[Proof of Katz's Theorem for Formal Power Series]
	Let $V = R^n$. 
	Let $e_0,\ldots,e_{n-1}$ be a basis, then it is a basis modulo $tV$. 
	Hence by Nakayama, $\widetilde{e}_j:=e^{-t\partial }e_j$ is also a basis for $V$ since it is a basis modulo $tV$. 
	Furthermore, by the Lemma $\partial(\widetilde{e}_j)=0$. 
	We now apply part one of Lemma~\ref{L:katz} to get 
	\begin{align*}
	\sum_{j=0}^{n-1} \frac{t^j}{j!}\widetilde{e}_j =& \sum_{j=0}^{n-1}\frac{t^j}{j!} \sum_{i\geq 0}\frac{t^i}{i!} \partial^i(e_j) \\
	=& \sum_{j=0}^{n-1}\sum_{i\geq 0}\frac{t^{i+j}}{i!j!} \partial^i(e_j).
	\end{align*}
	We can trim this down (using Nakayama again). 
	If $v$ is cyclic then $v+t^n c$ is also cyclic. 
	The ``large'' power $t^n$ ensures that it remains a basis after $n$ derivatives. 
	This allows us to kill off terms with $j+k \geq n$. 
	Hence 
	$$ \sum_{j=0}^{n-1}\sum_{i=0}^{n-1-j}(-1)^i \frac{t^i}{i!}\partial(e_j) $$
	gives a cyclic vector.
\end{proof}

%%%%%%%%%%%%%%%%
\subsection{Local Exponents}
%%%%%%%%%%%%%%%%
Consider a first order meromorphic system of rank $n$ on $\PP^1$ given by 
$$ Y' = A(t) Y, \qquad A(t) \in M_n(\CC(t)).$$
The eigenvalues of residue matrices play such an important role in the local behavior of solutions of differential equations we give them a name.
\begin{definition}
	Let $R= \Res_{t=t_0}(A(t)) \in M_n(\CC)$ be a residue at $t=t_0$.
	An eigenvalue of $R$ is called a \emph{local exponent} of the system at $t=t_0$.
\end{definition}
If $S=\lbrace s_1,s_2,\ldots,s_m \rbrace$ is the polar locus for a differential equation of rank $n$ with eigenvalues $\rho_1(s_j),\rho_2(s_j),\ldots, \rho_n(s_j)$ at the points $j$ we will often write down a so-called \emph{Riemann table} in the form
$$\begin{array}{cccc}
	s_1 & s_2 & \cdots & s_m \\
	\hline \hline 
	\rho_1(s_1) & \rho_1(s_2) & \cdots & \rho_1(s_m) \\
	\rho_2(s_1) & \rho_2(s_2) & \cdots & \rho_2(s_m) \\
	\vdots & \vdots & \ddots &\vdots \\
	\rho_n(s_1) & \rho_r(s_2) & \cdots & \rho_n(s_m)
\end{array}$$

We will now go on to show that solutions of $Y(t)$ locally have the form $t^{\rho} Z(t)$ for $\rho$ ``non-resonant'' local exponents. 
We say that an eigenvalue $\rho$ of $R$ is non-resonant provided there doesn't exist another eigenvalue $\mu$ of $R$ such that $\rho-\mu \in \ZZ$. 

\subsection{Theta Operator and Indicial Equations}
Consider a linear differential equation in one variable  
$$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) =0 $$
which is formally Fuchsian at $t=0$ so that 
$$b_j(t) := t^{n-1}a_j(t) \in \CC[[t]]. $$
We wish to derive an equation for the local exponents of this equation at $t=0$. 
To do this it will be convenient to write our operator (which we view as an element of the Weyl algebra)
$$L = \partial^n  + a_{n-1}(t) \partial^{n-1} + \cdots + a_0(t) \in \CC[[t]][\partial]$$
in terms of the \emph{theta operator}
$$\theta = t \partial_t \in \CC[[t]][\partial].$$
We remark that this operator is called the \emph{Euler operator} in \cite{Iwasaki1991} and is denoted by $\delta$.

The following basic identities will be useful. 
\begin{exercise}
	In this problem $\partial = \partial_t$. 
	Show that 
	\begin{enumerate}
		\item $t^n\partial^n = \theta(\theta-1)\cdots(\theta - n+1)$
		\item $\theta(t^{\rho} f) =t^{\rho}(\theta+m)f$. 
		\item $(\theta+\rho)t^j = (j+\rho)t^j$ for all $j\geq 0$
	\end{enumerate}
\end{exercise}
If we let $M = t^n L$ then we see that 
 $$ M = \sum_{j=0}^n a_j t^{n-j}t^j \partial^j = \sum_{j=0}^n b_j \theta(\theta-1) \cdots (\theta -j +1).$$
For concreteness we write out the order two case.
\begin{example}\label{E:order-two-theta}
	We have $L = a_0 + a_1 \partial + \partial^2$ and $M = b_0 + b_1\theta +\theta(\theta-1)= b_0 + (b_1-1)\theta + \theta^2$. 
	We can be even more explicit with $b_0 = t^2a_0$ and $b_1 = ta_1$ so that 
	 $$M = t^2a_0 + (ta_1-1)\theta + \theta^2.$$
\end{example}
One also has a cute form of the hypergeometric differential equation.
\begin{exercise}
	Check that the hypergeometric equation has the form 
	$$ \theta(\theta+1-c)y -t(\theta+a)(\theta+b)y =0. $$
\end{exercise}


Now in order to derive the indicial equation for the local exponents of a linear differential operator we will seek solutions of $My=0$ in the form 
 $$ f(t) = t^{\rho} \sum_{j=0}^{\infty} c_j t^j $$
and conclude a necessary identity about the exponent $\rho \in \CC$. 
To proceed we write each $b_i(t)$ for $1\leq i \leq n$ as $b_i(t) = \sum_{j=0}^{\infty} b_{ij}t^j.$.
We then just proceed with a computation
\begin{align*}
Mf =& \left( \sum_{i=0}^n \sum_{j=0}^{\infty} b_{ij}t^j \theta^i \right) \left( t^{\rho} \sum_{k=0}^{\infty} c_k t^k \right) \\
=&t^{\rho} \sum_{i=0}^n \sum_{j=0}^{\infty} b_{ij} t^j (\theta+\rho)^i \sum_{k=0}^{\infty} c_k t^k \\
=&t^{\rho} \sum_{i=0}^n \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} b_{ij} c_k t^j (k+\rho)^i  t^{k+j} \\
=& t^{\rho} \sum_{m=0}^{\infty}\left( \sum_{i=0}^n \sum_{j=0}^m b_{ij} c_{m-j}(\rho+m-j)^i\right)t^m.
\end{align*}
If $Mf=0$ as an element of $\CC[[t]][t^{\rho}]$ then by the linear independence of $t^m$ this gives a system of  equations for each $m$ given by 
 $$   \sum_{i=0}^n \sum_{j=0}^m b_{ij} c_{m-j}(\rho+m-j)^i=0.$$
In the case $m=0$ we can pull out $c_0$ and use that $b_{i0} = b_0(0)$ and the indicial equation.

\begin{theorem}[Indicial Equation]
	If $t^{\rho}f(t)$ is a formal solution of $L$ then $\rho$ is a solution of 
	 \begin{equation}\label{E:indicial-equation}
	  b_0(0) + b_1(0) \rho + \cdots + b_{n-1}(0)\rho^{n-1} + \rho^n =0.
	 \end{equation}
\end{theorem}
Equation~\ref{E:indicial-equation} is called the indicial equation for the differential equation at $t=0$. 
It is an exercise to compute derive the indicial equation at other points. 
The basic idea is to use $\theta = (t-t_0) \dfrac{d}{d(t-t_0)}$ rather than $t\dfrac{d}{dt}$. 
Similarly, for an equation at infinity one needs to change coordinates to $s$ where $t=1/s$ and $-s^2\dfrac{d}{ds}=\dfrac{d}{dt}$.

For a later application to Fuch's relation it will be useful to compute $b_{n-1}(0)$ explicitly the second to top coefficient is always the sum of the roots:
 $$ b_{n-1}(0) = -\rho_1 -\rho_2 - \cdots - \rho_n.$$
This formula is related to a residue and we will later apply the Residue theorem.
\begin{corollary}\label{C:residue-formula}
	 $ b_{n-1}(0) = \res_{t=0}( a_{n-1}(t)dt ) - {n \choose 2}.$
\end{corollary}
\begin{proof}
One sees that 
\begin{align*}
M =& \sum_{j=0}^n a_j t^{n-j} \theta(\theta-1)\cdots(\theta-j+1) \\
=& \theta^n + (-1-2-\cdots-(n-1))\theta^{n-1} + ta_{n-1}(t) \theta^{n-1} + \cdots \\
=& \theta^n + \left( a_{n-1}(t)t - {n \choose 2} \right)\theta^{n-1} + \cdots  
\end{align*}
and hence the statement follows.
\end{proof}

\begin{exercise}
	For this problem consider a Fuchsian differential equation 
	 $$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) y =0,$$
	where $a_j(t) \in \CC(t)$ for $0\leq j \leq n-1$.
	Let $S\subset \PP^1$ be the polar locus of this equation. 
	\begin{enumerate}
		\item Show that the general indicial equation at $t=t_0\neq \infty$ takes the form
		 $$ \sum_{j=0}^n c_j \rho(\rho-1)\cdots(\rho-j+1) =0 $$
		where $c_j = \lim_{t\to t_0} (t-t_0)^{n-j}a_j(t).$
		\item Show that if $t_0=\infty \in S$ then the indicial equation becomes 
		 $$ \sum_{j=0}^n (-1)^j c_j \rho(\rho-1)\cdots(\rho-j+1) =0 $$
		 where $c_j = \lim_{t\to \infty} t^{n-j} a_j(t)$. 
	\end{enumerate}
\end{exercise}
Using the formulas above one now has a more systematic approach to computing the local exponents for the hypergeometric functions. 
\begin{exercise}
	Compute the local exponents of the hypergeometric equation 
	 $$ y'' + \dfrac{(a+b-1)t -c}{t(1-t)}y' + \dfrac{ab}{t(1-t)} y =0, $$
	for each $t_0 \in S = \lbrace 0,1,\infty\rbrace$ using the indicial formulas. 
\end{exercise}


\subsection{Fuchs' Relation}

We now state Fuchs relation which tells us more about the local exponents of the hypergeometric function (for example).
\begin{theorem}[Fuchs' Relation]\label{T:fuchs-relation}
Consider a Fuchsian linear differential equation on $\PP^1$ of the form 
$$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) y =0.$$
Let $S \subset \PP^1$ denote the polar locus of the differential equation and assume that $\infty \in S$. 
If $\rho_1(a),\ldots,\rho_n(a)$ denote the local exponents at $a \in S$ then
$$ \sum_{a\in S} \left( \rho_1(a) + \cdots + \rho_n(a) \right) = (\#S-2){n \choose 2}.$$	
\end{theorem}

Fuchs' relation mposes an extra constraint on the possible eigenvalues of matrices. 
Note that in the case $n=2$ and $\# S=3$ (the hypergeometric case) we have 
$$ \sum_{a\in S}\left( \rho_1(a) + \rho_2(a) \right) =1.$$
\begin{exercise}
	Check that the exponents in the Riemann table in Exercise~\ref{E:exponents-for-hypergeometric} satisfy Fuchs' relation.
\end{exercise} 
We now give the proof of Fuchs' relation.

\begin{proof}
	We know that for $a \in S\setminus \infty$ we have 
	 $$ \rho_1(a) + \rho_2(a) + \cdots + \rho_n(a) = {n \choose 2} - \res_{t=a}(a_{n-1}(t)dt), $$
	similarly for $a =\infty$ we have 
	 $$ \rho_1(\infty) + \rho_2(\infty) + \cdots + \rho_n(\infty) = -{n \choose 2} - \res_{t=\infty}(a_{n-1}(t)dt).$$
	Using the residue formula (Theorem~\ref{T:residue-theorem}) we have 
	\begin{align*}
	  0=&- \sum_{a \in S} \res_{t=a}( a_{n-1}(t)dt) \\
	  =& \sum_{a\in S\setminus \infty}\left( \rho_1(a)+\rho_2(a) + \cdots + \rho_n(a) - {n\choose 2} \right) \\
	  & + \rho_1(\infty) + \rho_2(\infty) + \cdots + \rho_n(\infty) + {n \choose 2} \\
	  &= - (\#S -2){n \choose 2} + \sum_{a\in S}\left( \rho_1(a)+\rho_2(a) + \cdots + \rho_n(a) \right), 
	  \end{align*}
	  which proves the result. 
\end{proof}

\subsection{Local Solutions of Exponent $\rho$}
Consider a first order system of rank $n$ which is formally Fuchsian at $t=0$. 
We will write 
$$ Y' = \frac{A(t)}{t} Y, \qquad A(t) \in M_n(\CC[[t]]).$$
Note that this system is equivalent to $\theta(Y) = A(t) Y$ where $\theta$ operators component-by-component. 
We will let expand $A(t)$ in a power series
$$ A(t) = A_0 + A_1 t + \cdots, $$
and then consider power series solutions of the form $Y(t) = t^{\rho}Z(t)$ and develop $Z(t)$ as a power series 
$$ Z(t) = Z_0 + Z_1 t + \cdots. $$
We then find that 
$$ \theta(Y) = \theta( t^\rho Z) = t^{\rho}(\theta+\rho)Z, \quad AY =  t^\rho A Z, $$
which leads us to  
\begin{align*}
(\theta + \rho)Z(t) &= \rho Z_0 + (\rho Z_1 + Z_1)t + (\rho Z_2 + 2 Z_2)t^2 +\cdots, \\
A(t) Z(t) &= A_0 Z_0 + (A_1Z_0+A_0Z_1)t + (A_2 Z_0 + A_1Z_1+A_0 Z_2)t^2+\cdots 
\end{align*}
which when we equate coefficients tells us that $Z_0$ is an eigenvector of $A_0$ with eigenvalue $\rho$ and that for $n\geq 1$ we have the equation
$$ nZ_n + \rho Z_n = A_0 Z_n + A_1 Z_{n-1} + \cdots + A_n Z_0.$$
This equation allows us to solve inductively as long as $(\rho+n)$ is not an eigenvalue of $A_0$ for $n\geq 1$ since we have the expression
$$( \rho + n + A_0) Z_n = A_1 Z_{n-1} + \cdots + A_n Z_0,$$
and $\rho+n$ not being an eigenvalue puts $(A_0 - \rho-n)$ invertible. 
The fancy word for this is that $\rho+n$ is in the resolvent set of the operator $A_0$ (the resolvent set of a linear operator $L$ is precisely the set of $\lambda$ such that $\lambda-L_0$ is invertible). 
We will omit the proof of convergence. 
This has to do with estimating the operator norm of $(x-A_0)^{-1}$ for $x$ in the resolvent set. 

This proves the following.
\begin{theorem}
	Consider the formal Fuchsian system 
	\begin{equation}\label{E:formal-fuchsian}
	Y' = \frac{A(t)}{t} Y, \qquad A(t) \in M_n(\CC[[t]]).
	\end{equation}
	Let $A_0\in M_n(\CC)$ be the residue of $A(t)/t$ at $t=0$ and let $\rho$ be an eigenvalue such that $\rho+n$ is not an eigenvalue of $A_0$ for any integer $n\geq 1$. 
	Then \eqref{E:formal-fuchsian} admits a formal solution $Y(t) \in \CC[[t]]^n$ of the form 
	$$ Y(t) = t^{\rho}(Y_0 + Y_1 t + \cdots ) $$
	where $Y_0$ is an eigenvector of $A_0$. 
	Moreover the series is convergent is the series for $A(t)$ is. 
\end{theorem}
Note that even the resonant case where there are eigenvalues $\rho$ and $\mu$ with  $\rho-\mu \in \ZZ$ then still one of these admits a solution of the type above. 
One just needs some eigenvalue such that there is no positive integer that gives another. 
If $\rho$ and $\mu$ are equal then we don't need to worry about this. 
If $\rho-\mu$ is negative then we don't need to worry about this. 
If $\rho-\mu$ is positive then we can switch the role of $\rho$ and $\mu$ and again not worry about this. 

We record the following for later use. 
\begin{theorem}
	More generally, in the non-resonant case one has a fundamental matrix of the form 
	$$ \Phi(t) = \Psi(t) t^{A_0} $$
	where $\Psi(t)$ is a matrix of formal power series. 
	The matrix $\Psi(t)$ is convergent if $A(t)$ is convergent.
\end{theorem}
\begin{proof}
	This is \cite[III, Proposition 1.2.1]{Borel1987}
\end{proof}

\subsection{Exponent Shifting}
We record the following useful fact.
\begin{lemma}[Exponent Shifting]\label{L:exponent-shifting}
	The gauge transformation $Y=(t-t_0)^{\mu}\widetilde{Y}$ has the effect of $A(t)/(t-t_0)\mapsto \widetilde{A}(t)/(t-t_0)$ where $\widetilde{A}(t) = A(t)-\mu$.
\end{lemma}
\begin{proof}
	Without loss of generality we can suppose that $t_0=0$ since $\infty$ is invariant under the transfomation $t\mapsto t-t_0$. 
	The system has the form 
	 $$ \theta Y = A(t) Y' $$
	where $A(t) = A_0 + A_1 t + \cdots \in M_n(\CC[[t]])$ and $A_j \in M_n(\CC)$ for $j\geq 0$. 
	Then $\theta(t^{\mu}\widetilde{Y}) = t^{\mu}(\theta+\mu)\widetilde{Y}$ and $A(t)t^{\mu}\widetilde{Y} = t^{\mu}A(t)\widetilde{Y}$ which gives the equation
	 $$ \theta \widetilde{Y} = (A(t)-\mu) \widetilde{Y}.$$
	One can check that $\sigma_p(A_0-\mu) = \sigma_p(A_0)-\mu$ where $\sigma_p(B)$ denotes the eigenvalues of a matrix $B$.
\end{proof}

\subsection[Exponents Determine Equations]{Local Exponents Determine Equations in Hypergeometric Case}\label{S:hypergeometric-case}
In this subsection we work in the Fuchsian case where $\#S=3$ and rank two. 
In particular we work with single ordinary differential equations of the form
 $$ y'' + a_1(t)y' + a_0(t) y =0 $$
with $a_1(t),a_0(t) \in \CC(t)$ since all rank two order one systems are equivalent to order two differential equations in one variable.

\begin{theorem}\label{T:exponents-determine-equation}
Let $S = \lbrace t_1,t_2,t_3 \rbrace \subset \PP^1$ and fix a table of local exponents satisfying Fuchs' relation  
$$\begin{array}{ccc}
t_1& t_2 & t_3 \\
\hline \hline \alpha & \beta & \gamma \\
\alpha' & \beta' & \gamma' 
\end{array}.$$
There exists a unique $a_1(t),a_2(t) \in \CC(t)$ such that 
\begin{equation}\label{E:fixed-exponents}
  y''+a_1(t) y' + a_0(t) y =0
 \end{equation}
is a Fuchsian differential equation with polar locus $S$ and exponents as given in the table. 
\end{theorem}
\begin{proof}
	The proof is a partial fraction expansion computation and follows \cite[Chapter 2, Proposition 1.1.1]{Iwasaki1991} closely.
	We assume without loss of generality that $t_3=\infty$. 
\end{proof}

\begin{theorem}
	The equation~\ref{E:fixed-exponents} reduces to the hypergeometric differential equation.
\end{theorem}
\begin{proof}
Using a M\"obius transformation we can transform $(t_1,t_2,t_3)$ to $(0,1,\infty)$ giving a new order two differential equation with $S = \lbrace 0,1,\infty\rbrace$.
This gives a new exponent table
	$$\begin{array}{ccc}
	t_1 & t_2 & t_3 \\
	\hline \hline \alpha & \beta & \gamma \\
	\alpha' & \beta' & \gamma' 
	\end{array} \mapsto \begin{array}{ccc}
		0& 1 & \infty \\
		\hline \hline \alpha & \beta & \gamma \\
		\alpha' & \beta' & \gamma' 
	\end{array}.$$
We next apply the exponent shifting lemma (Lemma~\ref{L:exponent-shifting}).
Making the gauge transformation $y = t^{-\alpha}(t-1)^{-\beta}\widetilde{y}$ to tranform the exponent table again to
 	$$\begin{array}{ccc}
 	0& 1 & \infty \\
 	\hline \hline \alpha & \beta & \gamma \\
 	\alpha' & \beta' & \gamma' 
 	\end{array} \mapsto \begin{array}{ccc}
 0& 1 & \infty \\
 \hline \hline 0& 0 & \gamma +\alpha +\beta \\
 \alpha' -\alpha& \beta'-\beta & \gamma' +\alpha+\beta
 \end{array}.$$
 We then just relabel the exponents:
   \begin{align*}
   a &= \gamma +\alpha +\beta\\
   b &= \gamma' +\alpha+\beta\\
   1-c &= \alpha' -\alpha 
   \end{align*}
  and finally Fuchs' relation (Theorem~\ref{T:fuchs-relation}) forces $\beta'-\beta=c-a-b$.
  Since the exponents determine the equation (Theorem~\ref{T:exponents-determine-equation}) the transformed equation must be a hypergeometric equation with the given exponents. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Riemann-Hilbert]{Connections and Riemann-Hilbert Correspondences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
I started writing this section to present a proof of Plemelj's theorem, which at the time  (and the 70 years that follows) was thought to be a solution to Hilbert's 21st problem in a modern form. 
To do this we need to introduce vector bundles with connections which then will later be used again for constructing isomonodromic deformations. 

The basic strategy of Plemelj's proof is to glue a bunch of local equations together. 
We now understand this technique to be the part of what is called ``descent theory''. 
This is just a fancy word for ``the theory of gluing things together''.  
In order to glue things we need to define the things that we are gluing. 
These ``things'' are connections on vector bundles $(E,\nabla)$ over a Riemann surface (or complex manifold). 
The word ``connections'' is just a fancy word for ``locally a $D$-module''. 
 
The idea of a connection leads to a bunch of interesting mathematics and physics including the notion of curvature. 
The basic motto is 
\begin{center}
Force = Curvature
\end{center}
Physically, all of the basic forces in the standard model of physics the weak, strong, electromagnetic are all Yang-Mills theories which involve this concept.

We will follow the chapters of Haeflinger and Malgrange the book on $D$-modules  \cite[Chapters III, IV]{Borel1987} which largely follows \cite{Deligne1970}.
There some nice material from the \emph{Holomorphic Foliations and Algebraic Geometry} Summer School in Mathematics in 2019 which has excellent YouTube videos and notes. Viktoria Heu's \href{https://if-summer2019.sciencesconf.org/resource/page/id/1}{Notes} are brief and excellent. 
Also see Frank Loray's second lecture from the same Summer School \href{https://www.youtube.com/watch?v=qCujE4nU8bc}{here}.\footnote{There are actually many great videos of Frank Loray on YouTube if you do a quick search.}


\section{Vector Bundles and Connections}
In this section we define vector bundles and connections. 
They are mainly a global language for linear differential equations and give a formalism in which we can talk about complicated changes of coordinates.
The vector bundle encodes all possible changes of coordinates of the $D$-module and the connection is the derivation part of the $D$-module. 
More precisely it is the equation. 
Most importantly this global language allows us to glue together local information in order to solve (or show we can't solve) Hilbert's 21st problem.

Next given a vector bundle and a connection we get to talk about curvature and things like geodesics. 
A connection is a way to convert a derivative into a $D$-module structure.
If we think of derivatives as vector fields on space, as one does with tangent bundles, connections are telling us how to associate a direction on our manifold to a direction in our vector bundle. 
In particular for each direction one gets a differential equation and solving this differential equations tells us how to move things around in the vector bundle. 
This is just solving an initial value problem in ODEs.
One issue is that while moving around in our base space commutes, it doesn't necessarily translate to an commutative procedure for moving around in the vector bundle. 
This leads to a notion of curvature. 

Consider Figure~\ref{F:curvature}. 
In this picture we have our base space being a sphere $S^2$ and the vector bundle being the tangent bundle itself -- so note in particular that in this setup the tangent bundle is appearing twice: first as an object parametrizing directions and second as the object on which the directions act through covariant derivatives/assigning linear differential equations.
One can see that if we move a vector from the north pole down a longitude, then along a lattitude, then back up to the north pole around a longitude that we arrive with a vector which is displaced from the original one. 
This is what curvature is.
\begin{figure}[h]\label{F:curvature}
	\begin{center}
		\includegraphics[scale=0.5]{curvature.eps}
	\end{center}
	\caption{Curvature of the tangent bundle of $S^2$ associated to the Levi-Civita connection is evidenced by transporting tangent vectors around the the sphere.}
\end{figure}

Finally, while curvature is an interesting concept in itself, the conditions for curvature give interesting differential equations. 
In fact all the gauge theories in particle physics (strong, weak, electro-magnetic), all of the fundamental forces are encoded by curvatures. 
Later, we are going to need curvature to vanish in order for an overdetermined collection of partial differential equations to be well-defined.
This is needed for example in order to derive the Schlesinger equations for isomonodromic flows (this is an equation for equations!).
These conditions are what give rise to the $P_{VI}$, Painlev\'e six.
This is essentially a condition on equality of mixed partial derivatives for solutions of differential equations. 

\subsection{Vector Bundles (for the uninitiated)}
There are two objects that we will often conflate: vector bundles and locally free sheaves. 
Locally free sheaves are essentially modules that we associate to open sets. 
They are modules over rings of holomorphic functions and they ``glue'' together nicely. 
Vector bundles are spaces over another space which have the property that local sections of the vector bundle form a locally free sheaf. 
Later we will conflate the two since for every vector bundle there is a locally free sheaf and conversely, to every locally free sheaf we can construct a vector bundle. 
If you know what these words mean, this section probably isn't for you. 
I'm going to begin an introduction into these two fundamental objects with a discussion of coordinates of free modules. 
I will then extend this idea to describe the data of vector bundles given from two open sets. 
I will then describe the general definition and say a little bit about what it means to be a sheaf. 
After this section I'm going to assume everyone is familiar with these objects since a detailed discussion will lead us two far afield. 
We recommend \cite{Vakil2017} for a more detailed discussion of vector  bundles on schemes.

Let $R$ be a commutative ring and let $E$ be a free $R$-module of rank $n$. 
We have talked about coordinate isomorphisms $\psi: E \to R^{\oplus n}$ given by $v=f_1v_1 + \cdots + f_n v_n \mapsto (f_1,\ldots,f_n)$ where $v_1,\ldots,v_n$ is a basis for $E$ and $f_1,\ldots,f_n\in R$ are called the coordinates. 
The map $\psi$ is called a trivialization. 
We have also talked about how a change in choice of basis for $E$ transforms the coordinates by some element of $\GL_n(R)$. 
The element of $\GL_n(R)$ transitions from one set of coordinates in one basis to another set of coordinates in another basis. 

A vector bundle is sort of like this naive change of coordinates but we have a varying collection of $R_i$ and $E_i$ for $i$ in some index set $I$ and they need to satisfy compatibility conditions.
The $R_i$ are the functions on some open set of some space and the $E_i$ are local sections of the vector bundle. 

\begin{example}[Vector Bundles With Two Charts]
	The data for this is some $E_i$ $R_i$-modules for $i=1,2$ and an additional $R_{12}$-module $E_{12}$.
	
	There are ring homomorphisms 
	$$ \begin{tikzcd}
	R_1 \arrow[r] & R_{12} & \arrow[l] R_2 
	\end{tikzcd}$$
	and morphisms of abelian groups
	$$ \begin{tikzcd}
	E_1 \arrow[r] & E_{12} & \arrow[l] E_2 
	\end{tikzcd}$$
	which respects the module actions. 
	Moreover these have the property that a basis for $E_1$ or $E_2$ induce a basis for $E_{12}$ and hence trivializations for $E_1$ or $E_2$ induce trivializations for $E_{12}$. 
	There are additional glueing properties, but I will state those when I state the official definition.
\end{example}

\begin{example}[Vector Bundles on $\PP^1$]\label{E:charts-on-p1}
	In the case of $\PP^1$ we have two open sets $U_0$ and $U_{\infty}$ which cover $\PP^1 = U_0 \cup U_{\infty}$. 
	\begin{figure}[h]
		\begin{center}
			\includegraphics[scale=0.5]{charts-for-p1.eps}
		\end{center}
		\caption{$\PP^1$ is covered by $U_0$ and $U_{\infty}$.
			Here $U_0$ uses the standard $t$-coordinate and  $U_{\infty}$ uses the coordinate at infinity $s$ given by $s=1/t$. 
			On their intersection $U_0 \cap U_{\infty}$ you can use either coordinate. 
		}
	\end{figure}
	
	In the previous example (with indexing $0,\infty$ instead of $1,2$) we have 
	$$ \begin{tikzcd}
	R_0 =\hol(U_0) \arrow[r] & R_{0\infty} = \hol(U_0\cap U_{\infty}) & \arrow[l] R_{\infty} = \hol(U_{\infty}) 
	\end{tikzcd}$$
	where the maps are ``restriction of the domain''.
	Then to specify a vector bundle one can specify three modules $E_0$, $E_{\infty}$, and $E_{0\infty}$ which are free $R_0,R_{\infty},$ and $R_{0\infty}$-modules respectively. 
\end{example}

While the basic data above is correct it is terrible to define vector bundles this way. 
We want a definition that doesn't depend on the choice of cover, works for all spaces, and allows us to glue local objects together. 
The technical thing we want to say is that a vector bundles $E$ on $X$ is a \emph{locally free sheaf of $\Ocal_X$-modules}.
There are two categories in which we want to formulate this notion: the category of schemes and the category of complex manifolds. 


\begin{itemize} 
	\item ($\Ocal_X$-modules) 
	In the category of complex manifolds for an open set $U$ one has $\Ocal_X(U) = \hol(U)$ the set of holomorphic functions on $U$ and in the category of schemes $\Ocal_X(U)$ is the structure sheaf. 
	
	\item (Sheaves) For $E$ to be a sheaf of $\Ocal_X$-modules we need that $E(U)$ to be a $\Ocal_X(U)$-module for every $U\subset X$ and it needs to satisfy sheaf axioms. 
	Elements of $E(U)$ are called sections over $U$. 
	The first axiom says that if you have bunch of open sets and sections on those open sets that agree on the intersections then there exists a section over the union of the open sets that restricts to each of those sections. 
	The second axiom says that such a lifting is unique: if you have two sections which agree on a collection of open sets that cover the set it is a section over then the two sections must be the same. 
	\item (Locally free of rank $n$)
	Finally, for $E$ to be locally free of rank $n$ that means that for every $x\in X$ there exists some $U$ open containing $x$ and an isomorphism $\psi_U: E(U) \to \Ocal_X(U)^{\oplus n}$. 
\end{itemize}
The nice thing about vector bundles is that they satisfy effective descent. 
This is sort of like the sheaf axiom but for objects of the category themselves. 
If $E_i$ are vector bundles over $U_i$ and they $E_i \vert U_i \cap U_j \cong E_j \vert U_1 \cap U_j$ and these isomorphisms satisfy some compatibility conditions, then there exists a vector bundle over $\bigcup_{i} U_i$.
The correct way of talking about this now is to say that fibered category of vector bundles over the category of spaces you are considering is a stack. 
We aren't going to review this here, this would take an entire class. 
The take-away is that you can build up vector bundles from local data.

Let's do a simple example of a line bundle on $\PP^1$.
A line bundle is just a vector bundle of rank one. 
\begin{example}
	Lets consider the sheaf of holomorphic differentials on $\PP^1$. 
	This is the sheaf we denote by $E = \Omega_{\PP^1}^1$. 
	In our usual coordinates we have 
	$$ E(U_0) = \Ocal_{\PP^1}(U_0) dt, \quad E(U_{\infty}) = \Ocal_{\PP^1}(U_{\infty})ds.$$
	There are trivializations 
	$$ \psi_0\colon E(U_0) \to \Ocal_{\PP^1}(U_0), \quad f(t) dt \mapsto f(t) $$
	$$ \psi_{\infty} \colon E(U_{\infty}) \to \Ocal_{\PP^1}(U_{\infty}), \quad g(s)ds \mapsto g(s) $$
	Both of these trivializations are valid on $E(U_0 \cap U_{\infty})$ and over $U_0\cap U_{\infty}$ we have
	$$ \psi_{\infty}\psi_0^{-1}(1) = \psi_{\infty}(dt) = \psi_{\infty}(\frac{-ds}{s^2}) = \frac{-1}{s^2}\psi_{\infty}(ds) = \frac{-1}{s^2}.$$
\end{example}

In the above example we see that the transition map was given by multiplication by $-1/s^2$. 
It turns out that every line bundle on $\PP^1$ and admits trivializations over $U_0$ and $U_{\infty}$ with transition maps of  the form $f \mapsto -s^{-d} f$ for some integer $d$. 
The integer $d$ characterizes the line bundle up to isomorphism and we call the one with integer $d$, $\Ocal_{\PP^1}(-d)$. 
So for example, $\Omega_{\PP^1} \cong \Ocal_{\PP^1}(-2)$. 
There are a couple interpretation of these line bundles, one being sheaves of meromorphic functions with poles at more order $d$ at infinity. 
The notation is a little wonky too. 
If $U$ is an open subset of $\PP^1$ then to take sections of this vector bundle we write $\Ocal_{\PP^1}(d)(U)$ so the $d$ has nothing to do with the open sets we were plugging in earlier. 
\begin{remark}
	Readers already familiar with algebraic or complex geometry will recognize $-s^{-d} \in \Ocal^{\times}_{\PP^1}(U_0 \cap U_{\infty})$ as a representative of the cohomology class  in $H^1(\PP^1,\Ocal^{\times}) = \Pic(\PP^1)$ given in terms of a \v{C}ech cocycle with two open sets. 
\end{remark}
The comment about line bundles extends to vector bundles. 
The obvious vector bundles we can think of are direct sums of line bundles. 
These have transition matrices which are diagonal of the form $\operatorname{diag}(t^{d_1},t^{d_2},\ldots,t^{d_n}).$
These turn out to be all of them.

\begin{theorem}[Birkoff-Grothendieck]
	All vector bundles on $\PP^1$ are isomorphic to 
	$$ \Ocal_{\PP^1}(d_1) \oplus \cdots \Ocal_{\PP^1}(d_n) $$
	for some $d_1,d_2,\ldots,d_n \in \ZZ$.
\end{theorem}
\begin{proof}[Proof Reference and Sketch]
	This can be found in \cite{Hazewinkel1982} which works algebraically over a general ring. 
	By GAGA, that every algebraic vector bundle on $\PP^1$ (considered as a scheme) is equivalent to proving every holomorphic vector bundles on $\PP^1$ has this form.
	
	The prove relies on a matrix factorization theorem of Birkoff.
	If $A \in \GL_n(\CC[t,t^{-1}])$ then there $BAC=D$ where $C=C(t)$ and $B=B(t^{-1})$  have entries in $\CC[t]$ and $\CC[t^{-1}]$ respectively and $D$ is diagonal with each entry of the form $t^s$ for some integer $s$.
	
	One works on two charts of $\PP^1$ and then factors the transition data using this theorem. 
	This is a \v{C}ech cocycle classifying the vector bundle and has the appropriate diagonal form.
	This proves the results.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Systems vs Connections}
%%%%%%%%%%%%%%%%%%%%%%%
Consider a vector bundle of rank $n$ with connection $(E,\nabla)$ on complex manifold $X$.
In the special case that $E\cong \Ocal_X^{\oplus n}$ we often speak of a the connection as a \emph{system} since there is really no extra global information. 
For non-compact Riemann surfaces all connections are really just systems.
\begin{theorem}[Grauert-Röhrl Theorem]
	Every holomorphic vector bundle of rank $n$ on a non-compact Riemann surface $X$ is isomorphic to $\Ocal_X^{\oplus n}$.
\end{theorem}
\begin{proof}[Proof Reference and Sketch]
	This is \cite[Theorem 30.4]{Forster1981}.
	The proof is by induction on the rank of the vector bundle. 
	They show first that line bundles are trivial using the so-called Runge approximation theorem.
	Then they do an explicit computation with \v{C}ech cocycles after appling the inductive hypothesis to reduce the transition functions to unipotent matrices. 
	They then reduce further arguing about an additive \v{C}ech cocycle.
\end{proof}

This will mean that we don't need to worry about global information coming from the vector bundle when trying to establish a Riemann-Hilbert correspondence for log-connections on the projective line minus a finite set of points. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Babymost case: Covariant Derivatives and Connections Associated to ODEs}
%%%%%%%%%%%%%%%%%%%%%%%
Since connections can get abstract, before proceedings with the full definition, I'm going to explain what everything is for an ODE. 
The main idea is that we can convert ``$Y$ is a solution of a differential equations' into ``$Y$ is horizontal for a connection''.
$$
\dfrac{dY}{dt}=B(t)Y \quad \iff \quad \nabla_{\frac{\partial}{\partial t}}(Y) =0.
$$
The covariant derivative in this example is the $\CC$-linear operator 
$$ \nabla_{\frac{\partial}{\partial t}} = \dfrac{\partial}{\partial t} - B(t).$$
If, say, $B(t)$ is a holomorphic on $U$ some neighborhood of a points in $\PP^1$ then this defined an operator $R^n \to R^n$ whree $R=\hol(U)$.
The connection in this situation is a map
$$ \nabla = d-B(t)dt $$
where $d$ is the exterior differential acting on column vectors in $R^n$ and $-B(t)dt$ is a matric of differential 1-forms. 
This defined a map $\nabla: R^n \to \Omega^1_{\PP^1}(U) \otimes R^n$.
Explicitly 
$$ \nabla \begin{pmatrix}
y_1 \\
y_2\\
\vdots \\
y_n
\end{pmatrix} = \begin{pmatrix}
dy_1 \\
dy_2\\
\vdots \\
dy_n
\end{pmatrix} - B(t) \begin{pmatrix}
y_1 \\
y_2\\
\vdots \\
y_n
\end{pmatrix} dt. $$
Sometimes the matrix $-B(t)dt$ is called the connection 1-form and usually denoted by $\omega$ or $A(t)dt$ (so that $A(t)=-B(t)$). 
Before proceeding to the abstract theory we remark that $\nabla$ and $\nabla_{\frac{\partial}{\partial t}}$ are related by the pairing 
$$ \nabla_{\frac{\partial}{\partial t}}(Y) = \langle \nabla(Y), \dfrac{\partial}{\partial t} \rangle.$$
Usually this pairing is just defined differential forms and derivations, i.e. between $\Omega_{\PP^1}^1(U)$ and $T_{\PP^1}(U)$ but if $W$ is a vector of $1$-forms given by we extend the pairing to $W$ by pairing with each entry of $W$.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Connections}
%%%%%%%%%%%%%%%%%%%%%%%
There exists definitions of connections for schemes but for concreteness we will work with complex manifolds.\footnote{I'm really working with ringed spaces here.}
Let $X$ be a complex manifold and let $E$ be a vector bundle on $X$. 
\begin{definition}
	A \emph{connection} on $E$ is a $\CC$-linear map 
	$$ \nabla: E \to \Omega_X^1 \otimes_{\Ocal_X} E $$
	satisfying 
	\begin{enumerate}
		\item For all $f \in \Ocal_X$ and all $s\in E$, $\nabla(fs) = df\otimes s + f\nabla(s).$ 
		\item For all $s_1,s_2 \in E$, we have $\nabla(s_1+s_2)=\nabla(s_1) + \nabla(s_2)$.
	\end{enumerate}
\end{definition}
We now can extract a more general definition of local system.
\begin{definition}
	The space of \emph{horizontal sections} of $(E,\nabla)$ is defined by 
	$$ U \mapsto E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s) =0 \rbrace.$$
\end{definition}
This will play an important role in the Riemann-Hilbert correspondence in the case that $\nabla$ is a so-called integrable connection.

We remark that every connection $\nabla$ can be extended to $\Omega^i_X \otimes E$.
This means that given $\omega \otimes s \in \Omega^i_X \otimes E$ we define
$$ \nabla(\omega \otimes s) = d(\omega) \otimes s + (-1)^i \omega \wedge \nabla(S).$$


\subsection{Christoffel Symbols}
Let $X$ be a complex manifold of dimension $n$ and let $E$ be a vector bundle of rank $n$ on $X$.
Locally (on some open set $U\subset X$) we can fix coordinates $t_1,\ldots,t_m$ of $X$ and a basis $s_1,\ldots,s_n$ of $E$. 
This means that on $U$ we have 
$$ E(U) = \Ocal_X(U)s_1 + \cdots + \Ocal_X(U)s_n,$$
$$ T_X(U) = \Ocal_X(U) \dfrac{\partial }{\partial t_1} + \cdots + \Ocal_X(U) \dfrac{\partial}{\partial t_m},$$
$$ \Omega_X(U) = \Ocal_X(U)dt_1 + \cdots + \Ocal_X(U)dt_m.$$
In each of the above expression the sum are direct -- so each $\Ocal_X(U)$-module is free with the given basis. 
Now to get the Christoffel symbols we can just write down what $\nabla(s_j) \in \Omega^1_X(U) \otimes E(U)$ must look like. 
It must have the form
\begin{equation}\label{E:structure-constants}
\nabla(s_j) = \sum_{i=1}^n\sum_{\alpha=1}^m \Gamma^i_{\ j \alpha} dt_{\alpha} \otimes s_i.
\end{equation}
The Christoffel symbols are just the structure ``constants'' for the connection.
\begin{definition}
	The elements $\Gamma^i_{\ j \alpha} \in \Ocal_X(U)$ are called the \emph{Christoffel symbols} of $\nabla$ with respect to the local basis $s_1,\ldots,s_n$ of $E$ and local coordinates $t_1,\ldots,t_m$ of $X$.
\end{definition}
The is a convenient way to write this down using Einstein notation. 
If we write $t^{\alpha}$ instead of $t_{\alpha}$ then we can write \eqref{E:structure-constants} in the simple form
$$\nabla(s_j) = \Gamma^i_{j\alpha} dt^{\alpha}\otimes s_i.$$
In Einstein summation notation a upper index followed by a repeated lower index implies summation over that variable. 
So, in this expression, there is an implied sum over $\alpha$ and over $i$. 
We will make use of Einstein notation freely. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Covariant Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%
We now given the definition of a covariant derivative.
\begin{definition}
	Let $\theta \in T_X(U)$ be a derivation.
	We define the \emph{covariant derivative associated to $\theta$} to be the operator 
	$$ \nabla_{\theta}\colon E(U) \to E(U), \quad s\mapsto \nabla_{\theta}(s) = \langle \nabla(s),\theta\rangle.$$
\end{definition}
In the above expression the pairing $\Omega_X^1 \times T_X\to \Ocal_X$ (which we can write as $\Omega_X^1\otimes T_X \to \Ocal_X$) is extended to $(E\otimes \Omega_X^1)\otimes T_X \to E\otimes \Ocal_X= E$ by tensoring up to $E$.
All tensors here are over $\Ocal_X$.
Note that this pairing just means that we pair each component of $E\otimes \Omega^1_X$ with a tangent vector and take the element of $E$ obtained from the result. 
Also, since $\nabla$ satisfies a product rule and sum rule we will have 
$$ \nabla_{\theta}(fs) = \theta(f) s + f \nabla_{\theta}(s), \quad \forall f \in \Ocal_x, \forall s \in E,$$
$$\nabla_{\theta}(s_1+s_2) = \nabla_{\theta}(s_1) + \nabla_{\theta}(s_2), \quad \forall s_1,s_2 \in E.$$


In local coordinates, we have an explicit expression for covariant derivatives using Christoffel symbols.
Before proceedding I will make some remarks on notation.
First, derivatives $\dfrac{\partial}{\partial t^{\beta}}$ are ``naturally lowered'' in so Einstein notation and for convenience we often write them as $\partial_{\beta} = \dfrac{\partial}{\partial t^{\beta}}$.
In this notation, with these local coordinates, a general derivative $\theta$ is written as $\theta = a^{\beta}\partial_{\beta}$. 
Also, it is annoying to write $\nabla_{\frac{\partial}{\partial t^\beta}}$ all of the time.
We will write 
$$ \nabla_{\beta} = \nabla_{\partial_{\beta}} = \nabla_{\frac{\partial}{\partial t^{\beta}}}$$
to simplify the notation.
\begin{example}
	Let $t^1,\ldots,t^m$ be local coordinates for $X$ and let $s_1,\ldots, s_n$ be a local basis for $E$. 
	Then if $\theta = \dfrac{\partial}{\partial t^{\beta}}$ we have
	$$\nabla_{\frac{\partial}{\partial t^{\beta}}}(s_j) = \langle \Gamma^i_{\ j \alpha} dt^{\alpha} \otimes s_i, \dfrac{\partial}{\partial t^{\beta}} \rangle 
	= \Gamma^i_{\ j \alpha} \delta^{\alpha}_{\ \beta} s_i 
	= \Gamma^i_{\ j \beta} s_i.$$
	In particular if $s = f^js_j\in E$ then 
	\begin{align*}
	\nabla_{\beta}(f^js_j) &=\partial_{\beta}(f^j)s_j + f^j \nabla_{\beta}(s_j) \\
	&= \partial^{\beta}(f^j)s_j + f^j \Gamma^i_{\ j \beta} s_i\\
	&= \left(\partial^{\beta}(f^j) + f^i \Gamma^j_{\ i \beta} \right) s_j 
	\end{align*}
	
	
	A general $\theta$ we can write as $\theta = a^{\beta}\partial_{\beta}$ and
	$$\nabla_{\theta}(s_j) = \nabla_{a^{\beta} \partial_{\beta}}(s_j) = a^{\beta}\nabla_{\beta}(s_j)=a^{\beta}\Gamma^i_{\ j \beta}s_i. $$
\end{example}

\subsection{Connection 1-forms}
The purpose of this section is the show that after fixing a local basis $s_1,\ldots,s_n$ for $E(U)$, that locally 
$$ \nabla = d + \omega, \quad \mbox{ on $\Ocal_X(U)^{\oplus n}$} $$
where $\omega \in M_n(\Omega_X^1(U))$ is a matrix of $1$-forms.
Before proceeding, I need to explain how to multiply matrices of one forms and how the exterior derivative $d$ works.

\subsubsection{Calculus of Connection 1-forms}
For the purpose of iterating the connection later, we remark that tje exterior algebra $\Omega_X^{\bullet} = \bigoplus_{d=0}^m \Omega_X^{\bullet}$ is a sheaf of skew commutative ring satisfying 
$$\eta_1 \wedge \eta_2 = (-1)^{d_2} \eta_2 \wedge \eta_1, \quad 
\eta_j \in \Omega^{d_j}_{X}.$$
Matrices $\omega,\eta \in M_2(\Omega^{\bullet})$ are then multiplied by using the formula 
$$ \omega \wedge \eta = ( \sum_{l=1}^n \omega_{i l} \wedge \eta_{l j} ),$$
if $\omega = (\omega_{ij})$ and $\eta = (\eta_{ij})$.

One more notational remark before proceeding: If $s_i \in E(U)$ form a local basis we will let $s^i$ denote the dueal basis in $E^{\vee}(U)$ where $E^{\vee}$ is the dual vector bundle. 
It is defined by 
$$E^{\vee}(V) := E(V)^{\vee} = \Hom_{\Ocal_X(V)}(E(V),\Ocal_X(V))$$ 
for $V$ an open subset of $X$. 
The second $\vee$ is just usual $\Ocal_X(V)$-module duality as defined by the last equality.

\begin{exercise}
	Suppose that $t^{\alpha}$ are local coordinates of $X$ and that $s_i$ is a local basis for $E$. 
	We can write $\nabla$ as an element of $\Omega_X^1\otimes \End(E)$ by 
	$$ \nabla = \Gamma^i_{\ j \alpha} dt^{\alpha} \otimes s_i \otimes s^j.$$
	Here $\End(E)$ is the sheaf of $\Ocal_X$-linear maps from $E$ to itself (endomorphisms) and $\End(E) = E \otimes E^{\vee}$.
\end{exercise}

We now explain how to work with $\nabla=d+\omega$ as an element of a Weyl algebra.
The thing to keep in mind here when working with $\nabla =d+\omega$ is that $\omega$ is a matrix over a non-commutative ring i.e. $\omega \in M_n(\Omega_X^{\bullet})$ (so it is like super noncommutative) and $d$ is an operator on this ring. 
This means we need to do computations in a very weird looking Weyl algebra $M_n(\Omega_X^{\bullet})[d]$ where $d$ here is the exterior derivative. 
If $\omega \in M_n(\Omega_X^{\bullet})$ we need to understand how $d\omega$ acts on $\eta\in M_n(\Omega_X^{\bullet})$ when $\omega$ is homogenous. 
The key thing to keep in mind is that  $\omega$ act by wedge-matrix-multiplication:
$$(d\omega)(\eta) = d(\omega\wedge \eta) = d(\omega) \wedge \eta + (-1)^{\deg(\omega)} \omega \wedge d(\eta).$$
This proves the following:
\begin{lemma}[Basic Weyl Algebra Rules]\label{L:exterior-weyl-algebra}
In the Weyl algebra	$M_n(\Omega_X^{\bullet})[d]$ for $\omega \in M_n(\Omega_X^{\bullet})$ homogeneous we have 
	\begin{equation}
	d \wedge \omega = (-1)^{\deg(\omega)} \omega \wedge d + d(\omega).
	\end{equation}
\end{lemma}
This is going to be used when computing our formulas for curvature (Theorem~\ref{T:integrability-conditions}).

\subsubsection{The formula: $\nabla = d +\omega$}
We now verify the claim about the description in local coordinates.
Let $U$ be an open subset for which $s_1,\ldots,s_n$ is a basis for $E(U)$. 
Let $\psi: E(U) \to \Ocal_X(U)^{\oplus n}$ be the trivialization given by $\psi(f^is_i) = f^ie_i$ where $e_i$ is the standard basis vector on $\Ocal_X^{\oplus n}$ (say viewed as column vectors).
The trivialization extends to $E(U) \otimes \Omega_X(U) \to \Ocal_X(U) \otimes \Omega_X^1(U)$ by functorality of the tensor product and we will abusively also denote this isomorphism by $\psi$. 
We now have a square,
$$\begin{tikzcd}[contains/.style = {draw=none,"\in" description,sloped}]
s=f^is_i \arrow[dd,mapsto] \arrow[dr, contains] \arrow[rr,mapsto]& & \nabla(s) \arrow[d,contains] \\ 
& E(U) \arrow[r, "\nabla"] \arrow[d,above,"\psi"]& \Omega_X(U) \otimes E(U)  \arrow[d, "\psi"]\\
\begin{pmatrix}
f^1 \\
\vdots \\
f^n 
\end{pmatrix}\arrow[r,contains]& \Ocal_X(U)^{\oplus n} \arrow[r, "\nabla^{\psi}"] & \Omega_X(U) \otimes \Ocal_X(U)^{\oplus n}
\end{tikzcd}$$
where $\nabla^{\psi}$ denotes the connection in trivialized coordinates. 
Examining the diagram we see that we have 
$$ \psi(\nabla s) = \nabla^{\psi} \begin{pmatrix}
f^1 \\
\vdots \\
f^n 
\end{pmatrix},$$
so it remains to compute what $\psi(\nabla(s))$ is.
We get 
\begin{align*}
\nabla(s) &= df^i\otimes s_i + f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes s_j \\
&\mapsto \psi(\nabla(s))  =df^i \otimes e_i + f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes e_j = \begin{pmatrix}
df^1 \\
\vdots \\
df^n
\end{pmatrix} + \omega \begin{pmatrix}
f^1 \\
\vdots \\
f^n
\end{pmatrix}
\end{align*}
where we have written
$$ f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes e_j= \omega \begin{pmatrix}
f^1 \\
\vdots \\
f^n
\end{pmatrix}, \quad \omega = A_{\alpha} dt^{\alpha}, \quad A_{\alpha} = (\Gamma^j_{\ i \alpha}).$$
We summarize the above discussion with the following lemma.
\begin{lemma}
	If $E$ is a rank $n$ vector bundle on an $m$-dimensional complex manifold and $U \subset X$ is an open subset such that $X$ has local coordinates $t^1,\ldots,t^m$ and local basis $s_1,\ldots,s_n$ then in local coordinates  $\nabla_{\alpha}:\Ocal_X(U)^{\oplus n} \to \Ocal_X(U)^{\oplus n}$ takes the form
	$$ \nabla_{\alpha} = \partial_{\alpha} + A_{\alpha},$$
	where $A_{\alpha} = (\Gamma^j_{\ i \alpha})$ and elements of $\Ocal_X(U)^{\oplus n}$ are viewed as column vectors.
	In Einstein notation
	$$ \nabla_{\alpha} f^i = \partial_{\alpha}(f^i) + \Gamma^i_{\ j \alpha}f^j.$$
\end{lemma}

The matrix of $1$-forms $\omega$ is called the connection 1-form and we record this in a definition environment for those browsing looking for the definition.
\begin{definition}
	The matrix $\omega = A_{\alpha}dt^{\alpha} \in M_n(\Omega_X(U))$ is called a \emph{connection 1-form}.
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Curvature}
%%%%%%%%%%%%%%%%%%%%%%%
We keep our notation as in the previous section.
We will let $E$ be a rank $n$ vector bundle on an $m$-dimensional complex manifold and let $U$ be an open subset of $X$ which admits local coordinate $t^{\alpha}$ for $X$ and a local basis $s_j$ for $E$.

Recall that $\nabla$ tells us how to move on $E$ given movement on the base: to flow from $s_* \in E(U)$ to another point $s$ sufficiently near by along the direction of $\partial_{\alpha}$. 
See figure~\ref{F:parallel-transport} for a picture of paths in an open set $U\subset X$ being lifted to paths in the vector bundle by solving differential equations.
\begin{figure}[h]\label{F:parallel-transport}
	\begin{center}
		\includegraphics[scale=0.5]{parallel-transport.eps}
	\end{center}
	\caption{Movement in the base tells us how to move in the fibers by solving differential equations. The figure shows paths in the space being transported to the vector bundle locally by solving differential equations.}
\end{figure}

We describe these equations.
If $s_* = f^js_j$ then to we just solve 
\begin{equation}\label{E:parallel-transport}
\nabla_{\alpha} \begin{pmatrix} f^1 \\
\vdots \\
f^n  \end{pmatrix} = 0,
\end{equation}
which is just a linear differential equation with only derivatives in $t^{\alpha}$ appearing. 
Note that this is just an equation of the form 
$$ \dfrac{\partial Y}{\partial t^{\alpha}} = -A_{\alpha}(t) Y, $$
which we are very familiar with by now.
The two expressions are related by letting $Y = (f^1,\ldots,f^n)$ and viewing it as a column vector.

There are some natural questions that come up when thinking about this transport of motion on the base to motion on the fiber.
\begin{problem}[Integrability Problem]
	Is it possible to solve all of our equations \eqref{E:parallel-transport} at once (i.e. simultaneously for $\alpha=1,\ldots,m$)? 
\end{problem}
If this is possible we call the connection \emph{integrable}.
\begin{problem}[Curvature Problem]
	Is moving the direction of $t^{\alpha}$ then moving in the direction of $t^{\beta}$ the same as moving in the direction of $t^{\beta}$ then moving in the direction of $t^{\alpha}$?
\end{problem}
If this is the case we call the connection \emph{flat}.

It turns out that flatness and integrability are really the same thing and that this condition is given $\nabla^2=0$ which is equivalent to vanishing the the curvature tensor.
We will now explain.

The integrability issue is the concern that a solution of $\nabla_{\alpha}Y=0$ may not also be a solution of $\nabla_{\beta}Y=0$.
The first equation is imposes a formula for $\partial_{\alpha} Y$ and the second equation imposes a formula for $\partial_{\beta}Y$. 
It is not guaranteed that $\partial_{\alpha} \partial_{\beta}Y = \partial_{\beta}\partial_{\alpha} Y$. 
When we can do this, the equations (or connection) is integrable. 

As lifting paths is really about general derivations/vector fields.
We now formulate this commutation problem more generally.
First recall that if $\theta_1,\theta_2 \in T_X$ then the commutator
$$[\theta_1,\theta_2] = \theta_1 \theta_2 - \theta_2 \theta_1,$$
is also a derivation.
This is part of the fact that $T_X$ is a sheaf of Lie algebras.
The infinitesimal version of the curvature problem (taking the limit over small paths) leads to a ``curvature zero'' condition.
\begin{problem}
	When does $E$ have a well-defined structure of a $T_X$-module via $\nabla$. 
	In other words, when is it the case that for all $\theta_1,\theta_2 \in T_X$ the following equation holds:
	\begin{equation}\label{E:commutation}
	[\nabla_{\theta_1},\nabla_{\theta_2}] = \nabla_{[\theta_1,\theta_2]}?
	\end{equation}
\end{problem}

The failure of  the commutation relation \eqref{E:commutation} is measured by the curvature tensor which we now define.
\begin{definition}
	The map $R_{\nabla}:T_X\otimes T_X \to \End(E)$ given by 
	$$ R_{\nabla}(\theta_1,\theta_2)(s) = \nabla_{\theta_1}(\nabla_{\theta_2}(s)) - \nabla_{\theta_2}(\nabla_{\theta_1})(s) - \nabla_{[\theta_1,\theta_2]}(s) $$
	is called the \emph{curvature tensor}.
\end{definition}
When the context is clear we will just use $R=R_{\nabla}$ so that we don't have to keep writing the subscript $\nabla$.
In what follows we will soon see that $R$ as an alternative description in terms of  $\nabla^2$.
To see that this even makes sense note that $\nabla^2:E \to \Omega^2_X\otimes E$.
Hence $\nabla^2$ can eat two tangent vectors $\theta_1$ and $\theta_2$ and a section $s$ and spit out another section.
The subsequet theorem will prove that for all $s\in E$ and $\theta_1,\theta_2 \in T_X$ we will have 
$$ R(\theta_1,\theta_2)(s) = \langle \nabla^2(s), \theta_1 \wedge \theta_2 \rangle,$$
where the pairing $\langle -,-\rangle$ is between $\Omega_X^2$ and $T_X\wedge T_X$.

\begin{exercise}
	In local coordinates $t^{\alpha}$ and $s_i$ find an expression for $R_{\nabla}(\partial_{\alpha},\partial_{\beta})(s_j)$ in terms of the Christoffel symbols.
\end{exercise}

We now can give our equations for integrability/flatness.

\begin{theorem}[Integrability Conditions]\label{T:integrability-conditions}
	Let $E$ be a vector bundle of rank $n$ on a complex manifold $X$.
	Let $\nabla$ be a connection on $E$.
	The following are equivalent:
	\begin{enumerate}
		\item \label{I:nabla-squared}Iterating $\nabla$ twice is zero: $\nabla^2=0$.
		\item \label{I:vanishing-curvature} The curvature tensor is identically zero: $R_{\nabla}=0$.
		\item \label{I:connection-one-form} For every set of local coordinate so that $\nabla = d+\omega$ locally with $\omega$ a connection one form with respect to these coordinates we have 
		$$ d(\omega) + \omega \wedge \omega =0.$$
		\item \label{I:potentials} For every set of local coordinate so that $\nabla = d+\omega$ with $\omega = A_{\alpha} dt^{\alpha}$ and $A_{\alpha} \in M_n(\Ocal_X(U))$ one has 
		$$ \dfrac{\partial A_{\alpha}}{\partial t^{\beta}}- \dfrac{\partial A_{\beta}}{\partial t^{\beta}} = -[A_{\alpha},A_{\beta}].$$
	\end{enumerate}
	More generally 
	$$d(\omega) + \omega \wedge \omega= G_{\alpha\beta} \ dt^{\alpha}\wedge dt^{\beta}, \quad G_{\alpha\beta} = \frac{1}{2}\left(  \partial_{\alpha} A_{\beta} - \partial_{\beta} A_{\alpha} + [A_{\alpha},A_{\beta}]\right)$$
	 is a local expressions of the curvature $2$-form.
\end{theorem}
\begin{proof}
	We will first prove \eqref{I:vanishing-curvature} if and only if \eqref{I:potentials}.
	Let $U\subset X$ be an open set with coordinates $t^{1},\ldots,t^m$ and where $E(U)$ is has a basis $s_1,\ldots,s_n$. 
	Since every expression of $R(\theta_1,\theta_2)$ can be expressed in terms of $R(\partial_{\alpha},\partial_{\beta})$, we just need to compute $R(\partial_{\alpha},\partial_{\beta})$.
	The following computation is a Weyl algebra computation (meaning everything is viewed as an operator).
	Importantly, for a matrix $A$ and a derivative $\theta$ we have $\theta A = A\theta + \theta(A)$ there $\theta(A)$ denotes the application of $\theta$ to the entries of $A$ and $\theta A$ means the application of $A$ and an operator followed by $\theta$ as an operator.
	\begin{align*}
	R(\partial_{\alpha},\partial_{\beta}) =& \nabla_{\alpha}\nabla_{\beta} - \nabla_{\beta}\nabla_{\alpha} - \nabla_{[\partial_{\alpha},\partial_{\beta}]} \\
	=&\nabla_{\alpha}\nabla_{\beta} - \nabla_{\beta}\nabla_{\alpha} \\
	=&(\partial_{\alpha}+A_{\alpha})(\partial_{\beta} + A_{\beta}) - (\partial_{\beta}+A_{\beta})(\partial_{\alpha} + A_{\alpha}) \\
	=& (\partial_{\alpha} A_{\beta} +A_{\alpha}\partial_{\beta}) + A_{\alpha}A_{\beta} + \partial_{\alpha} \partial_{\beta}- \left( (\partial_{\beta} A_{\alpha} +A_{\beta}\partial_{\alpha}) + A_{\beta}A_{\alpha} + \partial_{\beta} \partial_{\alpha} \right) \\
	=& [A_{\alpha},A_{\beta}] + A_{\alpha} \partial_{\beta}-A_{\beta}\partial_{\alpha} \\
	& + A_{\beta}\partial_{\alpha} + \partial_{\alpha}(A_{\beta}) \\
	& + A_{\alpha} \partial_{\beta} + \partial_{\beta}(A_{\alpha}) \\
	=& \partial_{\alpha}(A_{\beta}) - \partial_{\beta}(A_{\alpha}) + [A_{\alpha},A_{\beta}].
	\end{align*}

	This implies (in coordinates) 
	$$ R(\partial_{\alpha},\partial_{\beta})=\partial_{\alpha}(A_{\beta}) - \partial_{\beta}(A_{\alpha}) + [A_{\alpha},A_{\beta}].$$
	
	Let's see that \eqref{I:connection-one-form} and \eqref{I:potentials} are equivalent. 
	We recall that locally $\omega=A_{\alpha}dt^{\alpha}$. 
	We will compute $d\omega + \omega\wedge \omega$.
	
	In what follows we are going to use the following trick: if $H_{\alpha\beta}$ is an antisymmetric tensor, e.g. there is some free $R$-modules $V$ with a basis $v^{\alpha}$ such that $H_{\alpha\beta} v^{\alpha}\wedge v^{\beta} \in V\wedge V$ then $H_{\alpha\beta} = -H_{\beta\alpha}$ and 
	$$H_{\alpha\beta} = \frac{1}{2}(H_{\alpha\beta} - H_{\beta\alpha} ).$$
	This will be used when we compute coefficients of differential forms ``coordinate tensor'' is not alternating. 
	You can just antisymmetrize.
	If you don't understand what this means now, it should become apparent in the following computation.
	
	Write $\omega = A_{\alpha} dt^{\alpha}$ hence in local coordinates.
	We compute:
	\begin{align*}
	\omega \wedge \omega &= A_{\alpha} \ dt^{\alpha} \wedge A_{\beta}\ dt^{\beta} 
	= A_{\alpha} A_{\beta} \ dt^{\alpha} \wedge dt^{\beta} =\frac{1}{2} (A_{\alpha} A_{\beta} - A_{\beta} A_{\alpha}) \ dt^{\alpha}\wedge dt^{\beta}\\
	d(\omega) &= d (A_{\beta} \ dt^{\beta}) = \dfrac{\partial A_{\beta}}{\partial t^{\alpha}}\ dt^{\alpha} \wedge dt^{\beta} =  \frac{1}{2}\left(\dfrac{\partial A_{\beta}}{\partial t^{\alpha}}-\dfrac{\partial A_{\alpha}}{\partial t^{\beta}} \right )\ dt^{\alpha}\wedge dt^{\beta} 
	\end{align*}
	which gives our desired identity.
	
	We will now show the computation $\nabla^2=0$ in \eqref{I:nabla-squared} equivalent to $d\omega + \omega\wedge \omega=0$ in \eqref{I:connection-one-form}.
	The key idea is to use the relations for the Weyl algebra of the exterior algebra (Lemma~\ref{L:exterior-weyl-algebra}).
	As elements of $R[d]$ where $R = M_n(\Omega_X^{\bullet})$ we have 
	\begin{align*}
	\nabla^2 =& (d+\omega)(d+\omega) = d^2+\omega\wedge d + d\omega + \omega \wedge \omega \\
	&= \omega \wedge d + (d(\omega) - \omega \wedge d ) + \omega \wedge \omega \\
	&= d(\omega) + \omega \wedge \omega.
	\end{align*}
	This proves the desired equality.
\end{proof}

%See Theorem~\begin{T:frobenius-integrability}.


\begin{exercise}
	Photons are the force carrying particles for the electro-magnetic force field (changes in force are mediated by the emission of light). 
	In this exercise the speed of light will be $c=1$.
	Space time is encoded by a manifold $X$ and the vector bundle is rank one corresponding to the Lie algebra of $U(1)$. 
	In local coordinates (some chart of spacetime) we let $E=(E_1,E_2,E_3)$ denote a electric field and $B=(B_1,B_2,B_3)$ denote a magnetic field. 
	Both $E$ and $B$ are functions of $(t,x,y,z)$.
	They are encoded in the Faraday tensor $F\in \Omega^2_X$ given by 
	$$F= E_1dx \wedge dt + E_2\ dy \wedge dt + E_3 dz \wedge dt + B_1 dy \wedge dz + B_2 dz \wedge dx + B_3 dx \wedge dy.$$
	If we order the variables $(x^0,x^1,x^2,x^3)=(t,x,y,z)$ then Faraday tensor is $F_{\mu\nu} = \partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}$ is  or as an antisymmetric matrix is
	$$(F_{\mu\nu})=\begin{bmatrix}
	0     &  E_1 &  E_2 &  E_3 \\
	-E_1 &  0     & -B_3   &  B_2    \\
	-E_2 &  B_3  &  0     & -B_1  \\
	-E_3 & -B_2   &  B_1  &  0
	\end{bmatrix}.$$
	Since we are in rank one everything commutes and there is nothing really interesting to say about the curvature equations. 
	Maxwell's equations become $dF=0$.
\end{exercise}

I'm tempted to put an exercise about Yang-Mills equations here but haven't done so. 
Perhaps you should just google these now and see all of these curvature tensors appearing everywhere.

\section{Plemelj's Construction (unstable)}

\taylor{I need to add my notes here,
This has to do with gluing together local solutions.

}

\section{Riemann-Hilbert Correspondences}\label{S:rhcs}
By \emph{a} Riemann-Hilbert correspondence we will mean a theorem which gives some equivalence between a category of differential equations of some flavor with a category of representations of fundamental groups of some flavor. 
This may not even be a functor but just a bijection of particular sets of some property. 

The general strategy for a functorial version is to have some category of connections or differential equations on some space $X$ which we will call $\Conn(X)$ (e.g. holomorphic connections, Fuchsian connections, holomorphic systems, Fuchsian systesm, holonomic $D$-modules), a category of local systems $\LocSys(X)$ (essentially solutions of the differential equations, perverse sheaves), and a category of representations of the fundamental group $\Repn(X)$. 

The idea is then to pass from differential equations to representations through ``local systems''.
\begin{equation}
\Conn(X) \cong \LocSys(X) \cong \Repn(X) .
\end{equation}

Before going any further we describe what a local system is. 
This conversation continues in section \ref{S:rhc-strat} which the reader should feel free to skip immediately to on the first reading. 


%%%%%%%%%%%%%%%%%%%%%%%
\section{Local Systems and Representations of $\pi_1(X)$}
%%%%%%%%%%%%%%%%%%%%%%%
In this section we are going to introduce the notion of a local system and then prove that the category of local systems on $X$ is equivalent to the category of finite dimensional representations of the fundamental group. 

Along the way we are going to work out what happens with the holomorphic differential equations on $\PP^1\setminus S$ where $S$ is a finite collection of points. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local Systems}
%%%%%%%%%%%%%%%%%%%%%%%
In what follows we will let $X$ be a topological space. 
For a ring or abelian group $K$ we will let $\underline{K}_X$ be the sheaf associated to the presheaf  constant presheaf defined by 
$$ U\mapsto F(U) = \begin{cases}
K, & U\neq \emptyset ,\\
0, & U = \emptyset 
\end{cases}$$
The presheaf is not a sheaf because it is possible for there to be two disjoint open sets $U_1$ and $U_2$ which means that any element $a \in F(U_1)$ and $b\in F(U_2)$ agree on their restriction but don't lift to a common element in $F(U_1 \cup U_2)$.
This is fixed by allowing for elements ``like'' $a\oplus b \in \underline{K}_X(U_1 \cup U_2)$  so that the sheaf axioms are satisfied. 
On topological spaces (as opposed to general sites) then $\underline{K}_X(U) = \operatorname{Cont}(U, K)$, the collection of continuous map from $U$ to $K$ where $K$ is given the discrete topology.

\begin{definition}
	Let $X$ be a topological space and let $K$ be a field. 
	A \emph{$K$-local system} over $X$ is a sheaf $L$ valued in finite dimensional $K$-vector spaces such that $L \cong \underline{K}_X$ locally. 
\end{definition}
This forms a category and morphisms are morphism of sheaves of $\CC$-linear vector spaces. 

\begin{exercise}\label{EX:locally-constant}
	Let $X$ be a topological space and let $V$ be an $R$-module for some ring $R$. 
	Show that the constant sheaf $\underline{V}_X$ on $X$ is the same thing as the sheaf  $$U\mapsto\Cont(U,V), \quad U \subset X \mbox{ open }$$ 
	where $\Cont$ denotes continuous maps.
	Here $V$ is given the discrete topology. 
\end{exercise}
The above exercise is important. 
Let $X$ be a topological space and let $f:X\to Y$ be a continuous map to a discrete topological space $Y$.
Then $f^{-1}(\lbrace y\rbrace)$ is open for every $y \in Y$. 
This holds for every $y\in Y$ and in particular the map $f:X\to Y$ is \emph{locally constant}. 

The main example of a local systems comes from solutions of differential equations. 
In fact this example is why the definition even exists. 
Consider the first order holomorphic system defined on $U \subset \CC$, given by 
$$ Y' = A(t) Y, \quad A \in M_n(\hol(U)).$$
For every $V \subset U$ open define  
$$ L(V) = \lbrace Y \in \hol(U)^{\oplus n} \colon Y' = A(t) Y \rbrace.$$
We know that at each $t_0 \in U$ we have an isomorphism 
$$ \lbrace Y \in \CC\langle t-t_0 \rangle^{\oplus n} \colon Y'=AY \rbrace \cong \CC^n $$
since solutions form a finite dimernsional $\CC$-vector space. 
By considering representatives of each basis element in $\CC\langle t-t_0 \rangle^{\oplus n}$ we know that there exists some $V \subset U$ containing $t_0$ where 
$$ L(V) \cong \CC^{\oplus n}.$$
This proves that $L$ is a local system. Note in particular that 
$$ L(V) = \Phi_V \cdot \CC^{\oplus n } $$
where $\Phi_V$ is a fundamental matrix valid on $V \subset U$. 

\subsection{The Espace \'Etale: Pullbacks and Pushforwards}

I was tempted to add the following exercise without comment:
\begin{exercise}
	The category $\LocSys(X)$ the category of local systems of finite dimensional $\CC$-vector spaces makes sense and for every morphism of topological spaces $f:X\to Y$ one a functor $f^{-1}:\LocSys(X) \to \LocSys(Y)$.
\end{exercise}
I thought about it a second time and concluded this isn't very nice.

We are going to need to talk about inverse images of pullback of local systems so I want to say a couple words about inverse images of sheaves in general. 
Let $\Gcal$ be a sheaf on a topological space $Y$. 
For a morphism of topological spaces $f:X\to Y$ we want to define the inverse image sheaf $f^{-1}\Gcal$ (sometimes denoted $f^*\Fcal$) which is the left adjoint of $f_*$. 
Here $f_*$ is the direct image sheaf and it turns sheaves $\Fcal$ on $X$ into sheaves $f_*\Fcal$. 
These direct image sheaves are super easy to describe: given $V \subset Y$ we have $(f_*\Fcal)(V) = \Fcal(f^{-1}(V))$. 
That is it.
The sheaves $f^{-1}\Gcal$, are no so simple. 
Harshorne, for example, defines $f^{-1}\Gcal$ to be the sheaf associated to the presheaf
$$ U\mapsto \varinjlim_{V\supset f(U)} \Gcal(U). $$
Yuck! I think this definition sucks and is hard to work with. 

For every sheaf $\Fcal$ on a topological space $X$, I'm going to introduce a topological space $\overline{\Fcal}$ and a morphism of topological spaces $\pi: \overline{\Fcal} \to X$ that is going to make our life easier. 
This space is called the \emph{espace \'etale} and has the important property that sections of $\pi$ over an open set $U$ correspond to elements of $\Fcal(U)$.
By a section over an open set $U$ we mean 
$$ \Gamma_X(\mathcal{F})(U) :=\lbrace s: U \to \overline{\Fcal} \colon \pi s = \id_U \rbrace. $$
That is $\Gamma_X(\mathcal{F}) \cong \Fcal$ as sheaves. 
I should mention that in the description $s$ is just a continuous morphism of topological spaces. 
The situation is pictured in Figure~\ref{F:section}.
\begin{figure}[h]\label{F:section}
	\begin{center}
		\includegraphics[scale=0.75]{section.eps}
	\end{center}
	\caption{A picture of the espace \'etale for a sheaf $\mathcal{F}$. 
		Pictures is a map $s$ over an open set $U$ such that $\pi s=\id_U$.}
\end{figure}

\begin{definition}
	Given a sheaf $\Fcal$ on a topological space $X$ we define the \emph{espace \'etale} of $\Fcal$ to be the topological space $\overline{\Fcal}$ whose underlying set is 
	$$ \overline{\Fcal} = \coprod_{x\in X} \Fcal_x = \lbrace (x,t) \colon t\in \Fcal_x \rbrace, $$ 
	and whose topology is generated in the following way: for each $s \in \Fcal(U)$ for $U\subset X$ open declare
	$$ W(U, s) = \lbrace  (x,s_x) \colon s_x \mbox{ germ of $s$ at $x\in U$ }\rbrace $$
	to be open and take the topology on $\overline{\Fcal}$ to be the smallest topology so that the $W(U,s)$ are open for every $s\in \Fcal(U)$. 
\end{definition}

\begin{exercise}
	Prove that $\Gamma_X(\mathcal{F})$ and $\Fcal$ are isomorphism as sheaves. 
\end{exercise}

We now turn to the point of introducing this construction. 
Given a morphism of topological spaces $f:X\to Y$ and a sheaf $\Gcal$ on $Y$ we define take define $f^{-1}\Gcal$ to by its espace \'etale $\overline{f^{-1}\Gcal}$ which is just the pullback of the $\overline{\Gcal}\to Y$ to $Y$.
That is 
$$ \overline{f^{-1}\Gcal} = \overline{\Gcal} \times_Y X,$$
where the fiber product is taken in the category of topological spaces. 
Similarly, $f_*\Fcal$ for a sheaf $\Fcal$ on $X$ is the pushout 

\begin{exercise}
	Using this definition of $f^{-1}\Gcal$ show that $(f^{-1},f_*)$ are an adjoint pair of functors. 
\end{exercise}


\subsection{Every Riemann Surface at Once}
Using the espace \'etale construction in the case of the sheaf of holomorphic functions in the complex plane give something amazing. 
Here points are really $(z_0,s_{z_0})$ where $z\in \CC$ and $s_{z_0} \in \CC\langle z-z_0\rangle$ is a germ of a holomorphic function on $z_0$.
This for example could be a germ of some branch of $\sqrt{z}$ at the points $z=1$. 
Then this continues to all other branches of $\sqrt{z}$. 

This branch determines a leaf or connected component of $\mathcal{X}$ where 
$$ \pi: \mathcal{X} =\overline{ \Ocal_{\AA^{1,\an}} } \to \CC,$$
is the espace \'etale for the sheaf of holomorphic functions on $\CC$. 

The topology of the espace \'etale is crazy looking. 
For every germ of a holomorphic function over a point $z_0$ there is an element $\mathcal{X}$. 
So the fibers are uncountable. 
Also, for any element $f$ of any fiber we can analytically continue germ around to get a connected component $\mathcal{X}(f) \subset \mathcal{X}$. 
This is actually a connected Riemann surface that maps to $\CC$ and is the Riemann surface of the germ $f$.
It may not be surjective onto $\CC$ since not every germ admits an analytic continuation to every points of $\CC$ (some ``overconvergent'' functions have a natural boundary) but it will give something. 

So that is it. The espace \'etale $\mathcal{X} \to \CC$ has as connected components all of the possible Riemann surfaces that come from analytic continuation of a germ, with many of them appearing with uncountable multiplicity.


\subsection{\'Etale Morphisms, Coverings, and Locally Trivial Sheafs}
The espace \'etale is very strange and to describe it geometrically we are going to recall the notions of \'etale and covering. 
The difference between these two notions is pictured in Figure~\ref{F:etale-vs-covering}.
In particular we have drawn something a priori horrible (a posteriori not so horrible) corresponding to what we might thing an espace \'etale of a sheaf might look like.
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.33]{etale-vs-covering.eps}
	\end{center}
	\caption{Both \'etale and covering morphisms are local homeomorphisms, it is just that \'etale morphisms can be weirder and include things in the fibers like lines with two origins which are not Hausdorff.}\label{F:etale-vs-covering}
\end{figure}

\begin{definition}
	In what follows we consider a morphism $f: Y\to X$ a morphism of topological spaces. 
	\begin{enumerate}
		\item We say that $f$ is \emph{\'etale} if and only if 
		\begin{enumerate}
			\item $f$ is locally a homeomorphism.
			\item For all $x\in X$, $f^{-1}(x)$ has the discrete topology. 
		\end{enumerate}
		\item We say that $f$ is a \emph{covering map} if and only if it is \'etale and locally trivial. 
	\end{enumerate}
\end{definition}
Covering spaces are just like the covering spaces you have seen from Hatcher but perhaps the fibers are larger (and too large of fibers indicates in many cases that there needs to be multiple connected components). 
\'{E}tale morphisms are like coverings by includes things like the an arbitrary disjoint union of any collection of open subsets of $X$; the fibers don't need to have any particular uniform size or anything like that. 
Also there can be issues with separatedness where you can have things like the line with two origins appearing above an open set.


We now show the espace \'etale is indeed gives an \'etale morphism. 
The theorem below is actually a good exercise and you should try it.
\begin{theorem}
	If $F$ is a sheaf of sets on a topological space $X$ then the morphism $\pi: \overline{F}\to X$ from the espace \'etale of $F$ to $X$ is an \'etale morphism.
\end{theorem}
\begin{proof}
	The morphism $\pi$ is given by $(x,s_x)\mapsto x$. 
	By definition the $F_x = \varinjlim_{U \owns x} F(U)$ we know there exists some $U$ such that $s\in F(U)$ restricts to $s_x \in F(U)$. 
	By definition of $\overline{F}$ we have $\lbrace (x,s_x) \colon x\in U, s_x = (s)_x \rbrace$ mapping homoemorphically onto $U$. 
	We are using $(s)_x$ to denote taking the stalk of $s \in F(U)$ at $x$.
	
	Given two points $(x,t_x)$ and $(x,s_x)$ corresponding sets above separate these points giving the fiber above $x$ the discrete topology.
\end{proof}

If $F$ is a sheaf of sets on $X$ then $\pi: \overline{F} \to X$ is an \'etale morphism yet $\pi^{-1}(x) = F_x$. 
In general \'etale morphisms and covering morphisms can seem awful. 
This is horribly large. 
Here $F_x$ is the stalk at the point. 
In the case when $F = \Ocal_X$ where $X$ is a complex manifold then $\Ocal_{X,x}$ is like a ring of power series which is uncountable, so the fiber is an uncountable set in the discrete topology. 


The following is Sabbah exercise 15.1 + a corollary around there. 
One needs to recall the Fundamental Theorem of Galois Theory for Covering Spaces: Given a connected space $X$, every subgroup of $\pi_1(X)$ comes from a connected covering of $X$ 
Conversely given a covering, its connected components correspond to subgroups of $\pi_1(X)$. 
The construction is to quotient the universal cover by the group $\Pi \subset \pi_1(X)$ which acts discretely.

\begin{theorem}
	Let $X$ be a connected topological space. 
	Let $F$ be a sheaf of sets on $X$.
	\begin{enumerate}
		\item The sheaf $F$ is constant if and only if $\pi:\overline{F}\to X$ is trivial in the sense that $\overline{F} \cong X\times F_0$ for some fixed $F_0$ with the discrete topology. 
		\item The sheaf $F$ is locally constant if and only if $\pi: \overline{F} \to X$ is a covering. 
		\item If $X$ is simply connected then any locally constant sheaf is constant.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Suppose that $F$ is constant with $F(U)=F_0$ for all $U$ nonempty.
	Then for all $x$ and all $y$ we have $F_x=F_y=F_0$.
	Then $\overline{F} \cong X \times F_0$. 
	The map is given by $(x,s_x) \mapsto (x,s_x)$ since there is a canonical isomorphism between $F_x$ and $F_0$.
	
	Conversely, take sections and use the property that continuous maps to discrete things are constant (Exercise~\ref{EX:locally-constant}).
	
	The second part follows from the first as we restrict to an open set. 
	Locally constant implies locally trivial. 
	
	We now use the remarks proceeding the statement of the theorem about fundamental groups. 
	If $X$ is simply connected then $\pi_1(X)=1$. 
	This means that there are no non-trivial coverings. 
	This means that every sheat is isomorphic to $X$ and hence $\overline{F} \cong \coprod_{d\in D} X=X\times D$ where $D$ is some space with the discrete topology. 
	This is the constant sheaf of sets that we are after.
\end{proof}

We now use this to disentangle the relationship beween $(E,\nabla)$ and $E^{\nabla}$.

\subsection{Local Systems vs Vector Bundles}
In this section we discuss the difference between a local system associated to some integrable $(E,\nabla)$ and the vector bundles $E$ itself.
In what follows we will let $L = E^{\nabla}$ and try to compare $\overline{L}$ and the physical vector bundle $E$.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.33]{connected-components.eps}
	\end{center}
	\caption{The shredding of the vector bundle $E$ by the foliation, giving us the espace \'etale.}\label{F:connected-components}
\end{figure}


Since there is sometimes some confusion in notation I want to review the difference between stalks of locally free sheafs and fibers of vector bundles. 
As stated previous we often conflate $\mathcal{E}$ the sheaf of sections of a vector bundle $E$ with $E$ itself and just write $E$ in place of $\mathcal{E}$.
Technically, $E$ is a geometric object with a morphism $f: E\to X$ a morphisms of spaces (schemes, complex manifolds, whatever).  
Also, technically, $\mathcal{E}$ is a locally free sheaf of $\Ocal_X$-modules. 
These two things live in different categories and $E$ ``represents'' $\mathcal{E}$.

With this conflation in mind there are two possible meanings for $E_x$ given $x\in X$ that can be conflated. 
It can mean either the stalk 
$$E_x = \varinjlim_{U \owns x} \mathcal{E}(U)$$ 
or the fiber  
$$E_x = f^{-1}(x).$$
In the case it is the stalk we are thinking of $E$ as a locally free sheaf and in the case of the fiber we are thinking about $E \to X$ as a physical vector bundle which is a space. 
The stalk description is huge and is the module over a local ring $\Ocal_{X,x}$, which when working with complex manifolds is a ring of power series so an infinite dimensional vector space. 
In the case of a fiber $E_x$ is a finite dimensional complex vector space. 
These are not the same.
Not even close. 
One needs to reduce the stalk modulo the maximal ideal of $\Ocal_{X,x}$ to get a comparison.

Now for a locally free sheaf, things start to get a little closer.
The picture of the discussion that follows is Figure~\ref{F:connected-components}.
In this discussion we restrict our attention to $X$ being a complex manifold.
If $E$ a vector bundle of rank $n$ (which we think of as a physical vector bundle) then $E_x$ is a vector space of rank $n$. 
For the espace \'etale $\pi:\overline{L}\to X$ of a local system $L$ we have $L_x = \overline{L}_x$, the fiber is the stalk. 
Also $L_x \cong \CC^n$ so we have a vector space of rank $n$ as the fiber of the espace \'etale. 
The big difference here is the topology!
The topology of $E_x$ is that of usual topological vector space. 
The topology of $\overline{L}_x$ discrete.
What have we done?
Well, the local system $L$ parametrizes local solutions of of our differential equation.
They are a linear combination of a basis of solutions.
The connection defined a flow on the space $E$ and the leaves of this flow are the solutions of the differential equation. 
What $\overline{L}$ is (at least locally), is the shredding of $E$ by these leaves (again see Figure~\ref{F:connected-components}). 
So $E$ has sort-of been ripped apart and reassembled initial condition by initial condition to give uncountably many fibers sitting discretely together.

\section{Monodromy Representations and Local Systems}\label{S:locsys-to-repn}
Let $X$ be a topological space which is connected and locally path connected so that $\pi_1(X,x_0)$ makes sense. 
Let $L$ be a local system on $X$ and let $\gamma:[0,1] \to X$ be a continuous path contained in $X$. 
Then $\gamma^*L$ is a local system on $[0,1]$ which is trivial.
In seeing this, Exercise~\ref{EX:locally-constant} is important.
Furthermore every germ $v \in (\pi^*L)_0$ extends uniquely to $(\gamma^*L)([0,1])$ via monodromy.
In particular there is a morphism 
$$ M_{\gamma}: (\gamma^*L)_0 \to (\gamma^*L)_1.$$
Since $(\gamma^*L)_t \cong L_{\gamma(t)}$ for each $t$ if $\gamma$ is a closed path starting and ending at $x_0\in X$ then $(\gamma^*L)_0 \cong L_{\gamma(0)} \cong L_{x_0} \cong L_{\gamma(1)} \cong (\gamma^*L)_1$.
This then means that $M_{\gamma}$ induces an automorphism of $L_{x_0}$.
This defines the representation associated to the pair $(L,x_0)$ consisting of a local system $L$ and a point $x_0 \in X$
\begin{definition}
	The \emph{monodromy representation} associated to the $(L,x_0)$ is 
	$$\rho_{(L,x_0)}: \pi_1(X,x_0) \to \Aut(L_{x_0}), \quad \gamma \mapsto M_{\gamma}, $$
	as described above. 
\end{definition}

We now want to give a comparison between the representation above and the monodromy representation we had previously discussed in dimension one. 

Recall that the category of $K$-representations of a group $\Pi$ is the category of $K[\Pi]$-modules and when we talk about two representations being isomorphic we talk about them being isomorphic as $K[\Pi]$-modules. 
We are interested in the case when $\Pi = \pi_1(U,t_0)$ for some $U\subset \CC$. 

\begin{theorem}
	Consider a holomorphic system on $U\subset \CC$ of the form 
	$$ Y' = A(t) Y, \quad A(t) \in M_n(\hol(U)). $$
	Let $L_A$ be the local system associated to this system. 
	Let $\rho_A$ be the monodromy representation associated to the fundamental matrix $\Phi(t)$ satisfying $\Phi(t_0)=I_n$. 
	Then $\rho_{L_A,t_0} \cong \rho_A$ as reprentations of $\pi_1(U,t_0)$.
\end{theorem}
\begin{proof}
	Our aim is to compute $\rho_{L_A}$ (we will drop the base point $t_0$ from the notation for convenience). 
	At a point $t_1\in U$ along a curve $\gamma_1$ starting at $t_0$ and ending at $t_1$ we have 
	$$ \Phi_{\gamma_1} \CC^n \cong L_{A,t_1},$$
	where $\Phi_{\gamma} \in \GL_n(\CC\langle t-t_1\rangle)$ is the local fundamental matrix. 
	The representation associated to the local system gives 
	$$\begin{tikzcd}
	\CC^n & \arrow[l, "\Phi ^{-1}"] \Phi \CC^n \arrow[r] & \Phi_{\gamma} \CC^n \arrow[r, "\Phi_{\gamma}^{-1}"] & \CC^n  \\
	& (\gamma^*L_A)_0 \ar[u, equal] & (\gamma^*L_A)_1 \ar[u, equal]& 
	\end{tikzcd}
	$$
	Here $v= \Phi c \mapsto \Phi_{\gamma} c = \Phi M_{\gamma} c$ implies that $\Phi M_{\gamma} \Phi^{-1}$ is the action on the stalk. 
	In triviallized coordinates we have 
	$$ v \mapsto \Phi v \mapsto \Phi M_{\gamma} \Phi^{-1} \Phi v \mapsto \Phi_{\gamma}^{-1} M_{\gamma} v \mapsto M_{\gamma}^{-1} \Phi M_{\gamma} v.$$
\end{proof}
In higher dimensions this is how we are going to define the representation associated to a connection. 
That is to every vector bundle $E$ with integrable connection $\nabla^2=0$ we have the associated local system $E^{\nabla}$ defined by 
 $$ E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s)=0 \rbrace, \quad U \subset X \mbox{ open }. $$
The very definition of integrability is exactly so that the system of equations $\nabla(s)=0$ has a local basis of solutions. 
More precisely if $t^1,\ldots,t^m$ are local coordinates for $X$ and $s_1,\ldots,s_n$ are local basis for $E$ then 
 $$ \nabla(s)=0 \quad \iff  \quad \nabla_{\partial_j}(Y) = \dfrac{\partial Y}{\partial t^j} + A_j Y =0, \quad 1\leq j \leq m,$$
where $Y\in \Ocal_X(U)^{\oplus n}$ is a presentation of $s \in E(U)$ under the trivialization given by the local basis and $A_jdt^j$ is the connection 1-form in local coordinates.

\begin{definition}
	Let $(E,\nabla)$ module be a module with integrable connection. 
	The construction $(E,\nabla)\mapsto E^{\nabla}$ is \emph{local system associated to $(E,\nabla)$} 
\end{definition}

\begin{definition}
	Let $(E,\nabla)$ be an integrable connection on a complex manifold $X$. 
	We define the \emph{monodromy representation} of $(E,\nabla)$ to be the monodromy representation of $E^{\nabla}$.
\end{definition}

This describes the functors
 $$ \Conn(X) \to \LocSys(X) \to \Repn(X).$$
The connection, goes to a local set of solutions (a local system), which by \S~\ref{S:locsys-to-repn} gives rise to a representation. 
Spoiler: for holomorphic vector bundles this will be an equivalence of categories. 
The map $\LocSys(X) \to \Repn(X)$ is always going to be an equivalence of categories. 


\subsection{Local Systems and Representations are Equivalent}\label{S:locsys-repn}
In this section we show that for a general topological space the category of representations is equivalent to the category of local systems. 

First some notation: let $\Pi$ be a group. 
We will let $\Mod^{\fin}_{\CC[\Pi]}$ denote the category of $\CC[\Pi]$-modules which are finite dimensional as $\CC$-vector spaces. 
We will often conflate a representation $\rho: \Pi \to \GL(V)$ with its underlying  $\CC[\Pi]$-module, which we also denote by $V$.  

Now we give our theorem.
\begin{theorem}
	Let $X$ be a topological space which admits a fundamental group and let $\Pi = \pi_1(X,x_0)$ for $x_0\in \CC$. 
	The category of local systems on $X$ is equivalent to the category of finite dimensional complex representations of the fundamental group 
	$$ \LocSys(X) \xrightarrow{\sim} \Repn(X), \quad L \mapsto \rho_{L,x_0},$$
	Here $\Repn(X)=\Mod_{\CC[\Pi]}^{\fin}$ of finite dimensional complex representations of the fundamental group (= $\CC[\Pi]$-modules).
\end{theorem}
\begin{proof}[Proof Sketch]
	The quasi-inverse is given by the so-called suspension construction. 
	From representaions to local systems is the map
	 $$ V \mapsto \widetilde{X} \times_{\Pi} V = \overline{L},$$
	which gives the espace \'etale of the local system $L$. 
	Here, $\Pi = \pi_1(X,x_0)$, $V$ is the underlying vector space of the $\Pi$-representation (it must be given the discrete topology), $\widetilde{X} \to X$ is the universal cover, and $\times_{\Pi}$ denotes the amalgamated product.
	
	We give some more details.
	Let $\rho: \Pi \to \GL(V)$ be a finite dimensional complex representation of $\Pi$.
	Note that we have an action of $\Pi$ on both $\widetilde{X}$ and on $V$ and hence on the complex manifold $\widetilde{X}\times V$ where the action is given by 
	$$(\gamma, (\widetilde{x},v))\to (\gamma(\widetilde{x}), \rho(\gamma)(v)).$$
	Note that since the action by deck transformations preserves fibers of $f$ we have that $\Pi$ also acts on $f^{-1}(U) \times V$ for every open subset $U \subset X$. 
	
	Define the constant sheaf $ \widetilde{L} = \underline{V}_{\widetilde{X}}.$
	We claim the total space of this constant sheaf is $\widetilde{X}\times V$ provided we give $V$ the discrete topology; that is, for every $\widetilde{U} \subset \widetilde{X}$ there is a bijection (see exercise~\ref{EX:total-space})
	\begin{equation}\label{E:total-space}
	\widetilde{L}(\widetilde{U})  \cong \lbrace \mbox{sections of $\pi_1: \widetilde{X}\times V \to \widetilde{X}$ above $\widetilde{U}$ } \rbrace.
	\end{equation}
	Hence there is an action of  $\Pi$ on $(f_*\widetilde{L})(U)$ for every $U$ subset $X$ and it makes sense to define $L(U)$ by the formula 
	$$ L(U) = (f_*\widetilde{L})(U)^{\Pi}. $$
	We claim that this is the local system with the corresponding monodromy represenation.
\end{proof}

\begin{exercise}\label{EX:total-space}
	In this exercise $M$ will be a topological space where fundamental groups make sense.\footnote{path connected, locally path connected}. 
	We will let $\Pi = \pi_1(M,x_0)$ for $x_0\in M$ some base point.  	
 Check \eqref{E:total-space} regarding the description of the total space of the local system.
\end{exercise}


%%%%%%%%%%%%%%%%%%%
\section{Holomorphic Riemann Hilbert Correspondence}\label{S:rhc-strat}
%%%%%%%%%%%%%%%%%%%
Let $X$ be a complex manifold. 
We have three categories.
\begin{align*}
\Conn(X) =& \mbox{ (holomorphic vector bundles on $X$ with integrable connections)} \\
\LocSys(X) =& \mbox{ (local systems on $X$) } \\
\Repn(X) =& \mbox{ (finite dimensional complex representations of $\pi_1(X,x_0)$ )}
\end{align*}
The holomorphic Riemann-Hilbert correspondence states that these categories are all equivalent. 
To estabilish this we will show $\Conn(X) \cong \LocSys(X)$ and that $\LocSys(X) \cong \Repn(X)$. 
In fact this section part was already established for general topological spaces in \S\ref{S:locsys-repn}.

The functor 
$$\Conn(X) \to \LocSys(X), \quad (E,\nabla) \mapsto E^{\nabla}$$ was already described.
Here $E^{\nabla}$ is ``the space of horizontal sections''. 
It doesn't hurt to repeat that 
$$ E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s) =0 \rbrace, \quad U\subset X \mbox{ open }.$$
The key condition here is integrability, which, \emph{by definition} is so that we have equatlity of mixed partials in the system of equations 
$$ \nabla(s)=0 \quad \iff  \quad \nabla_{\partial_j}(Y) = \dfrac{\partial Y}{\partial t^j} + A_j Y =0, \quad 1\leq j \leq m,$$
where $t^1,\ldots,t^m$ are local coordinates for $X$ and $s_1,\ldots,s_n$ are a local basis for $E$ giving rise to $s \mapsto Y$. 

The converse construction is as follows.
$$ \LocSys(X) \to \Conn(X), \quad L \mapsto (\Ocal_X\otimes_{\CC_X} L, \nabla_L),$$
where $\nabla_L(f\otimes v) = df \otimes v$ for $f\otimes v \in \Ocal_X \otimes L$, and then the definition is given by extending linearly.

Readers can check as much detail as they want that this construction is a quasi-inverse of $(E,\nabla)\mapsto E^{\nabla}$. 
The main idea here is that a basis of solutions always gives a matrix $\Phi$ which makes the connection equivalent to the trivial connection (see exercise~\ref{EX:equivalence-to-trivial}).

One of the major points here is that a connection is determined by its space of horizontal solutions. 
This allows us to define connections by declaring what their horizontal space is. 
When studying connections in characteristic $p$ on schemes we can define a canonical connection of the pullback of a vector bundle by the Frobenius $\nabla^{\can}$ by declaring the inverse image under the Frobenius to be horizontal.

\begin{theorem}[Holomorphic Riemann-Hilbert]
	We have the following equivalences of categories for $X$ a complex manifold:
	$$ \Conn(X) \cong \LocSys(X) \cong \Repn(X).$$
\end{theorem}

Later we will modify the category $\Conn(X)$ to get more information about the differential equations at the singularities. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Families of Differential Equations and Local Systems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Relative connections are the way we formalize families of differential equations depending on a parameters. 
This become relevant in section \S\ref{S:schlesinger} when we first start to discuss Schlesinger's equations and isomonodromy.
Here we seek a family of differential equations which when you deform a parameter you retain the same monodromy. 

\subsection{Relative Connections}
Recall that given a morphism $\pi:X \to S$ we think of the fibers over $s\in S$ which we denote as $X_s$ as a family. 
We will even call a morphism $\pi:X\to S$ a family. 
One way of thinking about relative connections are as families of differential equations (or more generally connections). 
Given morphism $\pi: X\to S$ of complex manifolds or schemes one can define a relative connection on a vector bundle $E$ on $S$ we make the following definition.
\begin{definition}
A \emph{relative connection} $\nabla_{X/S}$ on $E$ is an additive map 
 $$ \nabla_{X/S}: E \to \Omega_{X/S} \otimes E $$
such that $\nabla_{X/S}(f s) = df \otimes s + f \nabla_{X/S}(s)$ for all $f \in \Ocal_X$ and all $s \in E$. 
We say that the connection is \emph{integrable} if $\nabla_{X/S}^2=0$. 
\end{definition}
The fact that $df=0$ for $f \in \pi^{-1}\Ocal_S$ says that relative connections are $\pi^{-1}\Ocal_S$-linear.
Note that this makes sense: because we have a map $\Ocal_S \to \pi_* \Ocal_X$ there is an adjoint map $\pi^{-1}\Ocal_S \to \Ocal_X$ and it makes sense to talk about elements of $\pi^{-1}\Ocal_S$ as elements of $\Ocal_X$ (we are thinking about them via their image under the morphism just described). 

Morphisms of relative connections are morphisms of vector bundles which are equivariant with respect to their connection. 
We denote the category of integrable relative connections for $\pi:X\to S$ by $\Conn(X/S)$.

\begin{exercise}
	Given a relative connection check that $(E,\nabla)\vert_{X_s}$ makes sense as a module with integrable connection on $X_s$.
\end{exercise}

\subsection{Relative Local Systems}
Again we have a local system associated to an integrable relative connection
$$ (E,\nabla_{X/S}) \to E^{\nabla_{X/S}} $$
where for $U \subset X$ open we have 
 $$ E^{\nabla_{X/S}}(U) = \lbrace s \in E(U) \colon \nabla_{X/S}(s) =0 \rbrace.$$

Given a family $\pi:X\to S$ we make the following definition. 
\begin{definition}
	A \emph{local system relative to $\pi$} (of rank $n$) is a sheaf $L$ of $\pi^{-1}\Ocal_S$-modules on $X$ such that for all $x\in X$ there exists a $U\owns x$ open with 
	 $$ L \vert_U \cong (\pi^{-1}\Ocal_S)^{\oplus n}\vert_U $$
	 as sheaves of $\pi^{-1}\Ocal_S\vert_U$-modules. 
\end{definition}

The story is pretty much the same in the relative setting as it was in the absolute setting. 
We define morphisms of relative local systems as morphisms of $\pi^{-1}\Ocal_S$-modules and denote the category of relative local systems for a morphism $\pi:X \to S$ by $\LocSys(X/S)$.


For the following exercises $\pi:X\to S$ will be a morphism of complex manifolds. 
\begin{exercise}
	Given an vector bundle $E$ on $X$ with a relative connection $\nabla_{X/S}$ which is integrable check that $E^{\nabla_{X/S}}$ is a relative local system. 
\end{exercise}

\begin{exercise}
	Check that relative local systems restrict to local systems on fibers.
\end{exercise}

\begin{exercise}
	Check that relative connections restrict to connections on the fibers. 
\end{exercise}



\subsection{Relative Riemann-Hilbert Correspondence}
From relative local systems to relative integrable connections we use 
 $$ L \mapsto E_L = \Ocal_X\otimes_{\pi^{-1} \Ocal_S} L $$
and gives it the connection by extending $\pi^{-1}\Ocal_S$-linearly the map 
 $$ f \otimes v \mapsto df \otimes v.$$

\begin{exercise}
	Check that the map 
	$$\LocSys(X/S) \to \Conn(X/S), \quad L \mapsto E_L $$
	is an equivalence of categories with quasi-inverse $(E,\nabla_{X/S}) \mapsto E^{\nabla_{X/S}}$. 
	\deligne{Chapter I, Prop 2.23}
\end{exercise}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Isomonodromic Deformations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Painlev\'e VI equation takes the form
 \begin{align*}\frac{d^2q}{dx^2}&=
 \frac{1}{2}\left(\frac{1}{q}+\frac{1}{q-1}+\frac{1}{q-x}\right)\left( \frac{dq}{dx} \right)^2
 -\left(\frac{1}{x}+\frac{1}{x-1}+\frac{1}{q-x}\right)\frac{dq}{dx} \\&\quad +
 \frac{q(q-1)(q-x)}{x^2(x-1)^2}
 \left(\alpha+\beta\frac{x}{q^2}+\gamma\frac{x-1}{(q-1)^2}+\delta\frac{x(x-1)}{(q-x)^2}\right),
 \end{align*}
where $\alpha,\beta,\gamma,\delta \in \CC$ are constants.
The aim of this section is to show that this equation is really about isomonodromic deformations of rank two Fuchsian equations with trace free entries on $\PP^1\setminus \lbrace 0,1,\infty, x\rbrace$ where $x$ is a variable point.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Schlesinger's Equations}\label{S:schlesinger}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the case of a Fuchsian differential equation of rank 2 on $\PP^1$ with singular points at $\lbrace 0,1,\infty,x\rbrace$ for some variable $x\in \PP^1$ not equal to $0,1$ or $\infty$,
$$\begin{cases}
	  \dfrac{dY}{dt} = A(t,x) Y \\
	  A(t,x) = \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x} 
\end{cases}$$
and define $A_{\infty}(x)$ by the equation $A_0+A_1+A_2 +A_{\infty}=0$.
The matrices $A_j(x)$ we will assume depend holomorphically on the variable $x$ in some unspecified domain (we think of $x$ as varying a little bit around some $x_0$).
We will often write $A_j=A_j(x)$ for short.
We note that for a fixed $x=x_0$ the Fuchsian differential equation gives a monodromy representation. 
Also, as we vary $x\in \PP^1\lbrace 0,1,\infty\rbrace$ the fundamental group does not change (see Figure~\ref{F:moving-point}).
\begin{figure}[h]\label{F:moving-point}
	\begin{center}
		\includegraphics[scale=0.8]{moving-point.eps}
	\end{center}
\caption{Varying $x$ in $\PP^1\setminus\lbrace 0,1,\infty,x\rbrace$ does not change the fundamental group.}
\end{figure}
In what follows we will let 
 $$ \Pi=\langle \gamma_0,\gamma_1,\gamma_2 \colon \gamma_0\gamma_1\gamma_2=1\rangle.$$
For $x_0$ and $x_1$ in $\PP^1$ with $x_0\neq x_1$ we have 
 $$\Pi\cong \pi_1( \PP^1\setminus\lbrace 0,1,\infty,x_0\rbrace,t_0) \cong \pi_1(\PP^1\setminus\lbrace 0,1,\infty,x_1\rbrace,t_0).$$
Now fix $x=x_0$ and a fundamental matrix $\Phi_0(t)$ for our system at $x=x_0$ so that $\Phi_0'(t) = A(t,x_0)\Phi_0(t)$. 
This gives a monodromy representation 
$$ \rho_0\colon \Pi \to \GL_2(\CC), \quad \gamma \mapsto M_{\gamma},$$
where $M_{\gamma} \in \GL_2(\CC)$ is the matrix so that $(\Phi_0(t))_{\gamma} = \Phi_0(t) M_{\gamma}$.

\begin{problem}[Isomonodromic Deformations]
	Find a parameter space $X$ and a function 
	$$X\mapsto M_2(\CC)^3, \quad x\mapsto (A_0(x),A_1(x),A_2(x))$$
	so that the fundamental matrix $\Phi(x,t)$ of the system 
	$$ \dfrac{dY}{dt}(x,t) = \left( \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x} \right)Y(x,t) $$
	satisfies
	\begin{enumerate}
		\item (Monodromy at $x_0$) $\Phi(t,x_0) = \Phi_0(t)$ (so that $M_{\gamma}$ define the monodromy representation at that point).
		\item (Isomonodromy) For each $x\in X$, and all $\gamma \in \Pi$, $$\Phi(t,x)_{\gamma} = \Phi(t,x)M_{\gamma},$$
		up to conjugation of the collection of $M_{\gamma}$.
	\end{enumerate}
\end{problem}
 If a function $x\to A(x)$ satisfies the isomonodromy problem we call it an \emph{isomonodromic deformation} of the system at $x=x_0$.

\subsection{Schlesinger's Theorem}
In this section we are going to give some equations that determine when 
\begin{equation}
x\mapsto A(x,t) = \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x}.
\end{equation}
gives an isomonodromic deformation.
We will make some simplifying assumptions. 
\begin{enumerate}
	\item Assume that $A_{\infty}(x)$ is a constant diagonal matrix.
	\item (Non-resonance) Assume that for each $x$ the eigenvalues of $A_j(x)$ for $j=0,1,\infty$ do not differ by an integer. 
\end{enumerate}
Under the conditions above we can give equations.
Both of these conditations are used in the secret weapon of this theorem: the logarithmic Riemann-Hilbert correspondence. 
\begin{theorem}
	Assume the assumptions and work in the notation of this section.
	The map $x\mapsto A(x)$ is isomonodromic if and only if 
	$$\dfrac{\partial A_0}{\partial x} = \dfrac{[A_0,A_2]}{x}, \quad \dfrac{\partial A_1}{\partial x} = \dfrac{[A_1,A_2]}{x-1}, \quad \dfrac{\partial A_{\infty}}{\partial x}=0.$$
\end{theorem}
The above Theorem is a special case of a more general formula we will give. 
It turns out these equations are equivalent to integrability conditions for a connection on a certain space. 
To state the connection to connections we will change our setup slightly. 
We will work with a rank $n$ system on $\PP^1$ and allow it to have singularities at $m$-points (and for simplicity we will exclusing $\infty$) we will let $X$ be a parameter space of points
 $$ X = \lbrace (x_1,\ldots, x_m) \in \CC^m \colon x_i \neq x_j, \mbox{ for } i\neq j \rbrace .$$
We will use the notation $x=(x_1,\ldots,x_m)$
\begin{theorem}[Schlesinger's Equations]
	Consider the system, 
	 $$ \dfrac{\partial Y}{
	 \partial t}(t,x) = \sum_{j=1}^m \dfrac{A_j(x)}{t-x_j} Y(t,x)$$
 	on $\PP^1\times X$.
 	This system is isomonodromic if and only if 
 	\begin{equation}\label{E:schlesinger}
 	\begin{cases}
 	 \dfrac{\partial A_j}{\partial x_i} = \dfrac{[A_j,A_i]}{x_j-x_i}, &  i \neq j,\\ 
 	  \dfrac{\partial A_j}{\partial x_j} = -\sum_{i\neq j}\dfrac{[A_i,A_j]}{x_i-x_j}.
 	  \end{cases}.
 	 \end{equation}
\end{theorem}
In these notes we will call \eqref{E:schlesinger} \emph{Schlesinger's equations}.
The commutators in the Schlesinger's equation should make the reader suspect that these equations are possibly an integrability/curvature condition on some connection. 
This is indeed the case and we will actually have a ``logarithmic connection'' on the space $X\times \PP^1$.

\begin{figure}[h]\label{F:connection-space}
	\begin{center}
		\includegraphics[scale=0.75]{connection-space.eps}
	\end{center}
\caption{A picture of the $X\times \PP^1$ and the associated divisor $D$ in the case of two points and a single singular point $x$ varying.}
\end{figure}

\begin{theorem}
	The following are equivalent for a collection of matrix valued functions $A_j(x)$ on $X$.
	\begin{enumerate}
		\item The connection $\nabla=d+\omega$ on $\Ocal_{\PP^1\times X}$ given by 
		 $$ \omega = \sum_{j=1}^m A_j(x) \dfrac{d(t-x_j)}{t-x_j},$$
		 is integrable.
		 \item\label{I:differential-schlesinger} The matrices $A_j$ satisfy
		  $$ dA_j - \sum_{i\neq j} [A_i,A_j] \dfrac{d(x_i-x_j)}{x_i-x_j} = 0, \quad 1\leq j \leq m.$$
		  \item Schlesinger's equations \eqref{E:schlesinger} hold.
	\end{enumerate}
\end{theorem}
\begin{proof}
	 Also \eqref{I:differential-schlesinger} are equivalent to Schlesinger's equations.
	
	The computation of the equivalent between \eqref{I:differential-schlesinger} and the integrability condition can be rough if you try to do too much or move to Einstein notation.
	Also, we should observe that Schlesinger's equation are the literal equations one obtains from integrability but can be derived from the integrability conditions.
	
	The key is to not manipulate your equations too much and think about what you are doing.
	Integrability of the connection is equivalent to (see Theorem~\ref{T:integrability-conditions}) $$d(\omega)+\omega\wedge \omega=0.$$ 
	We compute.
	 $$d(\omega) = \sum_{i=1}^m \frac{dA_i}{t-x_i}\wedge dt - \sum_{i=1}^m \frac{dA_i \wedge dx_i}{t-x_i},$$
	 $$\omega \wedge \omega = \sum_{i,j} \frac{[A_i,A_j]}{t-x_j} \frac{dt\wedge d(x_i-x_j) + dx_i \wedge dx_j}{x_i-x_j}.$$
	 Collecting the terms with $dt$ in them one finds
	 \begin{equation}\label{E:take-residue}
	  \sum_{i} \frac{dA_i}{t-x_i} = \sum_{i,j} \frac{[A_i,A_j]}{t-x_i}\frac{d(x_i-x_j)}{x_i-x_j}
	 \end{equation}
	 The remaining terms give
	 \begin{equation}\label{E:dont-take-residue}
	  \dfrac{dA_i \wedge dx_i}{(t-x_i)} = \sum_{i,j} \frac{[A_i,A_j]}{t-x_i}\frac{dx_i\wedge dx_j}{x_i-x_j}.
	 \end{equation}
	 Taking residues of \eqref{E:take-residue} along $t=x_k$ we find that 
	  $$ dA_k = \sum_{j\neq k} [A_k,A_j] \frac{d(x_k-x_j)}{x_k-x_j}=0.$$
	 This proves \eqref{I:differential-schlesinger}.
	 
	 To see the converse observe that multiplying \eqref{I:differential-schlesinger} by $\frac{1}{t-x_j}$ and summing over $j$ gives \eqref{E:take-residue}.
	 To recover \eqref{E:dont-take-residue} we apply $-\wedge \dfrac{dx_i}{t-x_i}$ to \eqref{I:differential-schlesinger} and sum over $i$.
\end{proof}


One can be a little more general with the points if one likes.  
The following is a description of 

\begin{figure}[h]\label{F:plot-of-s}
	\begin{center}
		\includegraphics[scale=0.75]{plot-of-s.eps}
	\end{center}
\caption{A picture of $S \subset \PP^1\times X$ in the case of four parametrized points.}
\end{figure}

\section{Isomonodromy Theorem}

The aim of this section is to prove the that all isomonodromic collections of differential equations come from connections on the total space. 
This is one of the tools that we need to show that the Schlesinger equations actually describe an isomonodromic deformation.

\subsection{Isomonodromy In Terms of Relative Local Systems}
Let $\pi:W\to S$ be a morphism of complex manifolds.
Let $L$ be a relative local system of rank $n$ (so that $L \cong (\pi^{-1}\Ocal_S)^{\oplus n}$ locally).
For each $s_0\in S$ we will let $j_{s_0}$ denote the closed immersion $j_{s_0}\colon W_{s_0} \to W$ which includes the fiber $W_{s_0} = \pi^{-1}(s_0)$ into the total space $W$. 
For each $s_0\in S$ we have $j_{s_0}^{-1}(L)$ a local system on $W_{s_0}$.
For varying $s$ we would like to compare the local systems $j_s^{-1}(L)$ on $W_s$ near for $s$ near $s_0$ to $j_{s_0}^{-1}(L)$. 
We will do this using Ehresmann's theorem.

By Ehresmann's Theorem, there for every $s_0$ there exists some $U  \owns s_0$ be an open such and a diffeomorphism $\psi$ such that 
$$\begin{tikzcd}
\pi^{-1}(U) \ar[rr, "\psi"] \ar[dr, "\pi"] & & U\times W_{s_0} \ar[dl, "\pr_1"]\\
& U &
\end{tikzcd},$$
commutes. 
For $s \in U$, consider $i_s: \lbrace s \rbrace \to U$ the inclusion of the point $s$ into $U$. 
Pulling back the trivialization diagram for $\psi$ along this morphism gives
$$\begin{tikzcd}
W_s \ar[r, equal]& \pi^{-1}(x) \ar[rr, "\psi_s"] \ar[dr, "\pi"] & & \lbrace s \rbrace \times W_{s_0} \ar[dl, "\pr_1"]\\
& & U &
\end{tikzcd},$$
which, after abusively identifying $\lbrace s \rbrace \times W_{s_0}$ with $W_{s_0}$ with the projection $\pr_2$ given by the projection onto the second factor, gives an isomorphism $\psi_s: W_s\to W_{s_0}$. 
We can now pullback $j_{s_0}^{-1}(L)$ along $\psi_s$ to get $j_s^{-1}(L)$ and $\psi_s^{-1} j_{s_0}^{-1}L$ two local systems on $W_s$ which are comparable. 
Equivalently we can pullback by $\psi_s^{-1}$ to get a collection of local systems (one for each $s\in U$) on $W_{s_0}$ which can now be compared:
$$ L_s := ( j_s \psi_s^{-1})^{-1}(L) \in \LocSys(W_{s_0}). $$
This notation is very unfortunate because it has two ``$-1$''s which mean totally different things.

\begin{definition}
	Let $\pi: W\to S$ be a proper submersion of complex manifolds. 
	A relative local system $L \in \LocSys(W/S)$ is \emph{isomonodromic} near $s_0$ if and only if there exists a local topologically trivializing open set $U\owns s_0$ for the morphism $\pi$ such that $L_s = L_t$ for all $s,t\in U$. 
\end{definition}

Since we defined the monodromy of a general integrable connection in terms of the local system in then makes sense to define the monodromy of family of connections (i.e. a relative connection $\nabla_{W/S}$) in terms of a relative local system. 
\begin{definition}
	Let $\pi:W \to S$ be a proper submersion of complex manifolds. 
	Let $(E,\nabla_{W/S})$ be an relative connection which is integrable. 
	We say $(E,\nabla_{W/S})$ is \emph{isomonodromic} it associated relative local system is isomonodromic.
\end{definition}


\subsection{Existence of Isomonodromic Deformations (unstable)}
Let $\pi: W\to S$ be a morphism of complex manifolds and let $(E,\nabla)$ be an vector bundle with an integrable connection of $W$.
To each such vector bundle with integrable connection we can associate a relative connection $\overline{\nabla}\colon E \to \Omega_{W/S} \otimes E$ by using the quotient $\Omega_{W} \to \Omega_{W/S}$. 
In other words, $\overline{\nabla}$ is defined by the diagram 
$$\begin{tikzcd}
E \ar[r, "\nabla"] \ar[d, equal] & \Omega_{W} \otimes E\ar[d] \\
E \ar[r,"\overline{\nabla}"] &  \Omega_{W/S}\otimes E
\end{tikzcd}.$$
The following theorem says that isomonodromy is of a family of differential equations is characterized by the existence to a lift of a connection on the whole space. 
This is what gives rise to Schlesinger. 
\begin{theorem}[Isomonodromy Comes From Integrable Connections on the Total Space]
	Fix a proper submersion $\pi: W\to S$. 
	A relative integrable connection $\nabla_{W/S}: E\to \Omega_{W/S}\otimes E$ is isomonodromic if and only if there exists some integrable $\nabla: E\to \Omega_W \otimes E$ such that $\overline{\nabla} = \nabla_{W/S}$.
\end{theorem}

In what follows for a complex manifold $X$ we let $\Repn(X)$ denote the category of finite dimensional $\pi_1(X)$ representations. 
We let $\LocSys(X)$ denote the category of local systems.
We let $\Conn(X)$ denote the category of vector bundles with integrable connections. 
\begin{lemma}
	Given a morphism of schemes $f: Z \to W$ (which in applications will be a closed immersion) we have the following 2-commutative diagram of functors,
	$$\begin{tikzcd}
	\Repn(W) \ar[r, "f^*"] & \Repn(Z)  & \pi_1(Z)\to \pi_1(W) \mbox{ induced}\\
	\LocSys(W) \ar[r,"f^{-1}"] \ar[u,"\sim"]& \LocSys(Z) \ar[u,"\sim"] & \mbox{inverse image} &\\
	\Conn(W) \ar[r, "f^*"] \ar[u, "\sim"]& \Conn(Z) \ar[u,"\sim"] & \mbox{ pullback}
	\end{tikzcd}
	$$
	where the vertical arrows are functors in the Riemann-Hilbert Correspondence. 
\end{lemma}
We remind the reader that for $(E,\nabla) \mapsto E^{\nabla}$ and that $L\mapsto (\Ocal_X \otimes L, d\otimes \id)$ give the correspondence between local systems and integrable connections. 

There is a similar description in the relative setting.
\begin{lemma}\label{L:relative-functorality}
	Let $\pi: W\to S$ be a family and $j: W_{s_0} \to W$ be a closed immersion of a fiber over $s_0$. 
	The diagram is 2-commutative.
	$$\begin{tikzcd}[contains/.style = {draw=none,"\in" description,sloped}]
	L \ar[d,contains]  \ar[r,mapsto] & \pi^{-1} \Ocal_S \otimes L \ar[d, contains]& \\
	\LocSys(W) \ar[r] & \LocSys(W/S) \ar[r,"j^*"] & \LocSys(W_{s_0}) \\
	\Conn(W) \ar[r] \ar[u] & \Conn(W/S) \ar[u] \ar[r, "j^*"]& \Conn(W_{s_0}) \ar[u] \\
	(E,\nabla) \ar[u,contains] \ar[r,mapsto] & (E,\overline{\nabla}) \ar[r,mapsto] \ar[u,contains] & (E,\overline{\nabla})\vert_{W_{s_0}} \ar[u,contains]
	\end{tikzcd}
	$$
	The map $j^*$ from local systems to relative local system commutes with $j^{-1}: \LocSys(W) \to \LocSys(W_{s_0})$. 
	Similarly with the restriction of connections.
\end{lemma}

The following criterion is useful for showing that there exists a connection lifting a relative connection. 
\begin{theorem}
	Let $\pi:W \to S$ be a morphism of complex manifolds.
	Let $(\nabla_1,E)$ be a connection of $W$ with local system $L_1 = E^{\nabla_1}$. 
	Let $(\overline{\nabla}_1,E)$ be a relative connection for $\pi:W\to S$ and let $L_2 = E^{\overline{\nabla}_2}$ be the associated relative local system.
	The following are equivalent.
	\begin{enumerate}
		\item The connection resticts to the relative connection $(\overline{\nabla_1},E) \cong (\overline{\nabla}_2,E_2)$.
		\item The local system extends to the relative local system $L_2 \cong \pi^{-1}\Ocal_S\otimes L_1$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	This follows from the relative Riemann-Hilbert correspondence and the commutativity of the diagrams in Lemma~\ref{L:relative-functorality}.
\end{proof}

Let $\pi: W\to B$ be a family where $B$ is a ball. 
The fiber $W_0=W_{b_0}$ connected and smooth. 
Consider now an inclusion of a fiber $W_{0}$ into a total space $W$, where we let $j_{b_0} =j_0$ to simplify notation.  
$$\begin{tikzcd}
W_{0} \ar[r, "j_{0}" ] \ar[d] & W\ar[d,"\pi"] \\
\lbrace b_0 \rbrace \ar[r] & B 
\end{tikzcd}
$$
Using the long exact sequence of homotopy groups 
$$ \cdots \to \pi_2(B) \to \pi_1(W_0) \to \pi_1(W) \to \pi_1(B) \to \pi_0(F) \to \cdots   $$ 
we have $\pi_1(W_0) \cong \pi_1(W)$ since $\pi_i(B)=0$ for all $i$.
This means that $\Repn(W)\cong \Repn(W_0)$.

\begin{theorem}
	Integrable connections $(E_0,\nabla_0)$ on a fiber $W_0$ lift to integrable connections $(E,\nabla)$ on the total space.
	and hence can be put into some isomonodromic family of flat connections $(E,\nabla_{W/S}) = (E,\overline{\nabla})$. 
\end{theorem}
\begin{proof}
	We will now explain how to use this to lift an integrable connection $(E_0,\nabla_0)$ on $W_0$ to an integrable connection $(E,\nabla)$ on $W$. 
	We use the diagram 
	$$\begin{tikzcd}
	\Repn(W) \ar[r, "\sim"] & \Repn(W_0) \\
	\Conn(W) \ar[r]\ar[u] & \Conn(W_0) \ar[u,"\sim"]
	\end{tikzcd}
	$$ 
	The composition from $\Conn(W_0)$ going up, then left, then down in the diagram shows there exists some $(E,\nabla) \in \Conn(W)$ with the monodromy representation we want.
	Applying commutativity of the diagram we get $(E,\nabla)\vert_{W_0} \cong (E_0,\nabla_0)$. 
	
	Let $(E,\nabla_{W/B})$ be a family of isomonodromic connections and let $(E_0,\nabla_0)$ be the connection resticted to $b_0$. 
	The shape of the Schlesinger equations then comes from the logarithmic Riemann-Hilbert correspondence which says that all connections can actually be made to be Fuchsian.
\end{proof}


The converse of this statement is also true. 
\begin{theorem}
	Every isomonodromic family of integrable connections comes from an integrable connection on the total space. 
\end{theorem}

These two theorem explain the existence of the Schlesinger equations and why we are looking at connections on the space of parameters of Fuchsian differential equations of rank two.


\iffalse 

\subsection{The Geometry of the Connection}

\taylor{LOTS OF NOTES NEED TO BE PUT IN HERE. They are mostly from Sabbah, with some parts from Deligne and Malgrange for the Riemann Hilbert Correspondence stuff.}

In \cite[VI, \S1, pg 192]{Sabbah2007} Sabbah works with an arbitrary complex manifold $X$ some $S \subset X\times \PP^1$ such that the map $f:S \to X$ induced by the projection onto $X$ has degree $m$. 
This means that over $\mu \in X$ the we have a fiber $S_{\mu}$
$$S_{\mu} = f^{-1}(\mu) = \lbrace (\mu,x_1(\mu)) (\mu,x_2(\mu)),\ldots,(\mu,x_m(\mu))\rbrace.$$ 
By assumptions on $X$ (1-connectedness) the manifold $S$ is trivializable: there exists maps $x_j: X \to \PP^1$ such that $S$ is the union of the graphs of these functions:
 $$ S=S_1 \cup \cdots \cup S_m $$
where $S_j$ is the graph of the map $x_j$.
In what we were doing above the maps $x_j$ were the projections from $X\subset (\PP^1)^m$ onto $\PP^1$.

Note now that the fiber of $(X\times \PP^1)\setminus S$ above $x=\mu\in X$ is $\PP^1\setminus S_{\mu} \cong (\PP^1\setminus S_{\mu}) \times \lbrace \mu \rbrace$.
If we assume that $H_2(X,\ZZ)=0$ or that $\pi_2(X)=0$ then one can use the long exact sequence in homotopy groups to conclude that the inclusion of the fiber 
 $$ \PP^1\setminus S_{\mu} \hookrightarrow (\PP^1\times X)\setminus S $$
induces an isomorphism of fundamental groups.

\fi

\section{Painlev\'e Equations (unstable)}

Consider the systems on $\PP^1\setminus \lbrace 0,1,\infty,x\rbrace$ 
$$ \dfrac{dY}{dt} = AY, \quad A = \frac{A_0(x)}{t}+\frac{A_1(x)}{t-1}+ \frac{A_2(x)}{t-x},$$
where the matrices are in $\Lie(\SL_2)$ (and hence are trace free) and  $A_0+A_1+A_2+A_{\infty}=0$.
If the system is isomonodromic this defines a connection on 
$$W=(B\times \PP^1)\setminus D_0\cup D_1 \cup D_x \cup D_{\infty}$$ 
where $D_j$ is the divisor given by the line $\lbrace t = x_j \rbrace \subset B\times \PP^1$ where $(x_0,x_1,x_2,x_{\infty}) = (0,1,x,\infty)$ and $x\in B$ is the standard coordinate.
Here we have a map $\pi\colon W\to B$ and $\pi^{-1}(x) = \PP^1\setminus \lbrace 0,1,x,\infty\rbrace$.

We know that each matrix $A_j(x)$ has eigenvalues $\pm \theta_j(x)$.We will suppose that the collections are isomonodromic which implies several things.
First is tells us that $\theta_j(x)$ are constant. 
Also it tells us that the matrices $A_j(x)$ satisfy Schlesinger's equations. 

Using the theory of cyclic vectors we can suppose that $A_{\infty}$ is lower triangular. 
In particular we will have 
$$ A_{\infty} = \begin{pmatrix} \theta_{\infty} & 0 \\
*   & -\theta_{\infty}\end{pmatrix}.$$
where $*$ is some unspecified entry.
\begin{lemma}
	\taylor{This lemma is broken as stated. It needs to be modified using \cite[\S 6]{Iwasaki1991} }
	Let $R$ be a $\partial$-ring. 
	Every trace free matrix $A$ with eigenvalues $\pm \theta \in R^{\partial}$ can be written as 
	$$A=\begin{pmatrix}
	z & u(\theta-z) \\
	u^{-1}(\theta+z) & -z
	\end{pmatrix}$$
	for some $\theta,z\in R$ and $u\in R^{\times}$.
\end{lemma}
\begin{proof}
	First observe sufficiency.
	The matrix of the form in the statement has trace zero.
	Also the determinant is $-z^2-(\theta-z)(\theta+z)=-z^2-(\theta^2-z^2)=-\theta^2$
	
	Conversely, write $A = \begin{pmatrix} a &b \\ c & d \end{pmatrix}$.
	We know that $\tr(A)=0$ which means that $a=-d$.
	Letting $a=z$ we are good.
	We now that $\det(A) = -z^2-bc=-\theta^2$. 
	This implies that $bc=\theta^2-z^2=(\theta-z)(\theta+z)$.
	\taylor{This is the lie:}One can now see that it suffices to have $b=u(\theta-z)$ and $c=u^{-1}(\theta+z)$ to get $A$ trace free with the prescribed eigenvalues. 
\end{proof}

We will now explain the relationship to the 6th Painlev\'e equation.
Let's write 
$$ A = \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} 
\end{pmatrix} \frac{1}{t(t-1)(t-x)},$$ 
where we view $a_{ij}=a_{ij}(t)$ as polynomials in $t,x$ and the entries $(A_j)_{\mu\nu}$ (which we think of as functions of $x$). 
We have 
$$  \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} 
\end{pmatrix} = (A_0+A_1+A_2)t^2 -((1+x)A_0 + xA_1 + A_2)t + A_0x. $$
The $q$ for the Painlev\'e VI now appears as a root of $a_{12}(t)$ as we will explain.
\begin{lemma}
	The polynomial $a_{12}$ has degree 1 in $t$. 
\end{lemma}
\begin{proof}
	$A_0+A_1+A_2=-A_{\infty}$ and $(A_{\infty})_{12}=0$.
\end{proof}
We can now proceed in two ways. 
We now introduce a differential variables $k$ and $q$ which are constant with respect to $\partial_t$ and write  
	$$a_{12} = k(t-q).$$ 
We now state the result. 

\begin{theorem}
	The variable $q =q(x)$ satisfies 
	\begin{align*}\frac{d^2q}{dx^2}&=
	\frac{1}{2}\left(\frac{1}{q}+\frac{1}{q-1}+\frac{1}{q-x}\right)\left( \frac{dq}{dx} \right)^2
	-\left(\frac{1}{x}+\frac{1}{x-1}+\frac{1}{q-x}\right)\frac{dq}{dx} \\&\quad +
	\frac{q(q-1)(q-x)}{x^2(x-1)^2}
	\left(\alpha+\beta\frac{x}{q^2}+\gamma\frac{x-1}{(q-1)^2}+\delta\frac{x(x-1)}{(q-x)^2}\right).
	\end{align*}
	where the constants $\alpha,\beta,\gamma,\delta$ are related to the eigenvalues $\theta_0,\theta_1,\theta_2,\theta_{\infty}$ by the formula
	$$ \alpha = \frac{1}{2}\left( \frac{\theta_{\infty}}{2}- 1 \right)^2, \quad \beta = \theta_0^2/8, \quad \gamma = \theta_2/4, \quad \delta = \frac{1}{2}-\frac{\theta_1^2}{4}.$$
\end{theorem}
\begin{proof}[Proof Idea]
	The proof is a computational romp. 
	You essentially get a large system of equations from the entries of our matrices given by the Schlesinger equations.
	There is an differential equation for $a_{21}$ which we write in terms of $k$ and $q$ and then we systematically eliminate all of the other variables and put everything in terms of $q$. 
	There is actually an algorithm for this coming from Elimination Theory and we are going to use this horrible computation as a chance to advertise the computational differential algebraic tools at our disposal.
	Skip to the introduction of Chapter \ref{S:computational} to see how this works in \textsf{Maple}.
\end{proof}
The equation above we denote by $P_{IV,\theta}$ there $\theta=(\theta_0,\theta_1,\theta_2,\theta_{\infty})$ is the Painlev\'e VI.

There is amazingly a converse to this. 
\begin{theorem}
Every $q$ also gives an isomonodromic deformation.
\end{theorem}

The $q$ is also one half of a collection of angle-action coordinates with $p$ given by 
$$ p = \frac{1}{2}\left( \frac{(t-1)q'-\theta_1/2}{q} + \frac{q'-1-\theta_2/2}{q-x} + \frac{xq'-\theta_3/2}{q-1} \right). $$




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Painlev\'e Property (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The Painlev\'e equations were not originally derived via the Schlesinger equations. 
Classically, mathematicians were looking for special functions which were solutions of differential equations.
This is how they came up with Airy functions, Bessel functions, hypergeometric functions, trigonometric functions, etc.

\begin{definition}
	An ordinary differential equation in one variable has the \emph{Painlev\'e property} if and only if germ $f\in\CC\langle t-t_0\rangle$ admits a well-defined analytic continuation.
\end{definition}

%The main result concerning the 
\begin{theorem}
	Consider a differential equation of the form 
	$$ \dfrac{d^2q}{dx^2} = R(x,q,q'). $$
	If this equation has the Painlev\'{e} property then it is a specialization of the PVI.
\end{theorem}

An overview of the proofs can be found in \cite{Shimomura2003}.
The first proof is in the back of Ince's book \cite{Ince1944}. 
The second proof is due to Malgrange 

\taylor{I need to finish adding my notes here}


It then remains to determine if the functions obtained as solutions of the Painlev\'e differential equations are genuinely new. 
This was done by Umemura and others. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Ordinary Differential Algebra}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we are going to develop more the theory of $\partial$-algebra done previously with an eye towards computation. 
In this section and the next we are following Kaplansky \cite{Kaplansky1976}, Ritt\cite{Ritt1950}, Kolchin \cite{Kolchin1973}, and Boulier \cite[\href{https://hal.archives-ouvertes.fr/hal-02378197v2/document}{online version}]{Boulier2019}, Marker's notes \cite{Marker2000}, as well as Buium and Cassidy's chapter in Kolchin's selected works \cite{Kolchin1999}.

The goal of this section is to provide a foundation for differential algebraic geometry. 
The main thesis is the following

A crash course in differential algebra by way of analogy is given below. 
In what follows we are going to let $\Khat$ denote a differentially closed field. 
Here $\widehat{K}$ is a differential closure of the differential field $(K,\partial)$. 
For completeness we give a definition.
\begin{definition}
	A field $\widehat{K}$ is \emph{differentially closed} if and only if for every $f,s\in \Khat\lbrace y\rbrace$ with 
	$$\ord^{\partial}(s)<\ord^{\partial}(f) \implies \exists \alpha \in \Khat, \ \ f(\alpha)=0 \mbox{ and } s(\alpha)\neq 0. $$
\end{definition}
These things exist and are rather strange.

The most imporant philosophical point is that given $f_1,\ldots,f_e \in K\lbrace x_1,\ldots,x_n \rbrace$ that the ring 
 $$ A_{\Gamma} \colon = K\lbrace x_1,\ldots, x_n \rbrace/[f_1,\ldots,f_e] $$
is the fundamental object of study the solutions of the systems 
 $$ \Gamma \colon \quad f_1=\ldots = f_e =0 $$
That is the set 
 $$ \Gamma(\Khat) = \lbrace (a_1,\ldots,a_n) \in \Khat^n \colon f_1(a_1,\ldots,a_n) = \cdots = f_n(a_1,\ldots,a_n) =0 \rbrace,$$
is best understood from properties of the ring $A_{\Gamma}$.
In the future we will introduce the notion of a $D$-schemes, but what follows is mostly the analog of studying solutions of polynomial equations in $\CC^n$ (i.e. the theory of reduced irreducible schemes).
In fact the sets $\Gamma(\Khat)$ correspond to reduced irreducible $D$-schemes, where $D$-schemes are objects introduced in 

\begin{description}[leftmargin=*]
\item[Polynomials] In commutative algebra one works with polynomial rings  $K[x_1,\ldots,x_n]$ over finitely many indeterminates and studies the ideals inside of it.

In differential algebra one works with \emph{$\partial$-polynomial rings}
$$K\lbrace x_1,\ldots,x_n\rbrace = K[ x_i^{(j)} \colon 1\leq i \leq n, j\geq 0],$$ 
with finitely many differential indeterminates $x_1,x_2,\ldots,x_n$.
Elements of $K\lbrace x_1,\ldots,x_n\rbrace$ are called \emph{$\partial$-polynomials}. 
Note that the ring of $\partial$-polynomials has the unique differential operator $\partial$ extending the derivative on $K$ such that $\partial(x_i^{(j)}) = x_i^{(j+1)}$.
In $\partial$-algebra we study $\partial$-ideals (ideals closed under derivations) inside rings of $\partial$-polynomials.

Given $u \in K\lbrace x_1,\ldots,x_n\rbrace$ containing $x_j$ nontrivially, define the \emph{order} of $u$ in the variable $x_j$:
$$\ord^{\partial}_{x_j}(u) = \max\lbrace r \in \ZZ_{\geq 0} \colon \partial u/\partial x_j^{(r)} \neq 0 \rbrace.$$
If $u$ does not involve $x_j$, we define $\ord^{\partial}_{x_j}(u)=0$.

\item[Varieties] In algebraic geometry, we study subsets of $K^n$ cut out by polynomial equations. 
They are called algebraic sets and they form a basis of closed sets for the Zariski topology.

In differential algebraic geometry, we study subsets of $K^n$ cut out by differential polynomials. They are called \emph{Kolchin closed subsets} and form a basis of closed sets for the Kolchin topology.

\item[Nullstellensatz]
In algebraic geometry algebraic sets correspond to radical ideals. 

In differential algebraic geometry, Kolchin closed subsets correspond to radical differential ideals.

\item[Basis Theorems] In commutative algebra every ideal in a polynomial ring over a Noetherian ring is finitely generated. 

In differential algebra we only have a weaker statement. 
For all radical differential ideals $I$ there exists a finite number of elements such that the radical of the differential ideal differentially generated by those elements is equal to $I$.

Both of these theorems imply noetherianity of their respective topologies.

\item[Primary Decompositions] In commutative algebra we prove that every ideal $I$ in a Noetherian ring $R$ can be written as a finite irredundant intersection of primary ideals. 
This corresponds geometrically to the decomposition of a scheme (or variety) into finitely many  irreduble components.

In differential algebra we can show that every differential ideal $I$ in $K\lbrace x_1,\ldots,x_n\rbrace$ can be written as a finite intersection of prime differential ideals.
Note that we didn't say differential primary ideals.
There exists modest improvements of this but they are harder to describe.

\item[Transcendence Degrees] 

In commutative algebra one has can consider the transcendence degree of a field extension. 
Geometrically this corresponds to the dimension of an irreducible variety.

In differential algebra, given a differential field extension $F\supset K$ one can discuss the \emph{differential transcendence degree} of $F$ over $K$ which we denote by $\trdeg_K^{\partial}(F)$.
This is the maximal $n$ such that there exists an injection of rings $K\lbrace x_1,\ldots,x_n\rbrace\hookrightarrow F$.
\end{description}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differential Equations in One Variable}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section I want to study a differential algebraic variety associated to a single ordinary differential polynomial $f \in K\lbrace y \rbrace$.
That is $f = f(y) = F(y,y',y'',\ldots,y^{(r)})$ where $F(x_0,x_1,\ldots,x_r) \in K[x_0,x_1,\ldots,x_r]$ is just some polynomial and we investigate  $\Sigma \subset \widehat{K}$ be defined by 
 $$ \Sigma\colon F(y,y',y'',\ldots, y^{(r)}) =0. $$


\subsection{Envelopes and Separants}
We are going to need to redo this computation later.
Let $R$ be a $\partial$-ring and let $f \in R\lbrace x \rbrace$. 
Given $f \in R\lbrace x \rbrace$, the \emph{leader} $\ell_f$ of $f$ is the highest derivative of $x$, $x^{(r)}$ appearing in $f$ (i.e. such that $\partial f/\partial x^{(r)} \neq 0$. 

Something that we want to describe now is the separant whose vanishing or non-vanishing on a solution has important geometric consequences.

\begin{definition}
	The \emph{separant} of $f$ is $S_f:=\partial F/\partial \ell_f$. 
\end{definition}

I want to point out that we treat $x,x',x'',\ldots$ like indeterminates. 
In particular $\partial x^{(i)}/\partial x^{(j)} = \delta_{ij}$ where $\delta_{ij}$ is the Kronecker delta. 
For concreteness we give an example.
\begin{example}
	If $f = 2x(x')^2 + x^3$ then $\ell_f = x'$, $I_f = 2x$ and $S_f = 2x'x$. 
\end{example}

The following example is going to be our introduction into singular or envelope solutions of differential equations and how it interacts with the separant.
We are going to have a family of solutions and then there is going to be a limiting family around them which algebraically stands out.
\begin{example}[Our First Envelope]
	When we solve the ODE
	 $$ f=(\dot{y})^2 - 4y =0,$$
	we are going to find out that there are two types of solutions ``general solutions'' and an envelope solution. 
	The picture of the solutions is in Figure~\ref{F:envelope}.
	\begin{figure}[]
		\begin{center}
			\includegraphics[scale=0.5]{envelope.eps}
		\end{center}
	\caption{The above image plots solutions of the equation $\dot{y}^2-4y=0$.
		The general solutions are the parabolas which are tangent to the $t$-axis labeled $y_1(t)$ for varying $t_0$. 
		The envelope solution is $y_2(t)=0$ and it is exactly where the separant vanishes.	
}\label{F:envelope}
	\end{figure}
	 
	To do this lets observe that after taking a derivatives we get 
	$$0=2\dot{y}\ddot{y}-4\dot{y} = 2\dot{y}(\ddot{y}-2)=0.$$
	We can solve each of $\dot{y}=0$ and $\ddot{y}-2=0$ separately. 
	These will give rise to two components of the ideal $\lbrace f \rbrace_{\partial}$ if $f = \dot{y}^2-4y$. 
	
	The solution fo $\ddot{y}-2=0$ is $y_1(t) = t^2 + At + B$ for some constants $A$ and $B$.
	Plugging this back into our original equation we get 
	$$0=(2t+A)^2 -4(t^2+At+B)=A^2-4B$$
	This then given $(t+A/2)^2=t^2+At +A^2/4=t^2+At+B$.
	So setting $t_0=-A/2$ we see we get a collection of parabolas $y_1(t)$ hanging off the $t$-axis.
	These are going to be our ``general solutions''.
	
	The solution of $\dot{y}=0$ is $y_2(t)=C$ for some constant $C$. 
	Plugging this back into our original equation gives 
	$$ 0=0^2-4C$$
	which implies $C=0$ which gives $y_2(t)=0$. 
	This solution is going to be our ``envelope solution''.
	
	Before leaving, let's observe the relationship with the separant. 
	We have $S_f = \dot{y}$.
	The general solutions $y_1(t)$ do not vanish on the separant. 
	The envelope solutions \emph{do} vanish on the separant.
\end{example}

Given a general ODE in one variable
 $$ \Sigma \colon f=0, $$
defined by $f \in K[y]_{\partial}$ we can always decompose the space of solutions into two pieces. 
One of the pieces is where the separant $S_f$ vanishes (the envelope piece) and the other is where the separant doesn't vanish (the general piece). 
We don't know that these pieces yet are irreducible components of a differential algebraic variety, but when $f \in K[y,y',y'',\ldots]$ is irreducible (as a polynomial in many variables) this will turn out to be the case. 

\begin{theorem}
	For $g \in K[ y ]_{\partial}$ we have 
	 $$ \lbrace g \rbrace_{\partial} = \lbrace g, S_g \rbrace_{\partial} \cap ( \lbrace g \rbrace_{\partial} \colon S_g).$$
\end{theorem}
\begin{proof}
	Let $E= \lbrace g,S_g \rbrace_{\partial}$ and $( \lbrace f \rbrace_{\partial} \colon S_g)$. 
	Clearly $\lbrace g \rbrace_{\partial} \subset E \cap J$.
	Conversely, let $h \in E \cap J$. 
	We have $h^2 \in \lbrace g, S_g \rbrace_{\partial}$ and hence $h \in \lbrace hf, h S_g\rbrace_{\partial} \subset hg,g\rbrace_{\partial} \subset \lbrace g \rbrace_{\partial}$. 
	Here we used the property of colon ideals. 
\end{proof}

Let's state our results that we can prove after introducing the division algorithm.
One of the main thing that I wanted to talk about was the following application of pseudodivision. 
\begin{theorem}
	Let $g \in K \lbrace x \rbrace_{\partial}$. 
	\begin{enumerate}[topsep=0pt]
		\item If $f \in [g]$ and $\ord^{\partial}(f) \leq \ord^{\partial}(g)$ then $g \vert f$. 
		\item If $g$ is irreducible then $J = ( \lbrace g \rbrace_{\partial} : S_g)$ is a prime ideal. 
	\end{enumerate}
\end{theorem}
The proof of this is given in \S\ref{S:application-of-division}.
We remind the reader that for an ideal $I$ in a ring $R$ and an element $s\in R$ we have $(I:s) = \lbrace r \in R \colon sr \in I\rbrace$ is an larger ideal containing $I$ (see \cite{Atiyah2016}).


The second item in this theorem leads to two imporant ideas. 
The first idea is that simple field extensions and prime ideals in the differential setting are not so simple. 
Second, if we remember how prime and primary decompositions work from Commutative algebra, we see that this item is telling someting about the components of $\lbrace g\rbrace_{\partial}$.
In particular by localizing (or specifying the inequation $S_g\neq 0$) we are picking out an irreducible component of the variety of solutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Univariate Pseudodivision Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to define our division algorithm (which is sort of crappy compared to the Groebner basis tools we know and love from usual commutative algebra) we need to introduce some terminology which will help us order our terms of our differential polynomials.
\begin{definition}
	The \emph{initial} of $f \in K \lbrace y \rbrace$ is the coefficient $I_f$ of the top degree term of the leader.
	In other words if 
	$$ f = a_0 + a_1 \ell_f + \cdots + a_d \ell_f^d, \quad I_f = a_d $$
	where if $\ell_f = x^{(r)}$ then $a_j \in R\lbrace x,x',\ldots,x^{(r-1)}]$ is $I_f$. 
	The $d$ appearing in the formula about is the \emph{leader degree} and we denote it by $\ldeg(f)$.
\end{definition}

In what follows we will give $K\lbrace x \rbrace$ the term ordered induced by the lexicographic ordering such that $x^{(r)} \prec x^{(r+1)}$ for all $r\geq 0$. 
This is our first example of a ranking. 
This then defined an ordering on the collection of all polynomials where we say that $A\prec B$ if and only if $\LT(A)\prec\LT(B)$ where $\LT$ denotes the leading term.
This ordering is completely determined by $f\mapsto \ell_f^{\ldeg(f)}$ where higher derivatives are larger, then degrees break ties. 



\begin{lemma}
	Let $F \in K\lbrace x \rbrace$ be a non-constant $\partial$-polynomial. 
	\begin{enumerate}[topsep=0pt]
		\item $\ell_{\partial^n(F)} = \partial^n(\ell_F)$
		\item $I_{\partial^n(F)} = S_F$.
	\end{enumerate}
\end{lemma}
\begin{proof}
	We can write $\ell=\ell_F$ and then expand $F$ as $ F = \sum_{j=0}^d a_j \ell^j$.
	We then have 
	 $$ \partial(F) = \left( \sum_{j=0}^d \partial(a_j) \ell^j\right) + \partial(\ell) \left( \sum_{j=0}^d a_j j \ell^{j-1} \right ). $$
	This proves that $I_{\partial(F)} = S_F$ and $\ell_{\partial(F)} = \partial(\ell_F)$.
	The formula $\ell_{\partial^n(F)}=\partial(\ell_F)$ is evident. 
	Also, one can see from differentiating the formula repeatedly that $I_{\partial^n(F)} = S_F$. 
\end{proof}

We now get the following algorithm for pseudodivision. 
This is really the heart of the method of characteristics in several variables for determining ideal membership. 

\begin{theorem}[The Pseudodivision Algorithm]\label{T:ode-pseudodivision}
	Fix some non-constant $F \in K\lbrace x \rbrace$. 
	For all $A \in K\lbrace x \rbrace$ there exists integers $a,b\geq 0$, and some $\widetilde{A}\in K\lbrace x \rbrace$ with $\widetilde{A} \prec F$ such that 
	 $$ I^a S^b A \equiv \widetilde{A} \quad [F],$$
	where $I=I_F$ is the initial of $F$ and $S=S_F$ is the separant of $F$.
\end{theorem}
\begin{proof}
	We break the proof into cases. 
	\begin{itemize}
		\item (case 1) $A$ is lower than $F$. 
		\item (case 2) $\ord^{\partial}_x(A) = \ord^{\partial}_x(F)$ but $\deg_{\ell}(A) > \deg_{\ell}(F)$.
		\item (case 3) $\ord^{\partial}_x(A)>\ord^{\partial}_x(F)$
	\end{itemize}
    For simplicity of notation we will let $\ord^{\partial}_x(A) = r_A$, $\ord^{\partial}_x(F) = r_F$, $\deg_{\ell_A}(A)=d_A$ and $\deg_{\ell_F}(F)=d_F$.
    
    In case 1, we are done so we do nothing. 
    
    In case 2, we let $A_1= I_FA_0 - I_A\ell^{d_A-d_F}F$.
    We have $A_1$ of lower degree than $A_0=A$ in $\ell_A$. 
    If $A_1$ is lower than $F$ we are done. 
    If not, we repeat this process.
    Since the degree is lowered after every interation we eventually terminate. 
    
    In case 3, we let $A_0 = A$ and $F_0=\partial^{r_A-r_F}(F)$.
    These two polynomials will have the same order and we can now apply case 2. 
    Note that $I_{F_0} = S_F$ so we will eventually get 
     $$ S_F^b A \equiv \widetilde{A} \mod [F],$$
    with $\widetilde{A}$ lower than $F_0$. 
\end{proof}

In formulas this reads,
$$ I^a S^b A = (a_0F + a_1 \partial(F) + \cdots + a_r \partial^aS^b A) + \widetilde{A}$$ 
with remainder $\widetilde{A}$. 
We get a contribution to $a$ for every time case 1 is used in the proof and a contribution to $b$ every times case 2 is used in the proof. 
There is no clean division here like in the division algorithm for polynomials. 

\begin{remark}
	In the case that we only want to lower the order and don't care about the degree, we only need to use separants. 
	This plays a role in our decomposition into general and envelope components.
\textbf{}\end{remark}

\subsection{Application of Pseudodivision}\label{S:application-of-division}


\begin{lemma}[Divisibility Lemma]
	Let $g \in K [x]_{\partial}$.
	If $f \in [g]$ and $\ord^{\partial}(f) \leq \ord^{\partial}(g)$ then $g \vert f$. 
\end{lemma}
\begin{proof}
	This proof is a little tricky because it requires you to really treat elements of $K[x]_{\partial}$ as polynomials in weird looking symbols and nothing more. 
	For me, this is psychologically difficult for some reason.
	
	The relation $f \in [g]$ is equivalent to a formula 
	\begin{equation}\label{E:member-relation}
	 f = c_0 g + c_1 \partial(g) + \cdots + c_r \partial^r(g)
	\end{equation}
	where $c_i \in K[x]_{\partial}$.
	
	Let's write $\ell = \ell_g$, $S=S_g$ and observe that 
	 $$ g^{(j)} = S\ell^{(j)} + T_j, \qquad j\geq 1 $$
	where $T_j\prec \ell^{(j)}$.
	The key observation is that because $\ord^{\partial}(f)\leq \ord^{\partial}(g)$ only the right hand side of \eqref{E:member-relation} can involve the indeterminates $\ell^{(j)}$ for $j\geq 1$.
	In particular, we can map them to whatever we want and still have an identity!
	
	We start with using $\ell^{(r)} \mapsto T_r/S$ to get (after clearing denominators) a new equation
	 $$ S^{e_r} f = d_0 g+ d_1 \partial(g) + \cdots + d_{r-1} \partial^{r-1}(g).$$
	We then proceed inductively getting 
	 $$ S^{e_r+e_{r-1}+\cdots + e_1} f = z_0 f $$
	for some $z_0 \in K[x]_{\partial}$.
	We have that $g \vert S^e f$ by irreducibility of $f$. 
	Since $S\prec g$ this implies that $g\nmid S$ and hence that $g \vert f$. 
\end{proof}

The next Theorem we give should be contrasted with what happens in usual field theory from Dummit and Foote. 
We don't just mod out by irreducible polynomials, but we need to mod out by \emph{saturations} of irreducible polynomials. 
The following statement is the analog of the statement that the field $F[x]$ is a PID via the Euclidean algorithm.
\begin{theorem}[Structure Theorem]\label{T:structure-theorem}
If $g \in K\lbrace x \rbrace$ is irreducible then $( \lbrace g \rbrace_{\partial} : S_g)$ is a prime ideal. 
Conversely, for every prime $\partial$-ideal $P \subset K\lbrace x \rbrace$ there exists some $g\in K\lbrace x \rbrace$ such that $P=( \lbrace g \rbrace_{\partial} : S_g)$ furthermore this $g$ is unique up to associates.
\end{theorem}
\begin{proof}
	Suppose that $f_1f_2 \in J$.
	We will show that $f_1 \in J$ or $f_2 \in J$.
	For future reference we will let $S^{a_1}f_1 = \widetilde{f}_1$ and $S^{a_2}f_2 = \widetilde{f}_2$ where $\widetilde{f}_i$ have order less than or equal to that of $g$.
	\begin{align*} 
	f_1f_2 \in J \quad &\iff  S f_1f_2 \in \lbrace g \rbrace_{\partial} \\
	&\iff (Sf_1f_2)^b \in [g]_{\partial}\\
	&\implies S^{a_1b}S^{a_2b}S^{-1} (S f_1f_2)^b  = (S^{a_1}f_1 S^{a_2} f_2)^b \in [g]_{\partial} \\
	&\implies (\widetilde{f}_1\widetilde{f}_2)^b \in [g]_{\partial} 
	\end{align*}
	By the divisibility lemma $g \vert (\widetilde{f}_1\widetilde{f}_2)^b$. 
	By irreducibility $g \vert \widetilde{f}_1$ or $g\vert \widetilde{f}_2$. 
	Without loss of generality suppose it is $\widetilde{f}_1$.
	This implies $\widetilde{f}_1 \in [g]$ which implies $S^{a_1}f_1 \in [g]$ which implies $(Sf_1)^{a_1} \in [g]$ which implies $Sf_1 \in \lbrace g \rbrace_{\partial}$ which implies $f_1 \in (\lbrace g\rbrace_{\partial}:S_g)$. 
	
	Conversely, let $P \subset K\lbrace x \rbrace$ be a prime ideal and chose $g\in P$ minimal with respect to $\prec$. 
	We claim by minimality $g$ is irreducible and we claim that 
	 $$ P = ( \lbrace g \rbrace_{\partial} \colon S_g ).$$
	
	We will first prove that $P \subset (\lbrace g\rbrace_{\partial}\colon S_g)$.
	If $f \in P$ then we have $S_g^a f \equiv \widetilde{f} [g]$ where $\widetilde{f}\prec g$. 
	But this would make $\widetilde{f}\in P$ so $\widetilde{f}=0$. 
	This proves $f\in (\lbrace g\rbrace_{\partial}\colon S_g)$.
	
	We will now prove $(\lbrace g\rbrace_{\partial}\colon S_g)\subset P$.
	We have $S_g,I_g\notin P$ by minimality of $g$. 
	If $f \in (\lbrace g\rbrace_{\partial}\colon S_g)$ then $(S_g f)^n \in [g] \subset P$.
	Since prime ideals are radical $S_g f \in P$. 
	By primality, $S_g\in P$ or $f\in P$. 
	By the choice of minimality of $g$ we have $S_g\notin P$ and hence $f\in P$. 
	
	The uniqueness statement follows from the divisibility Lemma.
\end{proof}

The idea $( \lbrace f \rbrace_f \colon S_f )$ is the analog of a principal ideal generated by a polynomial in a polynomial ring. 
Because it is so important we will introduce a notation for it. 
We will let 
 $$ J_f \colon = ( \lbrace f \rbrace_f \colon S_f ) $$
and we will call it the \emph{general ideal} of $f$.

In analogy with usual field extensions, the degree of a field extension is replace by the order which is equal to the transcendence degree of the field extension.

The following lemma tells us that the smaller the order we are using the deeper the ideal we get.
\begin{lemma}
	Let $K$ be a $\partial$-field and let $f,g \in K\lbrace x \rbrace$ be irreducible. 
	Then
	 $$J_f \subsetneq J_g \implies \ord^{\partial}(g) < \ord^{\partial}(f). $$
\end{lemma}
\begin{proof}
	For the sake of contradiction suppose that $J_f \subsetneq J_g$ but $\ord^{\partial}(f)\leq \ord^{\partial}(g)$.
	By the containment we have $f \in J_g$.
	This implies $S_gf \in \lbrace g\rbrace$.
	But $\ord^{\partial}(S_gf) \leq \ord^{\partial}(f)$ so by the Division Lemma, we have $g \vert S_g f$ which implies $g\vert f$.
	So we have $g \vert f$ and $f\vert g$ which shows the two ideals are equal.
	This is a contradiction.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ritt-Raudenbaush, Noetherianity, Prime Decomposition (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%

One thing that we have to contend with in differential algebra is non-Noetherianity. 
The rings are are dealing with have an infinite number of generators:
$$ K[x,x',x'',x''',\ldots].$$
This ring is abstractly isomorphic (as rings) to $K[x_1,x_2,x_2,\ldots]$ which is not Noetherian since it contains the chain 
$$\langle x_1 \rangle \subset \langle x_1,x_2\rangle \subset \langle x_1,x_2,x_3\rangle \subset \cdots.$$
There are still hopes.

The most naive thing to do would be to put the word ``differential'' in front of everything. 
\begin{problem}
	\begin{enumerate}
		\item Are all $\partial$-ideals in $K[x,x',x'',\ldots]$, $\partial$-finitely generated?
		\item Do $\partial$-ideals in $K[x,x',x'',\ldots]$ satisfy the ascending chain condition?
		\item Are these two properties equivalent?
	\end{enumerate}
\end{problem}
In commutative algebra, if a ring $R$ is Noetherian then so is $R[x]$. 
This property allows us to conclude inductively that every ideal in $F[x_1,x_2,\ldots,x_n]$ for $F$ a field is finitely generated. 
This is the famous Hilbert Basis Theorem.
Since Noetherianity is equivalent to every ideal being finitely generated, this shows that every ideal in a polynomial ring is finitely generated. 

It is a general theme in differential algebra that naive generalizations don't work. 
Properties (1) and (2) above are false. 
\begin{exercise}[Counter Example to ACC]
	Consider the chain of $\partial$-ideals in $K\lbrace x \rbrace$ given by 
	$$ [x^2]  \subset [x^2, (x')^2] \subset [ x^2, (x')^2, (x'')^2 ] \subset \cdots, $$
	is an infinite ascending chain. (See Pogudin's \href{http://www.lix.polytechnique.fr/Labo/Gleb.POGUDIN/files/da_notes.pdf}{notes} Proposition 1.13)
\end{exercise}

The examples given in the next exercise show that not only are all ideals not $\partial$-finitely generated but even products of $\partial$-finitely generated ideals are mostly not $\partial$-finitely generated.
Here we say a $\partial$-ideal $I$ is \emph{$\partial$-finitely generated} if and only if there exist some $f_1,\ldots,f_n \in R$ such that $I=[f_1,\ldots,f_n]_{\partial}$.
Here is the counter-example.
\begin{exercise}[Example of Infinite Generation]
	\begin{enumerate}
		\item The ideal $\lbrace x y\rbrace_{\partial}$ is not $\partial$-finitely generated.
		\item The ideals $[x]$ and $[y]$ in $K\lbrace x,y\rbrace$ are $\partial$-finitely generated but the product
		$$[x][y] = \langle x^{(i)}y^{(j)} \colon i,j \geq 0 \rangle = \lbrace xy \rbrace_{\partial} = [x]_{\partial} \cap [y]_{\partial}$$ 
		is not finitely $\partial$-generated.
	\end{enumerate}
\end{exercise}
To do this exercise it is useful to have Levi's Lemmas in hand which are given in \S\ref{S:levi}.



So what *do* we get?  We get the Ritt basis theorem.
\begin{theorem}[Ritt Basis Theorem]
	If $I \subset K\lbrace x_1,\ldots,x_n \rbrace$ is a radical ideal then there exists some $u_1,\ldots,u_e$ such that 
	 $$ I = \lbrace u_1,\ldots,u_e \rbrace_{\partial}.$$
\end{theorem}

$$ \mbox{ (Basis Theorem) $\implies$ (Noetherianity), (Prime Decomposition) }$$

\taylor{Explain how radicals miss a large part of the story}

\subsection{Noetherianity}
In the Kolchin topology there can exist infinite descending chains of ideals.
\begin{example}[Gap Chain]
In $K\lbrace x \rbrace$ the chain 
$$[x]\supset [x'] \supset [x''] \supset \cdots$$
for an infinite descending chain of prime ideals.
Geometrically this corresponds to the constants being contained in linear polynomials being contained in quadratic polynomials etc. 
 $$ \lbrace x' =0 \rbrace \subset \lbrace x'' =0 \rbrace \subset \lbrace x''' =0 \rbrace \subset \cdots. $$
\end{example}
For ascending chains of radical ideals there can be no such example: Ritt's Basis Theorem implies the Kolchin topology on $\widehat{K}^n$ is Noetherian (A topological space is Noetherian if and only if any infinite descending chain of irreducible closed sets terminates). 

\begin{theorem}[Noetherianity of the Kolchin Topology]
 Kolchin topology on $\widehat{K}^n$ is Noetherian. 
 Equivalently any infinite ascending chain of radial differential ideals in $K\lbrace x_1,\ldots, x_n\rbrace$ terminates.
\end{theorem}
\begin{proof}
	Consider for the sake of contradiction and infinite ascending chain of $\Delta$-ideals
	 $$I_1 \subsetneq I_2 \subsetneq I_3 \subsetneq \cdots. $$
	Let $I = \bigcup_{j \geq 1} I_j$.
	By the Ritt Basis Theorem there exists some $f_1,\ldots,f_n\in R$ such that $I = \lbrace f_1,f_1,\ldots,f_n\rbrace_{\Delta}$.
	Since $f_i \in I$ there exists some $N_i$ such that $f_i \in I_{N_i}$. 
	Let $N= \max\lbrace N_1,\ldots,N_n\rbrace$. 
	We have $I \subset I_N$ and hence $I=I_N$ and the chain terminates.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differential Transcendence (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let $f_1,\ldots,f_n \in K\lbrace x_1,\ldots,x_n\rbrace$ and consider the differential system 
 $$ \Gamma \colon \quad f_1=\cdots = f_e =0.$$
This is just a space of solutions of differential equations. 

The algebraic object associated to this space is the ring 
 $$ A=K\lbrace x_1,\ldots, x_n\rbrace/[f_1,\ldots,f_e],$$
and we try to read off properties of the space from the ring $A$.



A big difference between algebraic geometry and differential algebraic geometry is how intersection theory works, as shown in Figure~\ref{F:two-pictures}.

\taylor{

For random $f,g \in K\lbrace x,y \rbrace$ we expect $K\lbrace x,y\rbrace/[f,g]$ to be finite dimensional. 
The idea is that (say) $f$ will kill off the higher derivatives of $x$ and $g$ will kill off the higher derivatives of $y$ and we will only be left with a ring which is the quotient of finitely many variables $x$ and $y$.

}



Let $F\supset K$ be a $\partial$-finitely generated extension of $\partial$-fields.
One has the following important correspondence:
$$ \trdeg_K^{\partial}(F)=0 \iff \trdeg_K(F)<\infty. $$
Geometrically this corresponds to the fact that a differential algebraic variety $\Sigma$ over $K$ has differential dimension zero if and only if it has finite absolute dimension
$$\dim^{\partial}(\Sigma)=0 \iff a(\Sigma)<\infty.$$ 
The number $a(\Sigma)$ is the \emph{absolute dimension} of the differential algebraic variety $\Sigma$ \cite[\S 2, pg 485]{Buium1993}. 

In the case that $\Sigma$ is irreducible $a(\Sigma)$ can be computed as the transcendence degree of the function field $K(\Sigma)$ over $K$.
Equivalently it can be computed as the (Krull) dimension of the underlying jet scheme $\Sigma^{[\infty]}$ of $\Sigma$.
By this we just mean the usual scheme-theoretic dimension of the scheme $\Sigma^{[\infty]}$. 
One issue we have to contend with is the fact that rings and ideals defining $\Sigma^{[\infty]}$ are not finitely generated, so the proofs of many theorems we would like to use do not apply directly.

The existence of the two notions of dimension means there are two ways to think about differential algebraic varieties: in terms of  $\partial$-indeterminates $\lbrace x_i \colon 1\leq i \leq n \rbrace$ and in terms of classical indeterminates $\lbrace x_i^{(j)} \colon 1\leq i \leq n, j\geq 0 \rbrace$ (see Figure~\ref{F:two-pictures}).
From the perspective of differential indeterminates, ``neighborhoods'' of intersections of $\partial$-dimension zero are described by finite dimensional schemes.
Both perspectives are useful.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.3]{two-pictures.eps}
	\end{center}
	\caption{The above picture shows two ways to draw the intersection of the differential algebraic varieties $\Sigma \colon x'=0$ and $\Gamma \colon x+(y')^2=0$. 
		From the point of view of differential transcendence degrees, $\Sigma \cap \Gamma$ has dimension zero. 
		From the point of view of transcendence degrees or absolute dimensions, $\Sigma\cap \Gamma$ has dimension 2.
	}\label{F:two-pictures}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differential Closures (unstable) \label{S:univariate-differential-closures}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For this section I'm going to borrow heavily from my model theory friends.
See for example Ronnie Nagloo's thesis or David Marker's notes \cite{Marker2000}.


%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix: Levi's Lemmas on $[y^n]$ and $[xy]$ (unstable)}\label{S:levi}
%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Levi1942} the following interesting questions were addressed:
\begin{problem}
	\begin{enumerate}
		\item Consider the differential ideal $[x^n]$.
		Which monomials  are in $[x^n]$? 
		\item Consider the $\partial$-ideal $[xy]$. 
		Which monomials $x^{\alpha}y^{\beta}$ are in $[xy]$?
		\item What about for $\lbrace xy \rbrace_{\partial}$?
	\end{enumerate}
\end{problem}

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.5]{levi.eps}
	\end{center}
	\caption{A picture of the variety $xy=0$ in the classical and differential algebraic case. 
		The big thick origin depicts the nilpotents contained in $[xy]_{\partial}$}
\end{figure}
Levi's Lemmas tell us exactly which monomials are in these rings. 
These results are particularly interesting because $xy=0$ and $x^n=0$ are exactly the two ways that zero divisors appear in differential algebra. 
One is telling us what happens at the intersection of two curves and one is telling us what happens at nilpotents.

\subsection{Monomials, Weight, and Degree}
It will be conveniant to introduce some notation for monomials in $K[ y ]_{\partial}$ where $y$ is a single indeterminant. 
For $\alpha \in \ZZ_{\geq 0}[\partial]$ with $\alpha = a_0 + a_1 \partial + \cdots + a_r \partial^r$ we will write 
$$ y^{\alpha} = y^{a_0} (y')^{a_1} \cdots (y^{(r)})^{a_r}.$$
We assign two gradings to $K[y]_{\partial}$. 
A grading by weight and grading by degree. 
\begin{definition}
We define the \emph{degree} of $y^{\alpha}$ (or just $\alpha$) to be 
$$\deg(y^{\alpha}) = a_0 + a_1 + \cdots + a_r.$$
We define the \emph{weight} of $y^{\alpha}$ (or just $\alpha$) to be
$$ \wt(y^{\alpha}) = a_1 + 2a_2 + \cdots + r a_r.$$
\end{definition}
A basic observation is that terms with no derivatives have weight zero, terms with single derivatives have weight one, terms with only second derivatives have weight two etc. 
If we view $\alpha \in \ZZ[\partial]$ as a polynomial $\alpha(x) \in \ZZ[x]$ then $\deg(\alpha) = \alpha(0)$ and $\wt(\alpha) = \alpha'(0)$. 

\begin{exercise}
	Let $A \in K\lbrace y \rbrace$ be a differential monomial which is homogeneous in both degree and weight. Show that $\partial(A)$ is homogeneous in both degree and weight and that 
	$$ \deg(\partial(A)) = \deg(A), \quad \wt(\partial(A)) = \wt(A)+1.$$
\end{exercise}

In several $\partial$-indeterminates $x=(x_1,\ldots,x_n)$ we can again use a multi-index notation. 
We will take $\alpha = (\alpha_1,\ldots,\alpha_n) \in \ZZ_{\geq 0}[\partial]^n$ and let 
$$ x^{\alpha} = (x_1,\ldots,x_n)^{(\alpha_1,\ldots,\alpha_n)} = x_1^{\alpha_1}\cdots x_n^{\alpha_n}$$
as where for each $i$ we have $\alpha_i = a_{i0} + a_{i1}\partial + \cdots + a_{ir_i}\partial^{r_i}$ and $x_i^{\alpha_i} = x_i^{a_{i0} + a_{i1}\partial + \cdots + a_{ir_i}\partial^{r_i}}$ as before. 


\subsection{Levi's Lemmas for $[xy]$ (unstable) }

\begin{theorem}
	We have the following equality of ideals in $K\lbrace x,y\rbrace$,
	$\lbrace xy\rbrace_{\partial} = \langle x^{(i)}y^{(j)} \colon i,j \geq 0 \rbrace$.
\end{theorem}

The following computation is the babymost example. 
\begin{example}
	We will prove $x\partial(y) \in \lbrace xy\rbrace_{\partial}$.
	We have $\partial(xy) = \partial(x)y+x\partial(y) \in [xy]$.
	Multiplying by $x$ gives $\partial(x)xy + x^2\partial(y) \in [xy]$.
	This means $x^2\partial(y) \in [xy] $ and $x^2\partial(y)^2 \in [xy]$ which proves $x\partial(y) \in \lbrace xy\rbrace_{\partial}$.
\end{example}

\subsection{Levi's Lemmas for $[x^n]$ (unstable) }

\taylor{Copy over notes}
The following is the most basic thing we can prove. 
\begin{lemma}\label{lem:power-lemma}
	Let $R$ be a differential $\QQ$-algebra. 
	Let $I$ be a differential ideal. 
	Suppose that $a\in R$ satisfies $a^n \in I$. 
	Then $\dot{a}^{n} \in I$. 
\end{lemma}
\begin{proof}
	If $a^n \in I$ then $\partial(a^n) = n a^{n-1} \dot{a}\in I$. 
	By the $\QQ$-algebra hypothesis, $a^{n-1}\dot{a}\in I$. 
	Taking derivatives again we get $(n-1)a^{n-2}\dot{a}^2 + a^{n-1}\ddot{a}\in I.$
	After multiplying by $a'$ this implies $a^{n-2} \dot{a}^2 \in I$ since $a^{n-1}\dot{a} \in I$.
	This process continues. 
	Knowing that $a^{n-j}\dot{a}^j \in I$ allows us to show that $a^{n-j-1}\dot{a}^{j+1} \in I$ by taking derivatives, multiplying by $\dot{a}$ and using the inductive hypothesis to get rid of a term. 
	Setting $n-j-1=0$ we get $j=n-1$ which shows $\dot{a}^n \in I$.
\end{proof}

We aim to give a much stronger version of the above in what follows culminating in Lemma~\ref{lem:levi}.
\begin{lemma}\label{lem:monomials-of-derivative}
	A monomial $y^{\alpha}$ for $\alpha \in \ZZ_{\geq 0}[\partial]$ appears in $\partial^r(y^n)$ nontrivially if and only if it has degree $n$ and weight $r$.
\end{lemma}
\begin{proof}
	The proof is by induction on $r$.
	The case $r=0$ is trivial.
	In the inductive step one needs to take a derivative of $y^{a_0} \dot{y}^{a_1}\cdots (y^{(r)})^{a_r}$ and observe that every monomial has an exponent which is a modification of $(a_0, a_1,\ldots, a_r,0)$ by one of  $(-1,1,0,\ldots, 0,0), (0,-1,1,\ldots, 0,0), \ldots, (0,0,0, \ldots, -1,1)$.
	Note that 
	\begin{align*}
	\partial(y^{a_0} \dot{y}^{a_1}\cdots (y^{(r)})^{a_r}) =& \partial(y^{a_0}) \dot{y}^{a_1}\cdots (y^{(r)})^{a_r} \\
	&+ y^{a_0} \partial(\dot{y}^{a_1})\cdots (y^{(r)})^{a_r} \\
	&+ \cdots \\
	&+ y^{a_0} \dot{y}^{a_1}\cdots \partial(y^{(r)a_r}).
	\end{align*}
	Since  $\partial((y^{(j)})^{a_j})=(y^{(j)})^{a_j-1}y^{(a_j+1)}$.
	This gives the contribution $-e_j+e_{j+1}$ in the exponent if $e_j$ is the $j$th elementary basis vector. 
	In the notation where exponents are $\ZZ_{\geq 0}[\partial]$ the new exponent is $-\partial^j + \partial^{j+1}$.
\end{proof}

Before diving in to the next Lemma the reader may wish to consult the example found directly after the proof.
\begin{lemma}\label{lem:lowest-monomial}
	The lowest monomial of $\partial^m(y^n) \in K\lbrace y \rbrace$ with respect to its unique ranking is 
	$$\Low(\partial^m(y^n)):= \partial^q(y)^{n-r}\partial^{q+1}(y)^r$$
	where $q, r \in \ZZ_{\geq 0}$ are the unique integers with $0 \leq r \leq n$ in the Euclidean algorithm such that
	$m= qn+r$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:lowest-monomial}]
	First observe that 
	$$ \deg( \partial^q(y)^{n-r}\partial^{q+1}(y)^r)= n,$$
	$$ \wt( \partial^q(y)^{n-r}\partial^{q+1}(y)^r) = q(n-r) + (q+1)r  =qn+r =m,$$
	so by Lemma~\ref{lem:monomials-of-derivative}, $\partial^q(y)^{n-r}\partial^{q+1}(y)^r$ appears as a monomial of $\partial^m(y^n)$.
	We now prove this is the lowest by induction on $m$. 
	We will let $y^{\beta}$ be a minimal term with $\beta\in \ZZ_{\geq 0}[\partial]$.
	If $m=0$ then $y^n$ is the lowest term which is when $q=0$ and $r=0$.
	We now do the inductive step. 
	The lower term of $\partial^{m+1}(y^n)$ will be a term of $\partial(y^{\gamma})$ where $\gamma= (n-r)\partial^q + r \partial^{q+1}$. 
	This is because if $\beta \prec \beta'$ then $\beta -\partial^i + \partial^{i+1} \prec \beta' -\partial^i + \partial^{i+1}$. 
	Now $\gamma - \partial^q + \partial^{q+1}$ is the smallest term.
	In the case $r=n-1$ we find that $m+1 = (q+1)n$ and can check directly that $\partial( \partial^q(y) \partial^q(y)^{n-1})$ has the term $\partial^{q+1}(y)^n$. 
	In the case $r<n-1$ we find that $m+1 = n+r+1$ and that $\gamma -\partial^q + \partial^{q+1} = (n-(r+1))+(r+1)\partial^{q+1}$ is the lowest term.
	\cite[\S21]{Ritt1943}
\end{proof}
Here is an example of the above theorem.
\begin{example}
	In the case $m=10$ and $n=3$ the proposition is saying that $\partial^{10}(y^3)$ has $\partial^3(y)^2\partial^4(y)$ as the lowest term in the ordering since $10=3(3)+1$.
\end{example}


Levi's Criterion will give a recipe on the weight and degree of $y^{\alpha}$ for membership in the differential ideal generated by $y^n$.
\begin{theorem}[Levi's Lemma]\label{lem:levi}
	Let $n$ be a non-negative integer. 
	Suppose that $y^{\alpha}$ for $\alpha \in \ZZ_{\geq 0}[\partial]$ has weight $w$ and degree $d$. We have the following
	$$ w < f(n,d) \implies y^{\alpha} \in [ y^{n+1} ] $$
	where $f(n,d) = q_d(q_d-1)(n-1)+2q_dr_d$ where $q_d$ and $r_d$ are the unique integers such that $d=q_dn+r_d$.
\end{theorem}
\begin{proof}
	We will say a monomial $y^{a_0 + a_1 \partial + \cdots + a_s \partial^s}$ is \emph{dilute} (for $n$) if and only if for all $j$, $a_j + a_{j+1} < n$. 
	If $y^{\alpha}$ is not dilute it will be called \emph{concentrated} (for $n$).
	We will omit the ``for $n$'' from now on.
	A differential polynomial is called \emph{concentrated} (resp \emph{dilute}) if all of its monomials are. 
	There are a series of claims:
	\begin{enumerate}
		\item \label{L:division} If $y^{\alpha}$ is concentrated then there exists some $j$ such that $L_j=\Low(\partial^j(y^n))$ divides $y^{\alpha}$.
		\begin{proof}
			Recall that $\Low( \partial^j(y^n)) = \partial^{q_j}(y)^{n-r_j}\partial^{q_j+1}(y)^{r_j}$ where $j=q_jn+r_j$ is the representation from the division algorithm.
			By definition $y^{\beta}=\partial^i(y)^a\partial^{i+1}(y)^{b}\vert y^{\alpha}$ for some $a,b$ with $a+b\geq n$. 
			Without loss of generality we can asume $a+b=n$. 
			We also must have $\wt(y^{\beta})=ia+(i+1)b=m$.
			To get the division we increase $j$ until $q_j=a$ so that $j = an$.
			In this case $L_j = \partial^a(y)^n$.
			We can then increase $j$ further to get $j=an+r$ for $0\leq r <n$ and $L_j=\partial^a(y)^{n-r}\partial^{a+1}(y)^r$.
		\end{proof}
		\item  
		\begin{lemma}
			If $A$ is degree-homogeneous of degree $d$ and weight-homogenous of weight $w$ it is congruent to a dilute bihomogenous polynomial of the same degree and weight modulo $[y^n]$.
		\end{lemma}
		\begin{proof}
			By the previous $y^{\alpha}\equiv 0 \mod [y^n]$ for every $y^{\alpha}$ concentrated. 
			The result follows.
		\end{proof}
		This shows that every $A$ is congruent to an $n$-dilute $\partial$-polynomial modulo $[y^n]$. 
		Conversely, we will show that the only $n$-dilute polynomial in $[y^n]$ is the zero polynomial.
		\item If $y^{\alpha}$ is degree $d$ and has weight less than $f(n,d)$ then $y^{\alpha}$ is $n$-concentrated. 
		\item By hypothesis, we have a concentrated polynomial which is equivalent to a dilute one. 
	\end{enumerate}
	
	We can peel off the concentrated terms, term-by-term and kill them. 
	Suppose $A$ is not dilute. 
	Then let $A=bB+R$ where $B$ is the lowest concentrated. 
	We have $B = L_j H$ for some $H$ and $j$. 
	We have $\partial^j(y^n) = cL_j + \sum_{i=1}^s c_i P_i$ where $P_i \succ L_j$. 
	Then 
	\begin{align*}
	A &= bB+R \\
	&= b \left( \frac{1}{c} \left[ \partial^j(y^n) - \sum_{i=1}^s c_i P_i \right] \right)H + R\\
	&\equiv \frac{-b}{c} \sum_{i=1}^s c_i P_i H + R \mod [y^n],
	\end{align*}
	and the terns $P_iH$ are higher that $G$ in the ordering. 
	We now repeat this process to kill higher and higher terms. 
	Since there are only finitely many terms we can do this with, the process terminates. 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Partial and Computational Differential Algebra (unstable)}\label{S:computational}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

I want to describe our motivating computational problem with the Painlev\'e IV again, but before doing that I want to give some references. 
We are going to follow Boulier's notes \href{https://hal.archives-ouvertes.fr/hal-02378197v2/document}{here} and \href{https://hal.archives-ouvertes.fr/hal-00139364/document}{here} for this romp through computational differential algebra.
Boulier is one of the current experts on Computational Differential Algebra and he wrote the base of the software I'm going to be using.
There is also Ovchinnikov's \href{http://qcpages.qc.cuny.edu/~aovchinnikov/MATH87800/notes.pdf}{course notes} who is another expert on Computational Differential Algebra and whose style and notation I think is closer to say Atiyah-MacDonald \cite{Atiyah2016}
or Kaplansky \cite{Kaplansky1976}. 
A lot of the basic proofs are similar to the proofs in  Ritt's second book \cite{Ritt1950} but are a bit harder to read.


We have a large system of differential equations given by the Schlesinger equations for $A_0(x),A_1(x),A_2(x)$ and $A_{\infty}(x)$ which define the isomonodromic deformation. 
We have a matrix element $a_{12}=k(x)(t-q(x))$ of 
$$\begin{pmatrix}
a_{11} & a_{12} \\ a_{21} & a_{22} \\
\end{pmatrix} \frac{1}{t(t-1)(t-x)} = \frac{A_0(x)}{t}+\frac{A_1(x)}{t-1}+\frac{A_2(x)}{t-x}.$$
We want to derive Painlev\'e VI equation
\begin{align*}
\end{align*}

We need to plug this into the system 
$$ \dfrac{dA_0}{dx} = [A_0,A_2], \quad \dfrac{dA_1}{dx} = [A_1,A_2], \quad \dfrac{dA_{\infty}}{dx}=0,$$ 
which has a horrific number of variables (in addition to $k(x)$ and $q(x)$)
$$ A_0(x) = \begin{pmatrix}
a_0(x) & b_0(x) \\
c_0(x) & d_0(x)
\end{pmatrix}, \quad A_1(x) = \begin{pmatrix}
a_1(x) & b_1(x) \\
c_1(x) & d_1(x)
\end{pmatrix}, $$
$$\quad A_2(x) = \begin{pmatrix}
a_2(x) & b_2(x) \\
c_2(x) & d_2(x)
\end{pmatrix},
\quad A_{\infty}(x) = \begin{pmatrix}
\theta_{\infty} & 0\\
c_{\infty}(x) & -\theta_{\infty}
\end{pmatrix}.$$
Moreover these differential equation are subject to algebraic equations in addition to differential equations:
$$ A_0+A_1+A_2+A_{\infty}=0, \qquad a_j(x) + d_j(x)=0, \quad j=0,1,2,\infty$$
$$k(x) +(1 +x)b_0(x) + xb_1(x) + b_2=0, \quad k(x)q(x) + b_0(x)x=0.$$
I'm going to use a computer to write eliminate all of these equations systematically and just derive an equation for $q(x)$. 
This equation will be the Painlev\'e IV. 



\noindent In \textsf{Maple}, I enter the following commands
\taylor{This command is incorrect as given}
\begin{spverbatim}
	>with(diffalg)
	>syst1 := [y(x)*diff(a0(x), x) - b0(x)*c2(x) - c0(x)*b2(x), y(x)*diff(b0(x), x) - a0(x)*b2(x) + b0(x)*d2(x) - a2(x)*b0(x) - b2(x)*d0(x), y(x)*diff(c0(x), x) - c0(x)*a2(x) + d0(x)*c2(x) - c2(x)*a0(x) - d2(x)*c0(x), y(x)*diff(d0(x), x) - c0(x)*b2(x) - b0(x)*c2(x), (y(x) - 1)*diff(a2(x), x) - b1(x)*c2(x) - c1(x)*b2(x), (y(x) - 1)*diff(b2(x), x) - a1(x)*b2(x) + b1(x)*d2(x) - a2(x)*b1(x) - b2(x)*d1(x), (y(x) - 1)*diff(c2(x), x) - c1(x)*a2(x) + d1(x)*c2(x) - c2(x)*a1(x) - d2(x)*c1(x), (y(x) - 1)*diff(d2(x), x) - c1(x)*b2(x) - b1(x)*c2(x), diff(c3(x), x), a0(x) + a1(x) + a2(x) + theta4(x), (y(x) - 1)*diff(d2(x), x) - c1(x)*b2(x) - b1(x)*c2(x), diff(c3(x), x), a0(x) + a1(x) + a2(x) + theta4(x), b0(x) + b1(x) + b2(x), c0(x) + c1(x) + c2(x) + c3(x), d0(x) + d1(x) + d2(x) - theta4(x), a0(x) + d0(x), a1(x) + d1(x), a2(x) + d2(x), k(x) - (-1 - y(x))*b0(x) + y(x)*b1(x) + b2(x), diff(y(x), x) - 1, diff(theta4(x), x)]
	>R1 := differential_ring(derivations = [x], notation = diff, ranking = [a0, b0, c0, d0, a1, b1, c1, d1, a2, b2, c2, d2, c3, k, y, q, theta4]);
	>ideal1 := Rosenfeld_Groebner(syst1, R1);
\end{spverbatim}
\ \newline
The first line imports the differential algebra package authored by Boulier et al into \textsf{Maple}. 
The second line is our system of differential equations where everything is expressed as a function of $x$. 
I did a quite a number of symbolic computations to arrive at these equations in the matrix entries. 
These are the entries in the equations above. 
Note that there are both algebraic equations and differential equations. 
The instantiation of the differential ring sets up what we are going to work with. 
It says we are working with a single derivative in $x$ and at we are going to use a term order determined by a so called orderly ranking with 
$$a_0\succ b_0 \succ c_0 \succ d_0 \succ a_1 \succ b_1 \succ c_1 \succ d_1 \succ a_2\succ b_2 \succ c_2 \succ d_2 \succ c_3 \succ  k \succ  y \succ q \succ  \theta_4.$$
The output of the Roesenfeld-Groebner computation is going to be a list of list
$$ C_1, C_2,\ldots, C_c, \quad C_j = \lbrace u_{j1}, u_{j2},\ldots, u_{js_j} \rbrace $$
of ordered list of $\partial$-polynomials $C_j$ called characteristics sets with respect to our ordering.

What are these Characteristic sets? 
Each one of these is associated to a differential prime ideal $P_h$ associated to the radical of the $\partial$-ideal generated $I$ generated by our equations.
That is our radical $\partial$-ideal $I$ has 
$$ I = P_1 \cap P_2 \cap \cdots \cap P_c $$
where the $P_i$ are prime ideals and the collections.
They aren't quite generators of the prime ideal but they allow us to solve the membership problem in the sense that there is a multivariate division algorithm (which, IMHO, is crappy and unsatisfying) where $f$. 
That for a characteristic set $C$ associated to a prime ideal $P$ one has 
$$ u\in P \iff \red_C(u) =0 $$
whre $r_C(u)$ is the remainder after pseudodivision by $C$.

While we are here I have to mention an extremely famous and important open problem in differential algebra and that is the Ritt Problem.
\begin{problem}[Ritt Problem]
	Given characteristic sequences $C_1$ and $C_2$ of two differential prime ideals $P_1$ and $P_2$ determine if $P_1 \subset P_2$.
\end{problem}
This problem is very open and Michael Singer (one of the authors of \cite{Put2003} and extraordinary differential algebraist) calls this the most important open problem in Differential Algebra. 

The main issue is subtle and a lot of introductions don't point this out so I'm going to try to be explicit about this:
\begin{center}
Characteristic sequences are not $\Delta$-generators of an $\Delta$-ideal!
\end{center}
Characteristic sets are not a basis! They are much weaker than that. 
They only \emph{characterize} membership.

\section{Monomials, Ranking, and Orders}\label{S:PDE-ranking}
Let $R$ be a $\Delta$-ring with $\Delta = \lbrace \partial_1,\ldots,\partial_m \rbrace$.
Let $A = R[y_1,\ldots,y_n]_{\Delta}$. 
Let $\Theta$ be the collection of differential operators
$$ \Theta = \lbrace \partial^{\alpha} \colon \alpha \in \ZZ_{\geq 0}^n \rbrace$$
where $\partial^{\alpha} = \partial_1^{\alpha_1}\cdots \partial_m^{\alpha_m}$.
\begin{definition}
	A \emph{ranking} is a total ordering $\prec$ on the set of differential variables
	$ \lbrace \theta(y_j) \colon \theta \in \Theta, 1\leq j \leq n \rbrace $
	satisfying the following two axioms
	\begin{enumerate}[topsep=0pt]
		\item $ u \prec \theta u$, 
		\item $u \prec v \implies \theta u \prec \theta v $
	\end{enumerate}
\end{definition}
For a given indeterminate $\theta(u)$ we define a filtration $(K[y_1,\ldots,y_n]_{\Delta})_{\leq \theta(u)}$ to be the ring generated by variables of lower rank.

Just as in the ordinary case we make a sequence of definitions.
\begin{definition}
	Let $f \in K[x_1,\ldots,x_m]_{\Delta}$ and fix a ranking on the variables. 
	The \emph{leader} $\ell_f$ of $f$ is the higher rank element of $\lbrace \theta x_j \colon 1\leq j \leq n, \theta \in \Theta$ such that $\partial f/\partial \ell_f\neq 0$. 
	If let $\ell = \ell_f$ and write 
	$$f = a_d \ell^d + a_{d-1} \ell^{d-1} + \cdots + a_0,$$
	with $a_j \in K[x_1,\ldots,x_n]_{\Delta, \leq \ell}$ then
	\begin{itemize}[topsep=0pt]	 
		\item  The top coefficient $a_d = I_A$ is called the \emph{initial} respect to the ranking.
		\item The partial derivative of $f$ with respect to the leader $\partial f/\partial \ell$ is called the \emph{separant} of $f$ with respect to the ranking and we denote it by $S_f$.
		\item The degree $d$ is the \emph{leader degree} and we denote it by $\ldeg(f)$.
	\end{itemize}
\end{definition}


Note that to compare $f$ and $g$ we can use $\ell_f^{\ldeg(f)}$ and $\ell_g^{\ldeg(g)}$.
The set of elements 
 $$ \lbrace \theta(x_j)^d \colon \theta \in \Theta, d\geq 1 \rbrace $$
is an ordered set called the set of ranks. 
We will set $\rk(f) = \ell_f^{\ldeg(f)}$ and call it the \emph{rank} of $f$ and write $f\prec g$ if and only if $\rk(f) \prec \rk(g)$.

Recall that a \emph{term order} is total ordering on monomial that respects multiplication. 
That is, it is a total ordering $\prec$ for that for all monomial $M$,$N$, and $L$ if $M\prec N$ then $LM \prec LN$. 
Given a ranking there is an induced term order on the collection of monomials given by taking lexicographic order on the differential variables. 
We will let $\prec$ also denote the term order induced by the ranking $\prec$. 

\begin{example}
	For a single variable $y$ there is a unique term in $K\lbrace y \rbrace$. 
	The monomials $y^{\alpha}$ are ordered first by order and then by degree. 
	Ritt phrases this as saying $y^{\alpha} \prec y^{\beta}$ if and only if the greatest $i$ such that $\alpha_i - \beta_i \neq 0$ we have $\alpha_i -\beta_i <0$.  
\end{example}

\begin{theorem}
	Fix a ranking on a ring of differential polynomials.
	If $A$ is a differential polynomial and $\LM(A) = I \ell^m$ where $I$ is the initial and $\ell^m$ is the leader to some power then $\LM(\partial(A)) = (I m \ell^{m-1}) \partial(\ell)$ with new leader $\partial(\ell)$ and new initial $mI\ell^{m-1}$.
\end{theorem}
\begin{proof}
	We have $\partial(I \ell^m) = \partial(I)\ell^m + m\ell^{m-1}\partial(\ell)I$. 
	We have $\partial(\ell) \succ \ell \succ I$. 
	This means that $\partial(\ell)$ will precede any differential variable in any of the monomials of $\partial(I)$ by the second axiom of rankings.
	Hence $\LT(\partial(A) ) = m\ell^{m-1}I \partial(\ell)$ and $I_{\partial(A)} = m \ell_A^{m-1} I_A$ and $\ell_{\partial(A)} = \partial(\ell_A)$.
\end{proof}




%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Characteristic Sequences (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Characteristic sequences $G=(g_1,\ldots,g_r)$ are things that characterize membership in a $\partial$-ideal $I$.
There are like a crappy version of a Groebner basis for an ideal in that they can determine membership of the ideal but they actually don't generate the ideal. 
They are a valuable computational technique but also a source of many of our headaches in that one of the largest open problems in differential algebra, the Ritt Problem, is centered around them.

For the purpose of having something to focus on we state the main result of this section. 
Definitions will be developed as we go along. 

\subsection{Chomp}\label{S:chomp}

The game chomp is a game that is useful for describe when certain processes terminate. 
This can be used in a proof of the Hilbert Basis Theorem or when we want to show that Buchberger's Algorithm terminates. 
A drawing of three moved in the game played in the plane is pictured in Figure~\ref{F:chomp}.
\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.33]{chomp.eps}
	\end{center}
\caption{The moves in the game chomp are pictured.
At each stage a vertex is selected in $\ZZ_{\geq 0}^2$ and every lattice point above and below it are removed. 
After a finite number of moves this game must terminate. }\label{F:chomp}
\end{figure}

The initial board is setup so there is a bead at each point in the first quadrant of the xy-plane including the lattice points on the axes. 
At each round a chomp is made. 
To make a chomp you select one bead is selected and all the beads above above and to the right are removed (so an infinite number of beads are removed at each stage). 
The game is over when there are no more chomps to be made.
I really didn't describe a two player winning or losing strategy, but that doesn't really matter. 
What matters is that the game terminates. 

The idea is that each node $(i,j)$ in the $xy$-plane represents a monomial $x^iy^j$ and the chomp represents all of the monomials that are divisible the node we selected. 

The are obvious variants of this game in higher dimensions.

\subsection{Well Orderings}
In this section we will let $(S,\leq)$ be a quasiordered set.
A quasiordering is just a reflexive and transitive ordering. 
It is distinct from a partial ordering in that $a\leq b$ and $b\leq a$ implies $a=b$ in a partial ordering. 
When we want a strict quasiorder we will use the $a<b$ to mean $a\leq b$ and $a\neq b$ for $a,b\in S$. 

A quasiordered set $(S,\leq)$ is a \emph{well ordered set} if and only every $S_0 \subset S$ has a minimal element. 
\begin{exercise}
	The following are equivalent for a quasiordered sets $(S,\leq)$.
	\begin{enumerate}[topsep=0pt]
	\item \label{I:well-order} The set $(S,\leq)$ is a well ordering.
	\item \label{I:dcc} Every descending sequence $s_1\geq s_2 \geq \ldots$ in $S$ terminates. 
	\end{enumerate}
\end{exercise}
\begin{proof}
	To show that the descending chain condition implies a well ordering we will argue by contrapositive. 
	Suppose there is a set without a minimal element. 
	Then form a descending chain. 
	
	To show that the well ordering property implies descending chains we again use a contrapositive. 
	Given an infinite descending chain $a_0>a_1>a_2 >\cdots$ the set $S_0 = \lbrace a_j \rbrace_{j \geq 0}$ is a set without a minimal element. 
\end{proof}

\subsection{Characteristic Sequences}
Let $\Delta = \lbrace \partial_1,\ldots,\partial_m\rbrace$ and let $\Theta = \lbrace \partial^{\alpha} \colon \alpha \in \ZZ_{\geq 0}^m\rbrace$ whre $\partial^{\alpha} = \partial_1^{\alpha_1} \cdots \partial_m^{\alpha_m}$. 

\begin{lemma}\label{L:dickson}
	There denote not exist an infinite sequence $\theta_1,\theta_2,\ldots$ where $\theta_j \in \Theta$ such that $\theta_j \nmid \theta_{j+i}$ for all $j\geq 1$ and $r>1$. 
\end{lemma}

\begin{exercise}
	Prove Lemma~\ref{L:dickson} using the idea that the game chomp from \S\ref{S:chomp} terminates. 
\end{exercise}

\begin{definition}
Fix a ranking on $K[x_1,\ldots,x_n]_{\Delta}$. 
Let $f,g \in K[x_1,\ldots,x_n]_{\Delta}$.
We say that $f$ is \emph{reduced} with respect to $g$ if and only if 
\begin{enumerate}[topsep=0pt]
	\item No property derivative of the leader of $g$ appears in $f$: for all $\theta \in \Theta$, $\partial f/\partial \theta(\ell_g) =0.$
	\item If the leaders are the same then $g$ has bigger leader degree: $\ldeg(g) > \ldeg(f)$.
\end{enumerate}
\end{definition}

\begin{theorem}[Pseudodivision Algorithm]\label{T:pde-pseudodivision}
	Let $g \in K[x_1,\ldots,x_n]_{\Delta}$. 
	For all $f \in K[x_1,\ldots,x_n]_{\Delta}$ there exists some $\widetilde{f}$ reduced with respect to $g$ such that
	 $$ s f \equiv \widetilde{f} \qquad [g]_{\Delta} $$
	where $s$ is in the multiplicative set generated by $I_g$ and $S_g$.
\end{theorem}

The algorithm above we call the pseudodivision algorithm with respect to a single variable. 

\begin{exercise}
	Building on the pseudodivision algorithm in the ODE case (Theorem~\ref{T:ode-pseudodivision}) construct an algorithm which gives the desired output to Theorem~\ref{T:pde-pseudodivision}.
\end{exercise}

For sequences $G=(g_1,\ldots,g_r)$ of differential polynomials we will use the notation $\vert G \vert = r$.

\begin{definition}
	A ordered sequence $G=(g_1,\ldots,g_r)$ is \emph{autoreduced} if and only if for all $g_i,g_j$ with $i\neq j$ we have $g_i$ reduced with respect to $g_j$. 
\end{definition}

We will always order our sequences with respect to a ranking for that 
$$g_1 \prec g_2 \prec \cdots \prec g_r.$$
Notice that if $\rk(g_i)=\rk(g_j)$ then the sequence is not autoreduced, so this arrangement can always be performed. 

\begin{definition}
	We will say that $f$ is reduced with respect to $G=(g_1,\ldots,g_r)$ if and only if $f$ is reduced with respect to $g_j$ for each $j$.
\end{definition}

\begin{theorem}[Pseudodivision Algorithm]\label{T:pde-pseudodivision}
	Let $g \in K[x_1,\ldots,x_n]_{\Delta}$. 
	For all $f \in K[x_1,\ldots,x_n]_{\Delta}$ there exists some $\widetilde{f}$ reduced with respect to $g$ such that
	$$ s f \equiv \widetilde{f} \qquad [g]_{\Delta} $$
	where $s$ is in the multiplicative set generated by $I_g$ and $S_g$.
\end{theorem}

\begin{itemize}
	\item $\red_G(f) = \widetilde{f}$ the reduction of $f$ with respect to $G$.
	\item $m_G(f)=s$ the element of $S_G$ given by the reduction algorithm.
	\item  $S_G$ the multiplicative set generated by $I_{g_i}$ and $S_{g_i}$ for $1\leq i \leq r$.
\end{itemize}

For any $h_1 \prec h_2 \prec \cdots \prec h_r$ which is comparable, we will have $\rk(h_i) = \rk(g_i)$ for each $i$. 

\begin{theorem}
	The do not exist infinite autoreduced sequences in $K[x_1,\ldots,x_n]_{\Delta}$. 
\end{theorem}
\begin{proof}
	Suppose there is some $g_1,g_2,\ldots$ which is infinite and autoreduced. 
	Then there is some $x_j$ such that infinitely manu leader have the form $\theta x_j$ for some $\theta \in \Theta$. 
	By the Lemma~\ref{L:dickson}, there is not infinite sequence $\theta_1,\theta_2,\ldots$ in $\Theta$ with $\theta_i \nmid \theta_{i+j}$ for $i\geq 1$ and $j>0$. 
	This means that $\theta_{3}(\theta_1(x_j))=\theta_2(x_j)$ for some $\theta_1 x_j$ and $\theta_2 x_j$ in the sequence. 
	This implies the set is not autoreduced.
\end{proof}

We now define a partial ordering on the collection of autoreduced sequences for the purposes of showing minimal ones exist. 
These will be the characteristic sequences.

\begin{definition}
Let $G = (g_1,\ldots,g_r)$ and $G=(h_1,\ldots,h_s)$ be two autoreduced sequences with respect to some ranking. 
We say that $H\prec G$ if and only if	
\begin{enumerate}[topsep=0pt]
	\item Reading from left to right we find some $j$ such that $h_j \prec g_j$ (we will assume that this $j$ is minimal and that for all of $i<j$ this does not hold)
	\item They match all the way but $s>r$. In other words, $H$ has more elements.
\end{enumerate}
\end{definition}

The following Lemma will be convenient, but can be skipped for now. 
\begin{lemma}\label{L:sequence-lemmas}
	Let $G=(g_1,\ldots,g_r)$ be an autoreduced sequence.
	\begin{enumerate}[topsep=0pt]
		\item \label{I:reduced-gives-lower} If $\red_G(f)$ is not in $K$ then we can build a sequence lower than $G$ involving $f$. 
		\item \label{I:initials-and-separants} Every $I_{g_i}$ and $S_{g_i}$ is reduced with respect to $G$. 
	\end{enumerate}
\end{lemma}
\begin{proof}
For the first part we compare $\rk(f)$ with $\rk(g_i)$. 
If at some point $\rk(f)\prec \rk(g_i)$ then we take $(g_1,\ldots,g_{i-1},f)$ as our new sequence. 
If no element it greater then $(g_1,\ldots,g_n,f)$ our new autoreduced sequence. 

Note that if some proper derivative of $\ell_f$ appears as $\ell_{g_i}$ then $\ell_f \prec \ell_{g_i}$ so this is where the first case applied. 
If $\ell_f = \ell_{g_i}$ then we replace $f$ by $g_i$ since it must be of lower degree again bringing us to the first case.
This proves the sequences obtained are indeed autoreduced. 

We first show that $I_{g_i}$ is reduced. 
We have $g_i = I_{g_i} \ell_{g_i}^{\ldeg(g_i)} + \cdots$.
If some proper derivative of $\ell_{g_j}$ appears in $I_{g_i}$ then $g_i$ is not reduced with respect to $g_j$.
If $\ell_{g_j}= \ell_{I_{g_i}}$ then we must have $i>j$.
\taylor{FINISH}

We now show that $S_{g_i}$ is reduced with respect to $G$.
\taylor{FINISH}


\end{proof}


We now give the well-ordering property.
\begin{theorem}
	Any set of autoreduced sequences has a minimal element.
\end{theorem}
\begin{proof}
	Let $\mathcal{G}$ be a collection of autoreduced sequences. 
	Form the sequences $\mathcal{G}_0 = \mathcal{G}$ and 
	 $$ \mathcal{G}_i = \lbrace (g_1,g_2,\ldots, g_i,\ldots) \in \mathcal{G}_{i-1} \colon \vert G \vert \geq i, \mbox{ $g_i$ minimal } \rbrace, \qquad i \geq 1. $$
	 Note that minimality makes sense since the collection of ranks
	 $$ \lbrace \ell_{g_i}^{\ldeg(g_i)} \colon (g_1,\ldots,g_i,\ldots) \in \mathcal{G}_{i-1} \rbrace $$
	is either empty or has a minimal element.
	If for every $i$ the set $\mathcal{G}_i$ is not empty then we could construct an infinite autoreduced sequence. 
	This is impossible. 
	
	Hence there exists some $N\geq 0$ such that $\mathcal{G}_N \neq \emptyset$ and $\mathcal{G}_{N+1}=\emptyset$. 
	At each stage $g_i$ was constructed to be minimal. 
	There will be no other autoreduced set below it by ``breaking an characterizer'' and there will be no autoreduced sets which are longer, any $G \in \mathcal{G}_N$ will be a minimal.
\end{proof}

\begin{definition}
	Fix a $\Delta$-ideal $I$ in $K[x_1,\ldots,x_n]_{\Delta}$ and some ranking. 
	A \emph{characteristic sequence} $G=(g_1,\ldots,g_r)$ for $I$ is a minimal autoreduced sequence of elements from $I$.
\end{definition}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ideal Membership (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
	Let $I$ be a $\Delta$-ideal in $K[x_1,\ldots,x_n]_{\Delta}$ not equal to the whole ring. 
	Fix a ranking on $K[x_1,\ldots,x_n]_{\Delta}$.
	Let $G=(g_1,\ldots,g_r)$ be a characteristic sequence of $I$. 
	\begin{enumerate}[topsep=0pt]
		\item If $f\in I$ then $\red_G(f)=0$. 
		\item If $I$ is prime then $\red_G(f)=0$ implies $f\in I$. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	We first prove the forward direction. 
	Let $f \in I$ and let $G \subset I$ be a characteristic seequence. 
	We have 
	 $$m_G(f) f \equiv \red_G(f), \quad [G], $$
	where $m_G \in S_G$ and $\red_G(f)$ are by the division algorithm.
	Since $[G] \subset I$ we have $\red_G(f) \in I$.
	
	If $\red_G(f) \in K[x_1,\ldots,x_n]_{\Delta}\setminus K$ is a non-coefficient element, since it would be reduced with respect to $G$ there would exists some autoreduced set lower than $G$ by Lemma~\label{L:reduced-gives-lower} which is a contradiction by minimality of $G$. 

	We also don't have $\red_G(f) \in K^{\times}$ by hypothesis. 
	
	We must have $\red_G(f)=0$. 
	
	Conversely, suppose that $I=P$ is prime. 
	Let $f \in K[x_1,\ldots,x_n]_{\Delta}$ be such that 
	 $$ m_G(f) f \equiv 0 \quad [G] $$
	as in the hypothesis of the Theorem.
	This implies that $m_G(f) f \in [G] \subset P$ which implies that $m_G(f)\in P$ or $f \in P$.
	We will show that $f\in P$ by way of contradiction.
	Suppose that $m_G(f) \in P$.
	
	This gives that $I_{g_i} \in P$ or $S_{g_i} \in P$.
	But again Lemma~\ref{L:sequence-lemmas} we get a contradiction to the minimality of $G$. 
\end{proof}

This makes the statement of the Ritt Problem more clear. 


\iffalse 
\begin{theorem}
	\taylor{must be a one variable thing}
	Let $I$ be a $\Delta$-ideal in $K[x_1,\ldots,x_n]_{\Delta}$. 
	Fix a ranking on $K[x_1,\ldots,x_n]_{\Delta}$.
	The following are equivalent for an ordered set $G=(g_1,\ldots,g_r)$ of elements from $I$,
	\begin{enumerate}[topsep=0pt]
		\item The sequence $G$ is a characteristic sequence of $I$.
		\item $f \in I$ if and only if $\red_G(f)=0$. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	We will show that $G$ is not minimal implies there exists some nonzero $f$ reduced with respect to $G$.
	This is the contrapositive of the ``membership characterization property'' implying that $G$ is a characteristic set.
	Suppose that $G=(g_1,\ldots,g_r)$ is an autoreduced subset of $I$ which is not minimal.
	Let $H=(h_1,\ldots,h_s)$ be a characteristic set below $G$. 
	The relationship $H\prec G$ can happen in two ways.
	
	First there exists some $i$ with $1\leq i \leq r$ such that 
	$$\rk(h_1)= \rk(g_1), \quad\rk(h_2)=\rk(g_2), \quad \ldots \quad \rk(h_{i-1})=\rk(g_{i-1}), \quad h_i \prec g_i. $$
	This $h_i$ will be reduced with respect to $G$ as reducedness with respect to $H$ imply this for $g_1,\ldots,g_{i-1}$ and being lower implies this for $g_i,\ldots,g_r$.
	
	The second way if for $\rk(h_j) = \rk(g_j)$ for $1\leq i \leq r$ but $r<s$. 
	In this case $h_{r+1}$ produces our element which is reduced with respect to $G$.
	
	Conversely suppose that $G$ is a characteristic set for some ranking $\prec$ and, for the sake of contradiction. that $f\in I$ is reduced with respect to $G$ and but not zero.
	
	There are two cases.
	In the first case there exists some minimal $j$ with $1\leq j \leq r$ such that $f\prec g_j$.
	In this case we have  
	$$ (g_1,\ldots,g_{j-1},f) \prec G, $$
	which contradicts minimality.
	
	In the second case we have $f \nprec g_j$ for $1\leq j \leq r$. 
	We then construct 
	$$ (g_1,\ldots, g_r,f) \prec G $$
	which again contradicts minimality.
\end{proof}
\fi 


\iffalse 
\begin{theorem}
	Let $I$ be a $\Delta$-ideal in $K[x_1,\ldots,x_n]_{\Delta}$. 
	Fix a ranking on $K[x_1,\ldots,x_n]_{\Delta}$.
	The following are equivalent for an ordered set $G=(g_1,\ldots,g_r)$ of elements from $I$,
	\begin{enumerate}[topsep=0pt]
		\item The sequence $G$ is a characteristic sequence of $I$.
		\item $f \in I$ if and only if $\red_G(f)=0$. 
	\end{enumerate}
\end{theorem}
\begin{proof}
	We will show that $G$ is not minimal implies there exists some nonzero $f$ reduced with respect to $G$.
	This is the contrapositive of the ``membership characterization property'' implying that $G$ is a characteristic set.
	Suppose that $G=(g_1,\ldots,g_r)$ is an autoreduced subset of $I$ which is not minimal.
	Let $H=(h_1,\ldots,h_s)$ be a characteristic set below $G$. 
	The relationship $H\prec G$ can happen in two ways.
	
	First there exists some $i$ with $1\leq i \leq r$ such that 
	$$\rk(h_1)= \rk(g_1), \quad\rk(h_2)=\rk(g_2), \quad \ldots \quad \rk(h_{i-1})=\rk(g_{i-1}), \quad h_i \prec g_i. $$
	This $h_i$ will be reduced with respect to $G$ as reducedness with respect to $H$ imply this for $g_1,\ldots,g_{i-1}$ and being lower implies this for $g_i,\ldots,g_r$.
	
	The second way if for $\rk(h_j) = \rk(g_j)$ for $1\leq i \leq r$ but $r<s$. 
	In this case $h_{r+1}$ produces our element which is reduced with respect to $G$.
	
	Conversely suppose that $G$ is a characteristic set for some ranking $\prec$ and, for the sake of contradiction. that $f\in I$ is reduced with respect to $G$ and but not zero.
	
	There are two cases.
	In the first case there exists some minimal $j$ with $1\leq j \leq r$ such that $f\prec g_j$.
	In this case we have  
	 $$ (g_1,\ldots,g_{j-1},f) \prec G, $$
	which contradicts minimality.
	 
	In the second case we have $f \nprec g_j$ for $1\leq j \leq r$. 
	We then construct 
	 $$ (g_1,\ldots, g_r,f) \prec G $$
	which again contradicts minimality.
\end{proof}
\fi 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prime Decomposition (unstable)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$D$-schemes (unstable) }\ref{S:d-schemes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Poincar\'{e}-Fuchs]{The Poincar\'{e}-Fuchs Theorem (unstable) }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
Which ordinary differential equations of the form
	  $$ P(t,y,y') =0, $$
with $P(u,v,w) \in \CC[x,y,z]$ admit meromorphic solutions $y(t)$ which have the property that $y(t)$ has no-movable singularities? 
\end{problem}
The Poincar\'e-Fuchs theorem states that the only equations of this form are 1) Ricatti equations and 2) Weierstrass equations. 
We are going to give two proofs of this. 
The first is foliation theoretic and the second is differential algebraic. 

A basic reference for the foliation theoretic proof is Pan and Sebastian \cite{Pan2004}.
There is also a great \href{https://www.youtube.com/watch?v=DoCCm8zjvXQ&list=PL0E0n75oNCDk5tuV-t2_K56sEfLd0Od8H&index=21}{YouTube Lecture} by Loray on the topic. 

A basic reference for the differential algebraic proof is Matsuda's book \cite{Matsuda1980} and is based of his paper \cite{Matsuda1978}.
I will follow the treatment from Buium's book \cite{Buium1986} where he further develops this theory in higher dimensions.

Both \cite{Pan2004} and \cite{Matsuda1978} give with the stated goal of making Poincar\'e's original proof rigorous. 
See \cite{Matsuda1978} for details.
%There is a recurring theme of mistakes in differential algebra bring corrected. Painlev\'e Gambier, Poincare Matsuda/Pain, Manin/Coleman/Chai,  


%H. Poincaré, C.R. Acad. Sci. Paris 99 (1885), 75–77; JFM 16.0250.01] and [H. Poincaré, Acta Math. 7 (1885), 1–32; JFM 17.0279.01]



%\section{Differential Equations from the Foliation-Theoretic Perspective}

%\subsection{Exterior Differential Systems}

%\subsection{Comparison to $\Delta$-Schemes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Differential Galois Theory]{Differential Galois Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the 18th and first half of the 19th century mathematicians sought an explicit formula for solutions of the quintic 
$$ ax^5+bx^4+cx^3+dx^2+ex+f=0 $$
in terms of radicals.
Less well known is the fact that they also wanted to determine general formulas for all ordinary  differential equations in terms of some collection of ``elementary functions''.
The most famous problem is the following:
\begin{problem}
	Can the function $\int e^{x^2} dx$ be written in terms of elementary functions?
\end{problem}
To recast this, note that we are seeking some $y(x)$ such that $y'(x) = e^{x^2}$ which is the simplest type of linear differential equation.

In 1833 a manuscript appeared in which Liouville showed that integrals like $\int e^{x^2}dx$ (or equations $y'(x) = e^{x^2}$) can't be solved in terms of elementary functions. 
This is one of the major results that we will present in this chapter.

\begin{remark}
I want to point out that Liouville's Theorem predates Galois Theory as we know it. 
It was in 1843, 10 years later, that Liouville announced the the Academy of Sciences in Paris many of the results that we now collectively know as Galois Theory. The corresponding publication appeared 1846.

%From a cursory reading it is unclear how Liouville aquired these manuscripts. 
%Galois was two years older than Liouville and they both were trying to enter \'Ecole Polytechnique at the same time (Liouville entered at age 16 in 1825 and Galois attempted his exam, but failed, in 1828 at age 17). 
%The manuscripts were perhaps passed on through Poisson, Galois' advisor.   
\end{remark}


Differential Galois Theory starts with the theory of linear differential equations, also called Picard-Vessiot Theory.
Let $(K,\partial)$ be a differential field. 
Given a linear differential equation $Y'=AY$ with $A\in M_n(K)$ one can consider a fundamental matrix $\Phi= (\phi_{ij})$ and consider the differential field extension $F = K(\lbrace \phi_{ij}\rbrace)$. 
It turns out that when $F^{\partial}=K^{\partial}$ there is a Galois group $G(F/K)$ which is a linear algebraic group.
There is a Galois correspondence and importantly 
 $$ \trdeg(F/K) = \dim( G(F/K) ).$$
This is again another example of how finiteness is replaced by finite dimensionality is differential algebra.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Integration in Finite Terms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aim of this section is to prove the following theorem.\footnote{Most of this section follows Rosenlicht's Monthly Article \cite{Rosenlicht1972}. 
We also use Hardy's Book on Integration \cite{Hardy1979}.
The presentation here starts with Liouville, then Ostrowski 1946, then Ritt 1948, then Risch 1969, and finally another paper of Rosenlicht 1968. 
}
\begin{theorem}[No Elementary Integral of $e^{x^2}$]
The is no ``closed form'' for $\int e^{x^2}dx$ in terms of ``elementary function''. 
\end{theorem} 
First observe that there is an expression for $\int e^{x^2}dx$ given by a convergent power series:
$$ \int e^{x^2}dx = \int \sum_{n=0}^{\infty} \frac{x^{2n}}{n!} dx = \sum_{n=0}^{\infty} \frac{x^{2n+1}}{(2n+1)n!} \in \CC\langle x\rangle.$$
This is not what we mean by a ``closed form''. 
By a closed from we will mean an element of a $\partial$-field $L$ which is a $\partial$-algebraic extension of $\CC(x)$. 
In terms of Galois Theory, $L$ plays the same role as the solvable closure of $\CC(x)$ and it is sometimes called ``Liouvillian''. 
This will be defined after we a preliminary discussion.
It will contain things like $\exp(x)$, $\log(x)$, polynomials, algebraic functions, and combinations of these. 

To show $e^{x^2}$ has no elementary antiderivative we will use Liouville's criterion. 
\begin{theorem}[Liouville's Criterion]
	Suppose that $f(x)$ and $g(x)$ are elementary. 
	The integral $\int f(x) e^{g(x)} dx$ has an ``elementary closed form'' if and only if there exist some $\mu(x) \in \CC(x)$ such that 
	 $$ f(x) = \mu'(x) + \mu(x) g'(x).$$
\end{theorem}
\begin{proof}[Proof of the Easy Direction]
One direction is way easier than the other.
Suppose that there exists some $\mu(x) \in \CC(x)$ such that 
 $$ f(x) = \mu'(x)+\mu(x)g'(x).$$
 Then we have 
  $$\dfrac{d}{dx}\left[ \mu(x) e^{g(x)}\right] = \mu'(x) e^{g(x)} + \mu(x) g'(x) e^{g(x)} = f(x)e^{g(x)}. $$
 We conclude by the Fundamental Theorem of Calculus, 
 $$\int f(x)e^{g(x)} dx= \mu(x) e^{g(x)} + c,$$
 where $c$ is some constant. 
\end{proof}

We now apply Liouville's Criterion to conclude that $e^{x^2}$ does not have an elementary derivative.
\begin{example}[Proof That $\int e^{x^2} dx$ Is Not Elementary]
	We will apply Liouville's Criterion. We have 
	 $$ \int e^{x^2} dx = \int f(x) e^{g(x)} dx,$$
	where $f(x) = 1$ and $g(x) = x^2$. 
	Our differential equation for $\mu(x)\in\CC(x)$ becomes 
	 $$ \mu'(x) + 2x \mu(x) =  1.$$
	Suppose that there exists some nontrivial $\mu(x) = p(x)/q(x) \in \CC(x)$ satisfying our equation for the sake of contradiction. 
	We will suppose that $p(x),q(x) \in \CC[x]$ are coprime polynomials with $q(x)\neq 0$. 
	Plugging in we get  
	 $$ \frac{p'(x)q(x) - p(x) q'(x)}{q(x)^2} + 2x \frac{p(x)}{q(x)} = 1,$$
	which is equivalent to 
	\begin{equation}\label{E:poly-id}
 q'(x) p(x) = q(x)( p'(x)-2xp(x)-q(x))
	\end{equation}
	Let $(x-\lambda)$ be an irreducible divisor of $q(x)$ appearing with multiplicity $m$.
	Then $(x-\lambda)^m \vert q'(x)p(x)$. 
	But $\ord_{\lambda}(q') = m-1$ which means $p(\lambda)=0$. \footnote{To see that the multiplicity of a root must go down after taking a derivative of the series expansion around $x=\lambda$,  $q(x)=\sum_{n=m}^d a_n (x-\lambda)^n. $}
	But we supposed that $p(x)$ and $q(x)$ were coprime. 
	This is a contradiction.	 
\end{example}

The general form of the equation \eqref{E:poly-id} takes the form 
\begin{equation}\label{E:poly-eqn}
q'(x) p(x) = q(x)( g'(x)p(x) + p'(x)-f(x)q(x)).
\end{equation}

The next example shows that $\int \dfrac{e^x}{x}dx$ has no elementary closed form. 
This example can be used to derive a number of other examples. 
\begin{example}[Antiderivative of $e^x/x$]
	The integral $\int \dfrac{e^x}{x}dx$ has no elementary closed form.
	We suppose there exists some $\mu(x) = p(x)/q(x)$ with $p,q$ coprime polynomials and $q\neq 0$. 
	To see this we use equation \eqref{E:poly-eqn} to get 
	 $$ xq'(x)p(x) = q(x)( xp(x) + xp'(x) -q(x)).$$
	 
	 If $\lambda\in \CC$ is a nonzero root of $q(x)$ of multiplicity $m$ one finds from $(x-\lambda)^m \mid x q'(x)p(x)$ that $(x-\lambda)\vert p(x)$ which gives a contradiction. 
	 
	 If $q(x) = x^m$ then we have $mx^m p(x) = x^m( xp(x)+xp'(x)-x^m)$ which implies $mp(x) = x( p(x) + p'(x) - x^{m-1})$ which proves $x \vert p(x)$ which again shows that $p(x)$ and $q(x)$ are not coprime.
	 
	 We conclude that no such $\mu(x) = p(x)/q(x)$ exists and hence the integral is not elementary. 
\end{example}

The next example shows how you can get more examples from $u$-substitution.
\begin{example}[Logarithmic Integrals]\label{E:log-recip}
	The integral $\int \dfrac{dx}{\ln(x)}$ has no elementary closed form.
	We use just some $u = \ln(x)$ substitution to get 
	$$\int \dfrac{e^x}{x} dx = \int \frac{du}{\ln(u)}.$$
	The previous example shows $e^x/x$ has no elementary anti-derivative. 
\end{example}
The next example shows how you can get more examples from integration by parts. 
\begin{example}[Log Log]
	The integral $\int \ln(\ln(x)) dx$ has no elementary closed form.
	We do an integration by parts with $u = \ln(\ln(x))$ and $dv=dx$. 
	This gives 
	$$\int \ln\ln(x) dx = \frac{1}{\ln(x)} + \int \dfrac{dx}{\ln(x)},$$
	which by Example~\ref{E:log-recip} has no elementary closed form.
\end{example}

\begin{exercise}
	Show that $\sin(x)/x$ has no elementary anti-derivative.
\end{exercise}

More interesting explicit examples can be found in Hardy's book \cite[pg 52]{Hardy1971}.

\subsection{Elementary Functions}

We are going to define what elementary functions are in this subsection. 
Before doing that observe that the two following elementary identities:
 $$ \varphi(x) = \exp( \int f(x) dx) \implies \dfrac{\varphi'(x)}{\varphi(x)} = f(x), $$
 $$ \varphi(x) = \log(f(x)) \implies \varphi'(x) = \dfrac{f'(x)}{f(x)}.$$

We now deal with these abstractly. 
Let $K$ be a $\partial$-field and let $f\in K$.
Let $\widehat{K}$ be a saturated $\partial$-field containing $K$.
\begin{itemize}
	\item We let $\int f$ to denote some $\varphi \in \widehat{K}$ such that $\varphi' = f$. 
	We call such an element an \emph{antiderivative} or \emph{integral} of $f$. 
	\item We let $e^f$ denote some $\varphi \in \widehat{K}$ such that $\varphi' = f'\varphi$. 
	We call such an element an \emph{exponential} of $f$.
	\item We let $\log(f)$ denote some $\varphi \in \widehat{K}$ such that $\varphi'= f'/f$. 
	We call such an element a \emph{logarithm} of $f$. 
\end{itemize}

In a manner similar to usual field theory for ``solving by radicals'' we introduce the notion of a basic extension then define a class of fields from these basic extensions inductively.
\begin{definition}
	Let $K$ be a $\partial$-field. 
	A \emph{basic elementary extension} (resp. \emph{basic Liouville extension}) $F \supset K$ is some $\partial$-field of the form $F=K(\varphi)$ where one of the following holds. 
	\begin{enumerate}[topsep=0pt]
		\item $\varphi$ is algebraic over $K$.
		\item $\varphi = \log(\psi)$ for some $\psi \in K$ (resp. $\varphi = \int \psi$).
		\item $\varphi=\exp(\psi)$ for some $\psi \in K$. 
	\end{enumerate}
\end{definition}
So the only difference between a basic Liouvillian extension and a basic elementary extension is that we replace logarithms with integrals. 
It will turn out these definitions are equivalent but we include both here because the style changes from reference to reference. 

We say that an element $\varphi$ is \emph{basic Liouvillian} (resp. \emph{basic elementary}) if and only if $\varphi$ is an element of a basic Liouvilliean extension (resp basic elementary extension).

\begin{example}
	Let $K= \CC(t)$. 
	Then $\sin(t)$ is basic elementary and basic Liouvillian since $\sin(t) \in K(e^{it})$.
\end{example}

\begin{example}
	Let $K = \CC(t)$. 
	The element $\varphi = \exp(t)$ is basic elementary as $\varphi'=f'\varphi$ with $f=t \in K$. 
\end{example}

An important philosophical point here is that these differential equations are really encoding composition of functions. 
Meditate on that. 

\begin{definition}
	A finitely generated $\partial$-field extension $F \supset K$ is \emph{elementary} (resp \emph{Liouvillian}) if and only if it is obtained by a finite sequence of basic elementary (resp basic Liouvillian) extensions.
	
	A $\partial$-field extension $F \supset K$ (which is not necessarily finitely generated) is called \emph{elementary} (resp. \emph{Liouvillian}) if an only if for all $\varphi \in F$ there exists some intermediate field $\widetilde{F}$ with $K \subset \widetilde{F} \subset F$ which contains $\varphi$ and is a finitely generated elementary (resp. Liouvillian) field extension. 
\end{definition}
For inductive arguments it is helpful to observe that elementary every finitely generated $\partial$-field extension can be written as 
 $$ F = K(\varphi_1,\ldots,\varphi_n) $$
for some $\varphi_1,\ldots,\varphi_n$ where if we let $F_0 = K$ and $F_i = K(\varphi_1,\ldots,\varphi_i)$ where each $\varphi_i$ satisfies on of the following
\begin{enumerate}
	\item $\varphi_i$ is algebraic over $K_{i-1}$.
	\item $\varphi_i$ is a logarithm of some element of $K_{i-1}$.
	\item $\varphi_i$ is some exponential of some element of $K_{i-1}$.
\end{enumerate} 
The same can be said about finitely generated Liouvillian extensions where we replace logarithms with integrals in the above definition.

\begin{lemma}
	Elementary extensions and Liouvillian extensions are the same thing. 
\end{lemma}
\begin{proof}
	We will show that every basic Liouvillian extension is elementary.
	We only need to show this for integrals. 
	Let $f \in K$ and suppose $\varphi = \int f$.
	Then $\int f = \log( e^{\int f})$ and logarithms and exponentials are basic elementary. 
	
	Conversely, we need to show that every logarithm is elementary. 
	Let $f\in K$ be nonzero and suppose that $\varphi =\log(f)$. 
	We use that $\log(f) = \int \frac{f'}{f}$.
	Note that $f'/f \in K$ so we are just taking an integral of a basic element. 
	This proves $\log(f)$ is Liouvillian.
\end{proof}

%%%%%%%%%%%%%%%%
\subsection{Proof of Liouville's Criterion}
%%%%%%%%%%%%%%%%

The following theorem is a sort of converse to the definition of elementary extensions. 
It says that any elementary integrand is a linear combination over the constants of derivatives and logarithmic derivatives. 
\begin{theorem}[Liouville's Theorem]
Let $K$ be a differential field and let $F \supset K$ be an elementary extension with $F^{\partial} = K^{\partial}$. 
Then $\int \varphi \in F$ if and only if there exists some $u_0,\ldots,u_n \in K$ and some $c_i \in K^{\partial}$ such that  
 $$ \varphi = u_0' + \sum_{i=1}^n c_i \dfrac{u_i'}{u_i}.$$ 
\end{theorem}
\begin{proof}
	See \cite[pg 169]{Risch1969}. 
	See also \cite[pg 59]{Hardy1971} for a ``proof by example''.
\end{proof}

\begin{remark}
If $F \supset K$ is $\partial$-algebraic then $F^{\partial}\supset K^{\partial}$ is algebraic. 
This means that if $K^{\partial}=\CC$ then the condition on the constants is vacuous.

This assumption is necessary since $\int \frac{dx}{1+x^2} = \tan^{-1}(x)$ is elementary but one needs to use elements of $\CC$ to express this in the appropriate form.
\end{remark} 

We will also need the following Lemma on transcendence of $e^g$.
\begin{lemma}
	If $g \in \CC(x)$ is non-constant then $e^g$ is transcendental over $\CC(x)$. 
\end{lemma}
\begin{proof}[Analytic Proof]
	First since $g(z) \in \CC(z)$ is non-constant then $\exp(g(z))$ has an essential singularity.
	But algebraic functions don't have essential singularities. 
\end{proof}

\begin{proof}[Algebraic Proof]
	Work over a general differential field $K$.
	Let $\tau = e^g \in F\setminus K$ where $g \in K$ nonconstant.  
	
	Suppose that $\tau$ is algebraic over $K$ and let $f(T) \in K[T]$ be its minimal polynomial. 
	We may write  
	$$f(T) = a_0 + a_1 T + \cdots + T^d$$ for $a_i \in K$.
	Using that 
	$$\partial(a_n \tau^n) = n \tau^{n-1} g \tau = a_n' \tau^n + n g a_n \tau^n = (a_n' + n a_n g) \tau^n$$ 
	we see that the identity
	 $0=\partial(f(\tau))$ gives rise to another monic degree $d$ polynomial on which $\tau$ vanishes. 
	 Let's call this polynomial $F(T)\in K[T]$. 
	 It has the form
	  $$ F(T) = ng' T^n + (a_{n-1}' + (n-1) a_{n-1}g')T^{n-1} + \cdots + a_0 $$ 
	 We have $f(T) \vert F(T)$ (both of degree $d$) which implies that 
	  $$ a_{n-1}  = \frac{a_{n-1}' + (n-1) a_{n-1}g'}{n g'},$$
	 or that $a_{n-1}'/a_{n-1} = g'$ or that $a_{n-1} = e^g$.
	 This is a coefficient, so this is in $K$. 
	 One the other hand, by hypothesis, this is in $F\setminus K$. 
	 This gives a contradiction.
\end{proof}

\subsection{Proof of Liouville's Criterion}

Let $K_0 =\CC(x)$ and $C = K^{\partial}$.
We will show that $\int f e^g$ is elementary if and only if there exist some $\mu\in K_0$ satisfying the differential equation $\mu'+g'\mu = f$. 
Let $\tau = e^g$ and work in $K = K_0(\tau)$.
By Liouville's theorem there exists constants $c_i \in C$ and functions $u_i \in K$ such that 
 $$ f\tau = u_0' + \sum_{i=1}^n c_i \dfrac{u_i'}{u_i}$$
By the homomorphic properties of logarithmic derivatives we can assume that $u_j \in K_0[\tau]$ for $j\geq 1$ are monic irreducible polynomials.

Without loss of generality we can assume that $\sum_{i=1}^n c_i u_i'/u_i \in K_0$. 

We can expand $v \in K_0(\tau)$ as a Laurent series to get as an expresions in $K_0((\tau))$ a sum of linearly independent $K_0$-vectors
 $$ u_0 = L + b_1 \tau + H$$
Taking derivatives we get 
 $$ u_0' = L' + b_1'\tau + b_1 \tau' + H'= L'+H' + (b_1' +b_1 g') \tau,$$
where in the last equality we used that $\tau' = g' \tau$.

Linear independence then gives $f\tau = (b_1' + b_1 g')\tau$.


\subsection{Algebraic Functions Satisfy Linear Differential Equations}
In what follows if $f(x) = \sum_{i=0}^n a_i x^i \in K[x]$ where $K$ is a $\partial$-field we will write 
$$ f^{\partial}(x) = \sum_{i=0}^n \partial(a_i) x^i.$$
These polynomials come up in terms when differentiating polynomials with coefficients in a differential field via the product rule. 
\begin{lemma}
	Let $F \supset K$ be an extension of $\partial$-fields.
	If $\alpha \in F$ is algebraic over $K$ then it satisfies a linear differential equations over $K$.
\end{lemma}
\begin{proof}
	Let $\alpha \in F$ be any element which is algebraic over $K$. 
	Let $f(x) \in K[x]$ be the minimal polynomial of $\alpha$. 
	Then we have $f(\alpha)=0$ and $\partial(f(\alpha)) = f^{\partial}(\alpha) + f'(\alpha) \partial(\alpha) =0$. 
	We know that $f'(\alpha)\neq 0$ by minimality. 
	This implies that 
	$$\partial(\alpha) = -\frac{f^{\partial}(\alpha)}{f'(\alpha)} \in K(\alpha).$$
	We can now iterate the above construction an get $\alpha, \alpha', \alpha'', \ldots$ all as elements of $K(\alpha)$. 
	Since $K(\alpha)$ have finite dimension as a $K$-vector space there exists some $a_j \in K$ and some $r$ such that 
	$$ a_0 \alpha + a_1 \alpha' + \cdots + a_r \alpha^{(r)} =0,$$
	which proves that $\alpha$ satisfies a linear differential equation over $K$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\taylor{Explain how Hopf algebras are groups with the axioms written out backwards}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Poincar\'e Problem]{The Poincar\'e Problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
	Consider differential equations of the form
	 $$ \dfrac{dy}{dx} = \frac{a(x,y)}{b(x,y)}, \quad a(x,y),b(x,y) \in \CC[x,y].$$
	 For which $a(x,y),b(x,y) \in \CC[x,y]$ does the equation admit algebraic solutions?
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Kolchin Irreducibility]{The Kolchin Irreducibility Theorem}\label{S:kolchin-irreducibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Dimension Theory]{Dimension Theory}\label{S:dimension-theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
	Let $(K,\Delta)$ be a $\Delta$-field. 
	Let $u_1,\ldots,u_n \in K\lbrace x_1,\ldots,x_n\rbrace_{\Delta}$ and consider the system of PDEs
	 $$ u_1=u_2=\cdots=u_n=0. $$
	How many constants of integration does a general solution of this equation need?
\end{problem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Analytic Aspects]{Analytic Aspects of Differential Equations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solutions of Homogeneous Differential Equations}
The following is a useful formula.
\begin{theorem}[Solutions of Homogeneous Differential Equations]\label{T:solutions}
	If $Y'= A(t)Y$ is an ordinary differential equation then 
	 $$ \Phi(t) = \exp( \int_{t_0}^t A(s)^T ds )^T $$
	provides a local fundamental matrix. 
\end{theorem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cauchy-Kovalevskya (Cauchy-Kowalevski)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter[Spaces]{Manifolds}
In this section we collect the results we need regarding smooth manifolds, schemes, and algebraic varieties.
\section{Ehresmann's Theorem}
The following Theorem will be used when discussing local systems. 
A consequence of this is that all elliptic curves are drawn topologically as donuts. 
In fact, all smooth complex algebraic varieties in families when viewed as smooth manifolds are topologically the same. 
They have the same hodge and betti numbers, fundamental groups, etc. 
To establish this we use Ehresmann's Theorem.
\begin{theorem}[Ehresmann's Theorem]
	Any proper submersions $f:X\to S$ of smooth manifolds is locally (diffeomorphically) trivial. 
\end{theorem}
We will remind the reader of the terms.
\begin{itemize}
	\item A morphism $f:X\to S$ is \emph{proper} if and only if the inverse image of a compact set is compact.\footnote{for those familiar with algebraic geometry, you know that you can rephrase this in terms of universal closedness and separatedness which can be formulated diagramatically.}
	\item A morphism $f:X\to S$ is a \emph{submersion} if and only if for every $x\in X$ the induced map $(TX)_x \to (TS)_{f(x)}$ is surjective. 
	\item A morphism $f:X\to S$ is locally trivial if and only if for every $s_0 \in S$ there exists a $U \owns s_0$ open and an diffeomorphism $\psi$ fitting into the diagram
	$$\begin{tikzcd}
	f^{-1}(U) \ar[rr, "\psi"]\ar[dr, "f"]& & U \times X_{s_0} \ar[dl, "\pr_1"] \\
	& U & 
	\end{tikzcd}.$$	
	In the above diagram $X_{s_0} = f^{-1}(s_0)$ is the fiber above $s_0$.
\end{itemize}

\begin{proof}[Sketch Proof]
	A doodle of the proof is found in Figure~\ref{F:ehresmann}.
	Without loss of generality we can assume that $U = \RR^n$ with coordinates $y^i$ for $1\leq i \leq n$. 
	\begin{itemize}
		\item We use the submersion property to show that there exists vector fields $w_i$ on $X$ lifting the vector fields $v_i =\frac{\partial}{\partial y^i}$.
		If $\phi_{v_i}^t$ and $\phi_{w_i}^t$ denote the flows (partially defined self maps $Y\to Y$ and $X\to X$ respectively) given by solving the flow ODE with the vector fields $v_i$ and $w_i$ respectively satisfying 
		$$ \phi_{v_i}^t \circ f = f \circ \phi_{w_i}^t $$
		for some time $t$. 
		\item We use the properness property to show that for each $s\in U$, the set 
		$$f^{-1}\lbrace \phi^t_{v_i}(s) \colon t\in [a,b] \rbrace$$ is a compact set for $[a,b]$ where the map is defined.
		This allows us to argue that the flows $\phi^t_{w_i}$ need to be defined for just as many $t\in [a,b]$ flows $\phi^t_{v_i}$ are. 
		\item We then define $\psi\colon f^{-1}(U) \to U\times X_{s_0}$ using two maps. 
		The first is projection to $U$, and the second is flowing to $X_{s_0}$ using the vector fields. 
	\end{itemize}
	
	\begin{figure}
		\begin{center}
			\includegraphics[scale=0.5]{ehresmann.eps}
		\end{center}
		\caption{A picture of the proof of Ehresmann's Theorem. 
			The first frame shows us lifting the vector field. 
			The second frame shows the flow-lines for the vector field. 
			The third frame shows the morphism ``go to the fiber'' using the flow lines.}\label{F:ehresmann}
	\end{figure}
\end{proof}


Some consequences of this are that the Betti and Hodge numbers of varieties in proper families are preserved.

Another consequence in that proper submersions $f:W \to S$ are actually fiber bundles with a common fiber $F$ over an open subset of $S$ where the fibers are smooth manifolds. 
\begin{corollary}[Lefschetz Fibration]
	Let $\pi:W\to S$ be a proper submersion of connected smooth manifolds. 
	Over a dense open subset of $S$ the map $\pi$ is a fiber bundle in the category of smooth manifolds with fiber $W_{s_0}$. 
\end{corollary}
\begin{proof}
	The open subset of $S$ is the set where $W_{s}$ is a smooth manifold. 
	We just need to show that the fibers are all diffeomorphic. 
	If $U$ and $V$ are two trivializing opens with $\psi_U$ and $\psi_V$ trivializations to $U\times W_{s_0}$ and $V \times W_{s_1}$ then we have isomorphisms over $U\cap V$ (obtained from base changing our trivializations via $U\cap V \to U$ and $U\cap V \to V$) 
	fitting into the diagram
	$$\begin{tikzcd}
	(U\cap V) \times W_{s_0} \ar[dr,"\pr_1"]&\pi^{-1}(U\cap V)\ar[d] \ar[r, "\psi_V"]  \ar[l, "\psi_U"]  & (U\cap V)\times W_{s_1} \ar[dl,"\pr_1"]\\
	& U \cap V &
	\end{tikzcd}
	$$
	Now the map $\psi_{U,V} = \psi_V\psi_U^{-1}:( U\cap V)\times W_{s_0} \to (U\cap V)\times W_{s_1}$ over $U\cap V$ which we can base change by any point. 
	This shows that $W_{s_0} \cong W_{s_1}$. 
	We then cover $S$ by trivializing open sets and use the transitive property of diffeomorphism to get a common fiber. 
\end{proof}

\taylor{Check this smooth fiber condition}



\backmatter



\bibliographystyle{amsalpha}
\bibliography{diff-alg.bib}

\end{document}
