\documentclass[12pt]{book}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{hyperref}

\usepackage{color}
\newcommand{\taylor}[1]{{\color{blue} \sf $\spadesuit\spadesuit\spadesuit$ Taylor: [#1]}}
\newcommand{\todo}[1]{{\color{purple} \sf $\spadesuit\spadesuit\spadesuit$ TODO: [#1]}}

\usepackage{tikz-cd}
\usepackage{graphicx}
\usepackage{soul}

\numberwithin{equation}{section}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{question}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{warning}[theorem]{Warning}


\newcommand{\trdeg}{\operatorname{trdeg}}

\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}

\renewcommand{\AA}{\mathbb{A}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\Ocal}{\mathcal{O}}

%\newcommand{\sec}{\operatorname{sec}}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\Hom}{\operatorname{Hom}}

\newcommand{\LM}{\operatorname{LM}}
\newcommand{\LT}{\operatorname{LT}}
\newcommand{\LC}{\operatorname{LC}}
\newcommand{\Low}{\operatorname{Low}}

\newcommand{\wt}{\operatorname{wt}}
\newcommand{\hol}{\operatorname{Hol}}
\newcommand{\Mer}{\operatorname{Mer}}
\newcommand{\adj}{\operatorname{adj}}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\llangle}{\langle \langle}
\newcommand{\rrangle}{\rangle \rangle}

\newcommand{\mult}{\operatorname{mult}}
\newcommand{\Res}{\operatorname{Res}}
\newcommand{\res}{\operatorname{res}}

\newcommand{\ord}{\operatorname{ord}}


%\newcommand{\hol}{\operatorname{hol}}
\newcommand{\LocSys}{\operatorname{LocSys}}
\newcommand{\Mod}{\mathsf{Mod}}
\newcommand{\Pic}{\operatorname{Pic}}

\newcommand{\fin}{\operatorname{fin}}

\newcommand{\tr}{\operatorname{tr}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\Mon}{\operatorname{Mon}}
\newcommand{\Sys}{\operatorname{Sys}}
\newcommand{\Conn}{\operatorname{Conn}}
\newcommand{\Repn}{\operatorname{Repn}}
\newcommand{\End}{\operatorname{End}}

\newcommand{\QQbar}{\overline{\mathbb{Q}}}
\newcommand{\Cont}{\operatorname{Cont}}
\newcommand{\Lie}{\operatorname{Lie}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\can}{\operatorname{can}}




\usepackage{tabto}
\def\quoteattr#1#2{\setbox0=\hbox{#2}#1\tabto{\dimexpr\linewidth-\wd0}\box0}
%\parskip 1em

\setlength\parskip{1em plus 0.1em minus 0.2em}
\setlength\parindent{0pt}

%opening
\title{An Introduction to the Algebraic Theory of Differential Equations}
\author{Taylor Dupuy }


\begin{document}

\maketitle

\frontmatter

\tableofcontents

\chapter{FrontMatter}

\section{Why do these notes exist?}
These notes are from a course taught in Fall 2022 at UVM entitled \emph{The Algebraic Theory of Differential Equations}. 
I decided to write these notes because there are a lot different sources for graduate students starting out in Differential Algebra but none of them cover exactly what I would like to cover. 

So what is the Algebraic Theory of Differential Equations? 
For me the starting place are Ritt's books on Differential Algebra. 
I love Ritt's books \cite{Ritt1932} and \cite{Ritt1950}. 
The issue is that they are a bit out of date and have a lot of dependencies within chapters that are not clearly marked so you almost have the read the books linearly. 
They are also missing a lot of the classical theory from the 1800's which motivated the subject. 
The successor to Ritt's books is Kolchin's books \cite{Kolchin1973} which, while mathematically very useful, invokes notation and terminology that gives me nightmares. 
Also, the algebraic geometry there largely ignores the development of scheme theory between from 1950 to 1970 by the French school. 
An alternative to these two is Kaplansky's book \cite{Kaplansky1976} which I love but is perhaps too brief. 
Following the spirit of these Differential Algebra Books are Buium's books \cite{Buium1986} (influenced by Matsuda's book \cite{Matsuda1980}),\cite{Buium1992}, and \cite{Buium1994} which are probably the most influential on my perspective. 
Again, to really understand those books (for example the Poincar\'e-Fuchs theorem on equations of the form $P(t,y,y')=0$ for a polynomial $P$ whose solutions have no movable singularities) one needs to go back to some of the older material which is best covered elsewhere.
They are about differential field theory, differential algebraic groups, and applications of differential algebra to diophantine geometry.

Also, in the spirit of these Differential Algebra books are the are the importants books on Picard-Vessiot Theory by Singer and van der Put \cite{Put2003} and Magid \cite{Magid1994}.
These books are about the Galois theory for linear differential equations.

To cover this perspective one would like to talk about hypergeometric functions, the Painlev\'{e} equations, and monodromy more generally. 
There is the classic book by Ince \cite{Ince1944} and a standard text \cite{Iwasaki1991} which is nice but focuses a lot of computations and de-emphasizes global geometry.
There are great discussion of Hilbert's 21st problem in \cite{Borel1987} and a more modern algebro geometric version in \cite{Deligne1970}.
Marrying this material with the field theoretic methods of \cite{Buium1986} is something that I want to do. 

Once one gets into the Painlev\'{e} equations more algebraic geometry surfaces. The japanese following  Okomoto \cite{Okamoto1987b,Okamoto1987,Okamoto1986, Okamoto1987a} (and many many papers which I'm not going to list following this thread) showed that there exist rational surfaces of ``spaces of initial conditions'' for the Painlev\'{e} which capture a lot of geometry. 

Also, there are so-called Lax Pairs for these Painlev\'{e} equations which leads to a theory of ``algebraic complete integrability''. 
The notion of algebraic complete integrability is discussed in, say, \cite{Beauville1990}\cite{Adler2004}. 
From here one can see that equations like the KP equation admit Lax Pairs and this theory again makes connections to algebraic geometry (this time abelian varieties) through so-called Jacobian Flows and Krichever modules \cite{Mulase1994}.

On top of all this there is a general differential Galois Theory beyond linear equations  developed by Umemura \cite{Umemura2011} and a general theory of Riemann-Hilbert  Problems and D-Modules following Malgrange and Kashiwara \cite{Borel1987}.

I haven't even mentioned differential algebraic geometry (it's associated tussles with dimension theory) and the geometry of foliations. To make things worse, much of this material generalizes beyond differential equations, to difference equations, $p$-derivations, and other operations.

Understandably, I can't cover this all. 
I'm not even going to pretend to try. 
My goal is to survey material.
Because of this, I'm going to need to assume some mathematics at times --- there already exists excellent references for much of the material we need to source.  
This will at times include basic Differential \cite{Ince1944} and Partial Differential Equations \cite{Evans2010}, Commutative Algebra \cite{Atiyah2016}, Galois Theory \cite{Cox2012}, Complex Analysis \cite{Ullrich2008}, Algebraic Topology \cite{Hatcher2002}, Manifolds \cite{Lee2013}, and Algebraic Geometry \cite{Vakil2017}. 
At the same time, I'm not crazy. 
I don't want to be writing to nobody. 
Things that I feel are part of a good introduction for well-prepared graduate students I will review. 

In addition to helping graduate students, I want to help  myself.
I have a number of things I would like to understand better. What is a $\tau$-function? What is a space of initial conditions? 
What is a Jacobian flow? What proofs work for differential equations but not for difference equations? 
What do we \emph{really} mean when we say $X$ equation is a limiting case of $Y$ equation?
What are the most fundamental examples to keep in mind and teach students when talking about this material?
How are classical asymptotic methods ``enriched by $D$-modules and sheaves''?

The subject is vast and I hope we have a fun time exploring it. 
It may be that I don't get anywhere on any of this material and we spend 3 months defining what a differential ideal is. 
We'll see. 
%At the end of all of this there is going to be many course that could be taught using this book.

\iffalse
\section{A Note After Fall 2022} 
There is a manuscript by Manin \cite{Manin1978} which also attempts to unify differential algebraic and foliation theoretic perspectives on differential equations.

\fi


\section{Where can I get a digital copy of these notes?}
A link to the .tex can be found here:
\begin{center}
	\url{https://tdupu.github.io/diff-alg-public/diff-alg.pdf}.
\end{center}

\section{How do I cite these notes?}

\begin{verbatim}
@Unpublished{Dupuy2022,
author  = {Dupuy, Taylor},
title   = {An Introduction to the Algebraic Theory of Differential Equations},
year    = {2022},
comment = {Course notes from lectures given at the University of Vermont in Fall 2022.},
journal = {preprint},
url     = {https://tdupu.github.io/diff-alg-public/diff-alg.pdf},
}

\end{verbatim}

\newpage 
\section{Notation}
\begin{itemize}
	\item $R^{\Delta}$ (or $R^{\partial}$) the constants of the derivations (or derivation). 
	\item $R\lbrace x \rbrace$ the ring of differential polynomials over $R$.
	\item $K(S)_{\partial} = K(\lbrace S \rbrace)$ the field extension of $K$ $\partial$-generated by $S$.
	\item $\CC\langle t-t_0 \rangle$ convergent power series at $t_0$
	\item $\CC\llangle t-t_0\rrangle$ Laurent series of meromorphic functions (so finite poles).
	\item $R[\Delta]$ the Weyl Algebra associated to a partial differential ring.
\end{itemize}
\newpage

\mainmatter



\chapter{Differential Algebra Basics}
I would skip this for now and only come back to this chapter when we need it. 

\section{The Basic Objects}

\subsection{$\Delta$-Rings and $\partial$-Rings}
In this book, unless stated otherwise, all rings are going to be commutative with a multiplicative unit. 
Let $R$ be a commutative ring. 
By a \emph{derivation} on $R$ we map a map of sets $\partial:R\to R$ that satisfied 
 $$ \partial(a+b) = \partial(a) + \partial(b), \qquad \forall a,b\in R,$$
 $$ \partial(ab) = \partial(a) b + a\partial(b), \qquad \forall a, b \in R, $$
 $$ \partial(1) = \partial(0) = 0.$$
Derivations are completely formal here. 
We don't care about limits. 

\begin{exercise}
	Check that all the usual rules hold. For example if $\partial:R \to R$ is a differential ring then 
	\begin{enumerate}
		\item For $n\in \ZZ_{\geq 0}$ and $a \in R$ we have $\partial(a^n) = na^{n-1}\partial(a)$.
		\item For $a \in R$ and $b\in R^{\times}$ we have $\partial(a/b) = (\partial(a)b - a \partial(b))/b^2$. Here $R^{\times}$ denotes the elements which have a multiplicative inverse.
		\item For $f \in R[x]$ and $a \in R$ we have $\partial(f(a)) = f^{\partial}(a) + f'(a)\partial(a)$. If $f(x) = \sum_{i=0}^d b_i x^i$ then $f^{\partial}(x) = \sum_{i=0}^d \partial(b_i) x^i$. 
	\end{enumerate}
\end{exercise}
Note that the one exception for derivative rules holding is the chain rule. 
For an abstract ring $R$ there is not a defined composition of elements $a\circ b$ (although you can compose with polynomials as above).


\begin{definition}
	A \emph{differential ring} or (\emph{$\Delta$-ring}) is a tuple $(R,\Delta)$ where $R$ is a commutative rings with unity and $\Delta = \lbrace \partial_1,\ldots,\partial_m\rbrace$ is a collection of commuting derivations $\partial_i:R \to R$. 
\end{definition}

When $\Delta = \lbrace \partial \rbrace$ then we call $(R,\Delta)$ a \emph{$\partial$-ring} and will use the notation $(R,\partial)$. 
We also call such a ring an ordinary differential ring. 


\begin{example}
	\begin{enumerate}
		\item The ring of polynomials in on variable $(\CC[t],\dfrac{d}{dt})$ 
		\item The ring of rational functions $(\CC(t), \dfrac{d}{dt})$, this is an example of a differential field. 
		In general a \emph{differential field} is a differential ring $(K,\Delta)$ where the underlying ring $K$ is a field. 
		\item The ring of holomorphic functions $\hol(U)$ for some $U\subset \CC^m$ is an example of a $\Delta$-ring, $(\hol(U), \lbrace \dfrac{\partial}{\partial t_1}, \ldots, \dfrac{\partial}{\partial t_m}\rbrace )$. 
		Here we are using $(t_1,\ldots,t_m)$ for the complex variables $t_j = \sigma_j + i \tau_j$ where $\sigma_j,\tau_j \in \RR$. 
		\item We can do the same thing with meromorphic functions $\Mer(U)$. These will give a differential field. 
	\end{enumerate}
\end{example}

\subsection{Morphisms of $\Delta$-Rings and $\partial$-Rings}
Let $(A,\Delta)$ and $(B,\Delta)$ be differential rings where we use $\Delta = \lbrace \partial_1,\ldots,\partial_m\rbrace$ for the derivatives on both $A$ and $B$.
\begin{definition}
A \emph{morphism} of differential rings is a ring homomorphism $f:A\to B$ such that for each $\partial_i \in \Delta$ we have $f(\partial_i(a)) = \partial_i(f(a))$ for each $a\in A$. 
\end{definition}

\chapter{Monodromy and Hilbert's 21st Problem }

\quoteattr{In the theory of linear differential equations with one independent variable $z$, I wish to indicate an important problem one which very likely Riemann himself may have had in mind. 
	This problem is as follows: To show that there always exists a linear differential equation of the Fuchsian class, with given singular points and monodromic group. 
	The problem requires the production of $n$ functions of the variable $z$, regular\footnote{Hilbert means holomorphic.} throughout the complex $z$-plane except at the given singular points; at these points the functions may become infinite of only finite order, and when $z$ describes circuits about these points the functions shall undergo the prescribed linear substitutions. 
	The existence of such differential equations has been shown to be probable by counting the constants, but the rigorous proof has been obtained up to this time only in the particular case where the fundamental equations of the given substitutions have roots all of absolute magnitude unity. L. Schlesinger (1895) has given this proof, based upon Poincar√©'s theory of the Fuchsian zeta-functions. 
	The theory of linear differential equations would evidently have a more finished appearance if the problem here sketched could be disposed of by some perfectly general method.}{Hilbert's 21st Problem}

In this chapter we are going to move towards Hilbert's 21st problem and some of the classical theory of monodromy of solutions of differential equations.


\section{The Monodromy Representation}
In this section we develop some basic tools we need to construct a monodromy representation associated to a linear differential equation on $\PP^1$.

\subsection{Wronskians}

Let $(R,\partial)$ be a differential ring. 
Let $f_1,\ldots,f_n\in R$. 
The \emph{Wronskian} of $f_1,\ldots,f_n$ is 
 $$ W(f_1,\ldots,f_n) = \det \begin{pmatrix}
 f_1 & f_2 & \cdots & f_n \\
 f_1' & f_2' & \cdots & f_n' \\
 \vdots & \vdots & \ddots & \vdots \\
 f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}
 \end{pmatrix}.$$
The Wronskian gives us a test for linear dependence over the constants of a differential field. 
\begin{theorem}
	Let $(K,\partial)$ be a differential field. 
	Let $f_1,\ldots,f_n \in K$. 
	Let $C = K^{\partial}$ be the constants. 
	We have that $f_1,\ldots,f_n$ are linearly dependent over $C$ if and only if $W(f_1,\ldots,f_n)=0$. 
\end{theorem}
\begin{proof}
	Suppose that $f_1,\ldots,f_n$ are linearly dependent over $C$.
	Then there exists $c_1,\ldots,c_n \in C$ not all zero such that 
	 $$ c_1 f_1 + \cdots + c_n f_n =0.$$
	 Taking derivatives gives 
	  \begin{equation} \label{E:element-of-kernel}
	  	\begin{pmatrix}
	  f_1 & f_2 & \cdots & f_n \\
	  f_1' & f_2' & \cdots & f_n' \\
	  \vdots & \vdots & \ddots & \vdots \\
	  f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}
	  \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\  \vdots  \\ c_n \end{pmatrix} =0.
  \end{equation}
	 Since the $c_i \in K$, this means the matrix $B$ such that $\det(B)=W$ is singular and hence $W=\det(B)=0$.
	 
	 Conversely, suppose that $W=0$. 
	 Then there exists some $c_1,\ldots,c_n \in K$ not all zero such that \eqref{E:element-of-kernel} holds. 
	 To prove our result, we need to show that $c_1,\ldots, c_n \in C$. 
	 If some proper subset of $\lbrace f_1,\ldots, f_n \rbrace$ have a non-trivial dependence relation we can replace our set with that subset and hence we can assume without loss of generality that $\lbrace f_2,f_3,\ldots, f_n \rbrace$ are linearly independent over $K$. 
	 We can also suppose that $c_1\neq 0$. 
	 Furthermore we can scale the vector $(c_1,\ldots,c_n)$ by $1/c_1$.
	 Hence we can further assume that $c_1=1$.  
	 
	 Now for $1\leq j \leq n-2$ (note the $n-2$ here) we can take a derivative of 
	  $$ c_1 f_1^{(j)} + \cdots + c_n f_n^{(j)} =0 $$
	 To get 
	 \begin{align*}
	 		   0 =& c_1 f_1^{(j+1)} + \cdots + c_n f_n^{(j+1)} + c_1'  f _1^{(j)} + \cdots + c_n' f_n^{(j)} \\
	 		   =&  c_2'  f _2^{(j)} + \cdots + c_n' f_n^{(j)}.
	\end{align*}
	But $f_2,\ldots,f_n$ are linearly independent. 
	This implies that $c_2'=\cdots=c_n'=0$ which implies that $c_1,\ldots,c_n \in C$ which proves our result. 
\end{proof}

We want to show that the Wronskian satisfies a linear differential equations. 
To do this we need a couple things. 

In what follows one needs to recall the definition of an adjugate matrix and how cofactor expansion works. 
Recall that if $A$ is an invertible $n\times n$ matrix then the \emph{adjugate} is defined by 
$$\adj(A) = \det(A) A^{-1}.$$
This is the best way to remember the formula.  
The adjugate is just what would be the inverse would be had we not inverted the determinant. 
Unlike inverse, tt turns out that every $n\times n$ matrix and we can obtain its formula from cofactor expansion. 
We have 
 $$ \adj(A)_{ji} = (-1)^{i+j} \det(\widetilde{A}_{ij})$$
where $\widetilde{A}_{ij}$ is the matrix obtains by deleting the $i$th row and $j$th column.
This all comes from the formula for the inverse of a matrix using cofactor expansion (sometimes also called ``Laplace's Formula'').

Finally, we need to know what the partial derivative of the determinant is with respect to each of its entries. 
In what follows we are going to consider $X = (x_{ij})$ as an abstract $n\times n$  matrix with entries being variables. 
This means that $\det(X)$ will be viewed as a polynomial in $\ZZ[x_{ij}\colon 1 \leq i,j \leq n ]$. 
\begin{lemma}
	Let $X =(x_{ij})$ be a symbolic matrix. 
	$$ \dfrac{\partial \det(X)}{\partial x_{ij}} = \adj(X)_{ji}. $$
\end{lemma}
\begin{proof}
	The proof is direct. 
	By cofactor expansion we have $\det(X) = \sum_{j=1}^n x_{ij} \adj(X)_{ji}$ hence
	 \begin{align*}
	 	\dfrac{\partial \det(X)}{\partial x_{ij}} = & \dfrac{\partial}{\partial x_{ij}} \left [ \sum_{\ell=1}^n x_{i\ell} \adj(X)_{\ell i}\right] \\
	 	&= \sum_{\ell=1}^n \dfrac{\partial x_{i\ell}}{x_{ij}} \adj(X)_{\ell i} + x_{i\ell} \dfrac{\partial }{\partial x_{ij}} \adj(X)_{\ell i} \\
	 	&= \sum_{\ell=1}^n \delta_{\ell j} \adj(X)_{\ell i} = \adj(X)_{ji}.
	 \end{align*}
 	Note that on the second to last equality we used that $\dfrac{\partial }{\partial x_{ij}} \adj(X)_{\ell i}=0$ since $\adj(X)_{\ell i}$ has no terms with $i$ in the first entry and $\ell$ in the second entry (this is the cofactor expansion formula).
\end{proof}

To apply this we need the formula for the dot product of matrices. 
Sometimes this is called the ``Killing form''.\footnote{Named after Wilhelm Killing 1847--1923}.
If you have never done this exercise in your life you should do it.
\begin{exercise}
	Let $A,B \in M_n(R)$ for a commutative ring $R$. 
	One has 
	$$\Tr(A^TB) = \sum_{1\leq i,j \leq n}A_{ij}B_{ij}.$$ 
\end{exercise}
We can now prove our result.
\begin{theorem}
	Let $A = (a_{ij}) \in M_n(R)$ with $(R,\partial)$ a differential ring. 
	We have 
	 $$ \partial(\det(A)) = \Tr(\adj(A) \partial(A))$$
	where $\partial(A)$ denotes the matrix $\partial(A)= ( \partial(a_{ij}))$.
	Furthermore if $A \in \GL_n(R)$ then 
	 $$\partial(\det(A)) = \Tr( A^{-1} \partial(A)) \det(A).$$
\end{theorem}
\begin{proof}
	Let $X = (x_{ij})$.
	We are going to use the chain rule
	\begin{align*}
		\partial(\det(X)) =& \sum_{1\leq i,j \leq n} \dfrac{\partial \det(X)}{\partial x_{ij}}\partial(x_{ij}) \\
		=& \sum_{1\leq i,j \leq n} \adj(X)_{ji} \partial(x_{ij}) \\
		=& \Tr( \adj(X)\partial(X)).
	\end{align*}
    To get the last formula, if $X$ is invertible we use the previous formula $\adj(X) = \det(X) X^{-1}$.
\end{proof}


\subsection{Stalks and Germs of Holomorphic and Meromorphic Functions}

Recall that for $U' \subset U$ open subset of $\CC^m$ we have injectures $\hol(U) \to \hol(U)$ and $\Mer(U) \to \Mer(U')$ given by restricting the domain of some $f(z)$ to $U'$.
Both of these ring homomorphisms are injective by the analytic continutation principle (which holds in several variables as well as one variable).\footnote{If you have never showmn that analytic continuation works in two variables this is a good exercise.}
The \emph{stalk} at some $t_0 \in \CC$ is 
 $$ \hol_{t_0} = \varinjlim_{U \owns t_0} \hol(U), \quad \Mer_{t_0} \varinjlim_{V \owns t_0} \Mer(U) $$
where the direct limit is taken over open set $U$ containing $t_0$.
Any element of a stalk is called a \emph{germ}. 

It is important to know that there is always a ring homomorphism $\hol(U) \to \hol_{t_0}$ and that any element of $\hol(U)$ is determined by its stalk. 
Same goes for meromorphic functions.

\begin{remark}
	For the uninitiated, we recall that if $I$ is a partially ordered set then a directed system is a collection $((R_i)_{i\in I},(f_{i,j})_{i<j})$ consisting of rings $R_i$ and morphisms $f_{i,j}:R_i \to R_j$ whenever $i<j$. 
	
	The direct limit of the directed system then is the ring 
	 $$ \varprojlim R_i = (\coprod_{i\in I} R_i)/\sim $$
	where $r_i \in R_i$ and $r_j \in R_j$ are declared equivalent when for some $k>i,j$ we have $f_{i,k}(r_i)  = f_{j,k}(r_j)$.
\end{remark}

In the one variable case we for $a\in \CC$ we are going to use the notation 
 $$ \CC\langle t-a \rangle := \hol_a, \quad \CC\llangle t-a \rrangle = \Mer_a $$
And in the several variable case for $(a_1,\ldots,a_m) \in \CC^m$ we will use the notation
 $$ \CC\langle t_1 -a_1,\ldots, t_m - a_m\rangle = \hol_{(a_1,\ldots,a_m)}, \quad \CC\llangle t_1-a_1,\ldots, t_m-a_m \rrangle = \Mer_{(a_1,\ldots,a_m)}.$$
 In other books they use $\CC\lbrace t \rbrace$ for convergent power series but we are going to reserve this symbol for the ring of differential polynomials.

\subsection{Reduction to First Order Systems}

Any system of PDEs is equivalent to a first order system of PDEs.
The idea is that we can always introduce more variables every times we need to take a new derivative so that all of our expressions only involve single derivatives of variables. 
Later, for linear differential equations we will see that we can actually go backwards. 

We illustrate this in the case of linear first order differential equations in one differential indeterminate. 
Here we consider the equation
 $$ y^{(r)} + a_{r-1} y^{(r-1)} + \cdots + a_0 y =0. $$
By introducing ``velocity variables'' $v_j = y^{(j)}$ for $j=0,1,\ldots, r-1$ we get a new system
$$\begin{cases}
	v_0' = v_1, \\
	v_1' = v_2 ,\\
	\ddots \\
	v_{r-1}' = -a_{r-1}v_{r-1} - a_{r-2} v_{r-2} - \cdots - a_0 v_0 .
\end{cases}$$
which then can be written in matrix form 
 $$ V' = AV $$
where 
 $$ V = \begin{pmatrix}v_0 \\
 v_1 \\
 \vdots \\
 v_{r-1} 
 \end{pmatrix}, \qquad A = \begin{pmatrix}
 0 & 1 & 0  & \cdots & 0 \\
 0 & 0 & 1 & \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & 0 & \cdots & 1 \\
 -a_{r-1} & -a_{r-2} & -a_{r-3} & \cdots & -a_0
 \end{pmatrix}.
 $$
Here $A$ is just the transpose of a companion matrix. 
We will often convert between higher order equations and first order equations in this way. 

\begin{remark}
	In the case of linear differential equations there is a way of going backwards using cyclic vectors. 
	That is, most first order linear systems of differential equations  $n$ variables in one derivative can be converted into a order $n$ equation in $n$ dependent variables. 
\end{remark}

\subsection{Linear Systems}
Let $A \in M_n(R)$ where $R$ is a differential ring.  
The system 
\begin{equation}
Y' = A Y
\end{equation}
 is called a \emph{linear system at over $R$} in the indeterminates $Y=(y_1,\ldots,y_n)$ (I am going to allow myself to abusively conflate row and column vectors). 
 The letter $n$ is sometimes called the \emph{rank} of the linear system. 
 
\begin{exercise}
	The solutions of a linear system form an $R^{\partial}$-module. 
\end{exercise}
 
A matrix $\Phi \in \GL_n(R)$ is called a fundamental set of solutions or \emph{fundamental solution} if 
 $$ \Phi' = A \Phi,$$
where the derivatives of $\Phi$ in the expression $\Phi'$ are taken component-wise. 
The idea is that the columns of the matrix $\Phi$ form a basis of solutions over the constants $R^{\partial}$.

Fundamental matrices are unique. 
Any solution of the linear system takes the form $\Phi Z$ for some vector $Z \in (R^{\partial})^{\oplus n}$. 
This menas that if $\widetilde{\Phi}$ is another fundamental matrix there exists some $M \in \GL_n(R^{\partial})$ such that
\begin{equation} \label{E:monodromy-matrix}
  \widetilde{\Phi} = \Phi M.
\end{equation}
In the theory of monodromy, these will become the monodromy matrices and in the Picard-Vessiot theory of linear differential algebraic extensions of differential fields these matrices are going to become the Galois group elements.
This is so important we are going to put it in a theorem environment. 
\begin{theorem}[Existence of ``Monodromy'' Matrices]
	If $\Phi$ and $\widetilde{\Phi}$ are two fundamental matrices of a rank $n$ linear system over a differential ring $(R,\partial)$ then there exists some $M \in \GL_n(R^{\partial})$ such that $\widetilde{\Phi} = \Phi M$.
\end{theorem}

To prove a fundamental set of solutions we are going to use existence and uniqueness together with the following lemma.

\begin{lemma}\label{L:linear-independence}
	Let $K$ be a $\partial$-field. 
	If $Y_1,\ldots,Y_n \in K^n$ are linearly independent over $K$ then they are linearly independent over $C=K^{\partial}$. 
\end{lemma}
\begin{proof}
	We prove this by proving they are linearly dependent over $K$ if and only if they are linearly dependent over $C$.
	If they are linearly dependent over $C$ then clearly they are linearly dependent over $K$. 
	Conversely, suppose that they are linearly dependent over $K$.
	We will prove this by induction so we can suppose that no proper subset is linearly dependent over $K$ otherwise we could apply the inductive hypothesis. 
	The base case is immediate. 
	
	Now we do the inductive step. 
	By clearing denominators we have $Y_1 = \sum_{j=2}^n c_j Y_j$ for some $c_j \in K$.
	We have 
	\begin{align*}
	0=&Y_1'-AY_1 \\
	&= \sum_{j=2}^n c_j' Y_j + \sum_{j=2}^n c_jY_j' - \sum_{j=2}^n c_j AY_j\\
	&= \sum_{j=2}^n c_j' Y_j 
	\end{align*}
	But since $Y_2,\ldots,Y_n$ were assumed to be linearly independent we must have $c_2'=\ldots=c_n'=0$ which proves the $c_j$'s are constants. 
\end{proof}
 
 \subsection{Holomorphic Linear Systems}
A \emph{holomorphic linear system} at $t_0 \in \CC$ is a linear system over $R= \CC\langle t -t_0\rangle$. 
That is, it is a system of linear differential equations 
$$ Y' = AY $$
where the matrix $A$ is holomorphic at $t_0\in \CC$.
 
 We now prove the existence and uniqueness theorem for holomorphic linear systems.
 
 \begin{theorem}[Existence and Uniqueness]
Let $t_0 \in \CC$
Let $A \in M_n(\CC\langle t-t_0 \rangle)$. 
Let $Y_0 \in \CC^n$. 
There exists a unique $Y \in \CC\langle t-t_0 \rangle^{\oplus n}$ such that 
$$\begin{cases}
	Y' = A Y,\\
	Y(t_0) = Y_0.
\end{cases}$$
 \end{theorem}
 There are three ways of doing this.
 I might add some more details later.
 \begin{enumerate}
 	\item Use power series expansions, then prove a convergence result. 
 	\item Big Hammer: Use Cauchy-Kowalevski\footnote{This is the same as Cauchy-Kovaleskaya.
 	Some people spell the Russian name differently. }
   This theorem is morally the same as above just with more complicated PDEs. 
   One shows that there is a power series solution then proves convergence.
    \item Bigger Hammer: Use the existence of differentially closed fields $\widehat{K}$ is the $\partial$-closure of the field $K \subset \CC\llangle t-t_0 \rrangle$ given by $K=\QQ(a_{ij} : 1\leq i,j \leq n )_{\partial}$.
    This is the differential field generated by the coefficients of the matrix $A$.
    The Siedenberg embedding theorem then tells us that $\widehat{K} \subset \CC\llangle t-t_0 \rrangle$, and this gives us a holomorphic solution of $Y'=AY$. 
    By the property of differential closures once we find a solution we can keep adjoining solutions using Blum's axiom. \taylor{explain this further}.
 \end{enumerate}

We now prove the existence of a fundamental matrix.
\begin{lemma}
	Every holomorphic linear system which is holomorphic at $t_0 \in \CC$ admits a fundamental matrix $\Phi(t)$ which is holomorphic at $t_0$.
\end{lemma}
\begin{proof}
By existence and uniqueness we can always find a solution $Y_i \in \CC\langle t-t_0 \rangle^{\oplus n}$ satisfying 
 $$ Y_i' = AY_i, \quad Y_i(t_0) = e_i $$
where $e_i$ is an elementary column vector (it has zeros everywhere except for the $i$th position).
The solutions $Y_1,\ldots,Y_n$ are linearly independent over $K=\CC\langle t-t_0 \rangle^{\oplus n}$ because $e_1,\ldots,e_n$ are linearly independent over $K$. 
Hence by Lemma~\ref{L:linear-independence} we get that the solutions are linearly independent over over $\CC$.
 The matrix $$\Phi = [Y_1 \vert Y_2 \vert \cdots \vert Y_n ]$$ 
 is our fundamental system.
 \end{proof}

\subsection{Restricting the Coefficient Matrix to a Lie Algebra}
It will be conventient in the equation $ Y' = A Y $ to restrict the matrix $A$ to a particular Lie algebra $\Lie(G)$ of some Lie group $G$. 
Here we recall that a Lie group (real or complex) is just a manifold (real of complex) with the structure of group. 
\footnote{There are also algebraic group or group schemes but readers familiar with those already know all of this, so I'm going to not say anything about that as it will take us two far afield.}
The Lie algebra of such a Lie group $\Lie(G)$ can be described either as the group of tangent vector at the identity of $G$ or as the globally invariant vector fields on $G$ (obtained by propagating the tangent vector at the tangent space at the identity to any other point of the manifold by pushforward by multiplication-by-$g$). 
All Lie algebras come with a Lie bracket $(A,B) \mapsto [A,B]$ which is an infinitesimal version of the group multiplication. 
It turns out that this multiplication satisfies the so-called Jacobi identity.
The fundamental property of Lie algebras of Lie groups are that if $G$ is a matrix Lie group then
 $$A\in \Lie(G) \implies e^A \in G.$$
Table~\label{T:lie} gives some common Lie groups with their Lie algebras.
As a sanity check not that if $A^*=-A$ then $(e^A)^* = e^{A^*}=e^{-A} = (e^{A})^{-1})$ so skew-adjoint matrices give rise to unitary matrices after exponentiation.
\begin{table}\label{T:lie}
	\begin{center}
		\begin{tabular}{ccc}
			type & Lie Group & Lie Algebra\\
			\hline complex & $\GL_n(\CC)$ & $M_n(\CC)$ \\
			real & $U_n$ unitary, $U^*=U^{-1}$ & skew-adjoint $A^*=-A$ \\
			complex & $\SL_n(\CC)$, $\det(A)=1$ & trace free $\tr(B)=0$ 
		\end{tabular}
	\end{center}
	\caption{Some common Lie groups and their Lie algebras.}
\end{table}

For the uninitiated we mention that we often axiomatize the notion of a Lie algebra as an $R$-module (or a functor to $R$-modules) which has a Lie bracket and satisfies the Jacobi identity axiom. 
This is useful but not what we mean here. 
The collection of abstract vector fields on a space (scheme, complex manifold, real manifold) satisfy these axioms for example and here the ring $R$ is a ring of functions on a space. 

The main reason we mention Lie groups is because if we restrict our linear equations to have values in a Lie algebra, then the solution will be valued in a Lie group.
\begin{theorem}
	Consider $Y'(t)=A(t)Y(t)$. If $A(t)$ is valued in $\Lie(G)$ then any fundamental matrix will be valued in $G$. 
\end{theorem}
\begin{proof}
	This is a consequence of Theorem~\ref{T:solutions}
\end{proof}



\subsection{Monodromy of Holomorphic Linear Systems}

Consider a holomorphic linear system 
$$Y'=AY, \quad A=A(t) \in M_n(\hol(U)),$$ 
where $U\subset \CC$ a connected open set. 
By the previous section for each $t_0 \in U$ there exists a fundamental matrix $\Phi$ which is holomorphic in a neighborhood of $t_0$.
We are going to want to analytically continue $\Phi$ along every path $\gamma$ starting at $t_0$ and obtain $\Phi^{\gamma}$ which will eventually allow us to cook-up a group homomorphism from the fundamental group of paths starting at $t_0$ to $\GL_n(\CC)$ which measures how much $\Phi$ changed once we take it around the look. 

\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.5]{analytic-continuation.eps}
	\end{center}
\caption{A picture of analytically continuing a local fundamental matrix along a path.}
\end{figure}

The group homomorphism 
 $$ \rho: \pi_1(U,t_0) \to \GL_n(\CC), \quad \rho(\gamma) = M_{\gamma}$$
is called the \emph{monodromy representation}.
We will now explain what $M_{\gamma}\in \GL_n(\CC)$ is  supposing $\Phi_{\gamma}$ exists:
since $\Phi$ and $\Phi_{\gamma}$ are both fundamental matrices at $t_0$ then as in  \eqref{E:monodromy-matrix} there exists some $M_{\gamma}$ such that 
 $$ \Phi_{\gamma} = \Phi M_{\gamma}.$$
That is all.

We need to set our convention for concatenation of paths. 
If $\gamma_1$ and $\gamma_2$ are two paths in $U$ where the endpoint of $\gamma_2$ is the starting point of $\gamma_1$ then we will let $\gamma_2\gamma_1$ denote the path which first performs $\gamma_1$ then performs $\gamma_2$. 

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{fundamental-group-convention.eps}\label{F:fundamental-group-convention}
\end{center}
\caption{The convention we use for composition of paths. Other people use other conventions and it will mess up your formulas.}
\end{figure}

\begin{remark}[WARNING]
	Conventions on concatenation of paths changes from text to text and this will mess with your formulas.
\end{remark}

With our convention for concatenation of paths we have.
 $$ \Phi_{\gamma_2\gamma_1} = (\Phi_{\gamma_1})_{\gamma_2}$$
On one hand we have $\Phi_{\gamma_2\gamma_1} = \Phi M_{\gamma_2\gamma_1}.$
On the other hand we have $ (\Phi_{\gamma_1})_{\gamma_2}= (\Phi M_{\gamma_1})_{\gamma_2} = \Phi_{\gamma_2} (M_{\gamma_1})_{\gamma_2} = \Phi M_{\gamma_2} M_{\gamma_2}.$
This proves that 
 $$ M_{\gamma_2\gamma_1} = M_{\gamma_2}M_{\gamma_1}.$$

Finally, suppose that $\Psi$ is another fundamental matrix at $t_0$.
Then $\Psi = \Phi M$ for some $t_0$ and let $\Psi_{\gamma} = \Psi N_{\gamma}$. 
Then we have 
 $$\Phi M N_{\gamma} =\Psi N_{\gamma}  =\Psi_{\gamma} = \Phi_{\gamma} M = \Phi M_{\gamma} M, $$
which implies 
 $$ N_{\gamma} = M^{-1} M_{\gamma} M .$$
 This proves the representation is independent of the choice of fundamental matrix up to conjugation.

\begin{example}[Babymost Example]
	In the rank one case we have a differential equations 
	 $$ y'(t) = a(t) y(t), \quad a(t) \in \hol(U). $$
	This has a solution $\phi(t) = \exp( \int_{t_0}^t a(s) ds)$ which is also the fundamental matrix. 
	This formula makes sense in a small disc around $t_0$.
	For things to be interesting we need  $ \int_{\gamma} a(s) ds $ to have monodromy.
	
	If $a(t) = 1/t$ this would be the simplest case. 
	This is a little to simple as $\int_{t_0}^t \frac{ds}{s}$ would give a branch of $\log(s)$ which would only change the exponent by $2\pi i$.
	
	If $a(t) = c/t$ for some constant $c$, then things get a little interesting. 
	One then has $y(t) = t^{c} := \exp( c \log(t))$ as a solution. 
	In this case if we let $\gamma_0$ be a loop around the origin and $M_0 = M_{\gamma_0}$ we find that 
	$$M_{0} = \exp{2 \pi i c}.$$
	\qed
\end{example}

For any path $\gamma$ in $U$ starting at $a \in U$ and ending at $b\in U$ and any fundamental matrix $\Phi$ in a neighborhood of $a$ we are going to show that we can analytically continue $\Phi$ along $\gamma$ to get a new fundamental matrix $\Phi^{\gamma}$ which is the analytic continuation of $\Phi$ along gamma.
There are some issue that we need to address.
\begin{enumerate}
\item How do we know that the fundamental matrix doesn't have a natural stopping point where it can't be continued further?
\item How do we know that the continuation $\Phi^{\gamma}$ doesn't degenerate after leading the initial ball $B$ where the power series defining it converged? How do we know solutions don't become linearly dependent?
\end{enumerate}

Let's address the first issue. 
Suppose that $\Phi$ is analytic in some ball $B$ around $a$ and that there is some $a_1$ on the boundary of $B$ where $\Phi$ doesn't extend. 
Well since $a \in U$ we know that there exist some $\Phi_1$ a fundamental matrix which is valid in some neighborhood $B_1$ of $a_1$. 
Then on $B\cap B_1$ we there exists some matrix $M_1 \in \GL_n(\CC)$ such that 
 $$ \Phi_1 = \Phi M_1.$$
By analytic continuation we could actually extend $\Phi$ to $B\cap B_1$ and hence by the sheaf property there exists some unique $\Phi_2$ such defined on $B \cup B_1$ which restricts to $\Phi$ and $\Phi_1$ on there respective domains.

Let's now address the second issue. 
Let $\det(\Phi)=W$. 
We need to show that $W(t)$ is never zero on these continuations. 
We know that 
 $$ W'(t) = \Tr( \Phi^{-1} \Phi') W(t). $$
But since $\Phi' = A\Phi$ we have that $\Phi^{-1} \Phi' = \Phi^{-1}(t) A(t) \Phi(t)$ and since trace is invariant under conjugation our scalar equation becomes 
 $$ W'(t) = \Tr(A(t)) W(t), $$
and we see that 
 $$ W'(t) = \exp(\int_{t_0}^t \Tr(A(s)) ds),$$
where the integral is understood to be a path integral. 
This is never zero which implies that $\Phi_{\gamma}(t)$ always remains a fundamental system of solutions.

Finally, we just want to make the remark that $\Phi_{\gamma}$ only depends on the homotopy class $[\gamma]$ of $\gamma$. 
This is because path integrals are well-defined on homotopy classes.  



\begin{theorem}
For every $U \subset \CC$ and every $A(t) \in \hol(U)$ and every $t_0 \in U$, monodromy of a fundamental set of solutions is well-defined and hence induces a well-defined monodromy representation $ \pi_1(U,t_0) \to \GL_n(\CC),$ given by $[\gamma] \mapsto M_{\gamma}$ where $M_{\gamma}$ is the matrix  $\Phi_{\gamma}=\Phi M_{\gamma}$ for a fundamental matrix $\Phi$.
\end{theorem}

	\begin{example}[Euler Systems]
	In a punctured neighborhood around $0 \in CC$, consider the system 
	$$ Y' = \frac{A}{t} Y. $$
	Consider the function $t^A = \exp(A \log(t))$ for some branch $\log(t)$ and $\exp$ denoting the matrix exponential. We have 
	$$\dfrac{d}{dt}\left[ t^A\right] = \exp(A \log(t) ) A \frac{1}{t} = \frac{A}{t} t^A,$$
	so the matrix $\Phi(t) = t^A$ is a local matrix solution of this equation. 
	Since $\det(e^B) = e^{\Tr(B)}$ for any matrix $B$ we have $\det(\Phi) = t^{\Tr(A)}$ which is never zero and hence $\Phi(t)$ is a fundamental matrix. 
	
	Now let $\gamma$ be a loop in $U$ that encloses the origin.  
	We can compute 
	 $$\Phi_{\gamma}(t) = \exp( A( \log(t) + 2\pi i) = \Phi(t) \exp(2\pi i A) $$
	and hence $M_{\gamma} = \exp(2\pi i A).$
\end{example}

\begin{exercise}
	Every matrix $M \in \GL_n(\CC)$ can appear as the monodromy matrix of some system. (Hint: use the Euler system and show that for every $M \in \GL_n(\CC)$ there exist some $A \in M_n(\CC)$ such that $\exp(2\pi i A) = M$. This needs some ideas like a matrix logarithm or using a Jordan canonical form.)
\end{exercise}

\section[Fuchsian Condition]{Classification of Fuchsian Equations}
The Fuchsian condition is a condition on meromorphic differential equations that we impose that make it so that solutions aren't divergent. 
Maybe this isn't obvious but if one applies the power series technique to innocent looking differential equations they can have formal power series solutions which are completely divergent. 
The next example shows this.
\begin{exercise}
	Consider the equation 
	 $$ t^3 y''(t) + (t^2+t) y'(t) - y(t) =0.$$
	If we expand in a power series we find that for $ y(t) = \sum_{n=0}^{\infty} a_n t^n $
	to be a solution one had the initial value difference equation
	$$\begin{cases}
	a_0 =0, \\
	a_1 = a, \\
	a_n = -(n-1)a_{n-1}.
	\end{cases}$$
	where $a \in \CC$ is arbitrary. 
	One finds that 
	$$ y(t) = a \sum_{n=1}^{\infty} (-1)^{n+1} (n-1)! t^n \in \CC[[t]]\setminus \CC\langle t \rangle$$
	is a divergent power series solution! Note that $\vert a_{n+1} \vert/\vert a_n \vert = n \to \infty$ as $n\to \infty$.
\end{exercise}	

So what is the issue? 
The issue is that when we convert this equation into a first order system of differential equations is has a pole of order bigger than one. 
One can check that if we let $y'(t)=v(t)$ in the above example we see that  $v'(t) = -\frac{t^2+t}{t^3}v(t)+\frac{1}{t^3}y(t)$ and letting $Y(t) = (y(t),v(t))$ we get the first order system 
 $$ Y' =A(t) Y $$
where 
\begin{align*}
 A(t) =& 
\begin{pmatrix}
0 & 1 \\
-\frac{t+1}{t^2} & \frac{1}{t^3}
\end{pmatrix} 
=&
\begin{pmatrix}
0 & 0 \\
0 & 1
\end{pmatrix} \frac{1}{t^3}
+\begin{pmatrix}
0 & 0 \\
-1 & 0 
\end{pmatrix}\frac{1}{t^2}
+
\begin{pmatrix}
0 & 0 \\
-1 & 0 
\end{pmatrix}\frac{1}{t}
+
\begin{pmatrix}
0 & 1 \\
0 & 0  
\end{pmatrix}
\end{align*}
The matrix expansion of $A(t)$ has a pole of order bigger than one at $t=0$. 

In what follows we are going to let $\PP^1$ denote the projective line (equivalently the Riemann sphere). 
In order to avoid all of the divergent behavior we introduce the notion of a Fuchsian differential equation. 
We will later prove that these differential equations have ``regular singular points''.
\begin{definition}[Fuchsian Differential Equations]
Consider a rank $n$ first order system of differential equations 
\begin{equation}\label{E:first-order-system}
  Y' = A(t) Y. 
 \end{equation}
with $A(t) \in M_n(\hol(\PP^1\setminus T))$ for $T\subset \PP^1$ a finite collection of points. 
We say the system  is \emph{Fuchsian at $t_0 \in T$} if $A(t)$ has the form
		 $$ A(t) = \frac{B(t)}{t-t_0},$$
where $B(t)$ is  holomorphic at $t_0$. 
We say the system is \emph{Fuchsian} at if it is Fuchsian at every point in $T$. 
\end{definition}
We extend this concept to higher order differential equations in one variable by saying that they at Fuchsian and Fuchsian at a point if there associated first order system is. 

\begin{exercise}[Fuch's Criterion For ODEs In One Variable]
	Consider a univariate holomorphic system on $\PP^1\setminus S$. 
	A first order system 
	 $$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t)=0 $$
	is Fuchsian at $t=t_0$ if and only if  the poles of the coefficients are restricted by $\mult_{t=t_0}( a_j(t) ) \geq n-j $.
	This means that the equation takes the form
	 	 $$ y^{(n)} + \frac{b_{n-1}(t)}{t-t_0} y^{(n-1)} + \cdots + \frac{b_{0}(t)}{(t-t_0)^n} =0, $$
	where the $b_j(t)$ are holomorphic at $t=t_0$
\end{exercise}

\begin{example}[Airy Equation]
	Consider the Airy equation 
	 $$ y'' = ty.$$
	One can see that this system is regular singular at $t \in \PP^1\setminus \infty$.  
	At $t=\infty \in \PP^1$ we need to change variables $t=1/s$ and we find that $dt = \frac{-1}{s^2}ds$ which means $\frac{d}{dt} = -s^2 \frac{d}{ds}$ and $$\frac{d^2}{dt^2} = s^2 \frac{d}{ds} s^2 \frac{d}{ds}= s^2 (s^2\frac{d}{ds} + 2s)\frac{d}{ds} = s^4\frac{d^2}{ds^2}+2s^3 \frac{d}{ds},$$
	which gives 
	%$$s^4 \frac{d^2y}{ds^2}+ 2s^3 \frac{dy}{ds} = \frac{1}{s} y, $$
	%or 
	 $$ \frac{d^2y}{ds^2}+ \frac{2}{s} \frac{dy}{ds} - \frac{1}{s^5} y=0. $$
	From this we see that there is an irregular singular point at $s=0$.
\end{example}

\begin{remark}
	There are two ways to compute what $\frac{d^2}{dt^2}$ in the chart at infinity. 
	The first way is to act on an unknown function $f$ by the operator $-s^2 \frac{d}{ds}$ twice and then pretend line you never used the symbol $f=f(s)$ for a computation.
	The second way is to consider the non-commutative ring $\CC[s,\partial]$ subject to the relations $\partial s = s\partial + 1$. 
	This is the a ring of linear differential operators on $\CC[s]$ called the \emph{Weyl algebra}.
	The second way is really equivalent to the first way.
\end{remark}

As stated before, we care about Fuchsian differential equations because they tell us that the solutions are nice. 
By ``nice'' we mean that the singularities are not out of control.
By ``out of control'' we mean, regular singular. 
This means that in every sector $S_{t_0}(\alpha,\beta)$, if we approach the points $t=t_0$ with bounded angle of variation then the solution must have at worst a pole.
Here is a picture of such a sector:

\begin{figure}[h]
	\begin{center}
	\includegraphics[scale=0.5]{sector.eps}
	\end{center}
	\caption{A sector used in the definition of regular singular. }
\end{figure}

In what follows we will let $S_{t_0}(\alpha,\beta) = \lbrace t \in \CC : \alpha<\arg(t-t_0) <\beta$ where $t_0 \in \CC$, $\alpha,\beta \in [0,2\pi]$ with $\alpha>\beta$ and $\arg$ the branch of the argument taking valued in $[0,2\pi)$.
We will also let $B_R(t_0)$ denote the open disc of radius $R$ centered at $t_0$.
A set of the form $S_{t_0}(\alpha,\beta) \cap B_R(t_0)$ will be called a bounded sector eminating from $t=t_0$, and a bounded sector contained in another bounded sector as an open set will be called a bounded subsector.
\iffalse 
\begin{definition}
Let $S = S_{t_0}(\alpha,\beta) \cap B_R(t_0)$ be a sector of bounded radius eminating from $t_0$ (which is by definition open and doesn't contain $t_0$).
We say that a matrix $\Phi(t) \in M_{m,n}(\hol(S))$ is \emph{regular singular at $t_0$} if and only if for all subsectors $S' \subset S$ of strictly smaller radius and angle there exists some integer $m$ such that 
 $$ \lim_{t \to t_0, t\in S'} \Vert \Phi(t) \Vert  = O( \vert t-t_0 \vert^{-m}).$$
 \end{definition}
\fi

\begin{definition}
	We say that $t=t_0$ is a \emph{regular singular point} if and only if  for every local sector at $t=t_0$ there exists a holomorphic basis of solutions $Y_1(t),\ldots,Y_n(t)$ with $Y_j(t)=(y_{j1}(t),\ldots,y_{jn}(t))^T$ and $\lambda \in \CC$ such that 
	$$ \lim_{t\to t_0} (t-t_0)^{\lambda} y_{ji}(t) =0. $$
\end{definition}

That seems like a lot but all this is saying is that as you approach your point in question you don't blow up like an essential singularity.
\begin{theorem}[Fuch's Criterion]
	Solutions of Fuchsian systems only have at worst regular singular points locally. 
\end{theorem}
\begin{proof}
	The trick in this proof is to use the isomorphic $\CC^n \cong \RR^{2n}$ and make estimates as if the functions were real valued. 
	Here we will view $A(t)$ as a function to $M_{2n}(\RR)$ which is real analytic and a soltion $Y(t)$ as a function to $\RR^{2n}$ which is real analytic. 
	Also we can observe that a solution $Y(t)$ from this real analytic perspective has $\vert Y(t) \vert^2 = Y(t) \cdot Y(t)$. 
	Also, we let $Y'(t)$ denote it's usual real analytic derivative which coincides with its complex analytic one (after again changing the complex analytic one again to real analytic function). 
	We have 
	$$\dfrac{d}{dt}\left[ \ln \vert Y(t) \vert^2 \right] = \dfrac{2 Y'\cdot Y}{\vert Y\vert^2} = 2\frac{ (A Y)\cdot Y}{\vert Y \vert^2}.$$
	 Taking norms and using $\vert V \cdot W \vert \leq \vert V \vert \cdot \vert W \vert$ with $\vert A V \vert \leq \Vert A \Vert \cdot \vert V \vert $ we get  
	  $$ \vert \dfrac{d}{dt}\left[ \ln \vert Y(t) \vert \right] \vert  \leq \Vert A(t) \Vert \leq \frac{C_0}{\vert t \vert}$$ 
	 where in the last line we used the Fuchsian hypothesis.
	 This then gives along a given contour $\gamma(r) = e^{i\theta_0}(r_0-r)$ starting at $\gamma(r_0) = t_0 = e^{i\theta_0}r_0$ and ending at $t=e^{i\theta_0}(r_0-r)$ that $$ \ln\vert Y(t) \vert \leq C_1 + \int_{\gamma}\frac{C_0}{\vert s \vert } d\vert s\vert =  C_1 - \int_{0}^r \frac{C_0}{r-r_0} \vert e^{i\theta_0}dr\vert = C_1 - C_0 \ln \vert r-r_0 \vert $$ 
	 which implies that $\vert Y(t) \vert \leq e^{C_1} \vert t \vert^{-C_0}$.	
	 \taylor{FIXME, add Gronwall-like statement to appendix}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%
\section{Hilbert's 21st Problem}
%%%%%%%%%%%%%%%%%%%%%%%
We are now in a position to state Hilbert's 21st problem. 
The Monodromy map associated to every Fuchsian system on $\PP^1$ with poles contained in $T$ a representation of its fundamental group. 
$$ \frac{\lbrace \mbox{Fuchsian systems, rank $n$ on $\PP^1$ 
		with poles on $T$ } \rbrace}{\mbox{(global holomophic gauge trans)}} \to\frac{\lbrace \mbox{repns $\rho:\pi_1(\PP^1\setminus T, t_0) \to \GL_n(\CC) $}\rbrace}{(\mbox{matrix conjugation})} $$
Hilbert's 21st problem asks if this map of sets is surjective. 
\begin{problem}[Hilbert's 21st Problem]
	Is it the case that every representation $\rho:\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ comes from a Fuchsian differential equation with poles supported on $T$?
\end{problem}
This problem has a rather crazy history. 
The problem was first posed by Hilbert in 1900 during the International Congress of Mathematicians (ICM). 
This is the event where the give out Field's Medals and occurs once every four years. 
In 1907 Plemelj\footnote{Nalini Joshi pronounces this ``Plum-ell-i'', I'm not sure how to pronounce this name} published a positive answer to the question. 
In 1983, Treibich-Koch published a gap in the proof; it turns out that previous work from Dekkers in 1979 implies that the map is indeed surjective in the rank two case. 
Finally, in 1990, Bolibruch showed that the map is not surjective in rank higher than two disproving the conjecture.



%%%%%%%%%%%%%%%%
\subsection{Representations of $\pi_1(\PP^1\setminus T)$}
%%%%%%%%%%%%%%%%
The representations $\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ are rather easy to describe. 
The key observation is that $\PP^1$ minus some points is homotopy equivalent to a bouquet of circles:
$$ \PP^1 \setminus \lbrace t_1,\ldots,t_n\rbrace \approx \underbrace{S^1 \vee \cdots \vee S^1}_{ \mbox{ $(n-1)$-times}}$$
where $\approx$ denotes homotopy equivalence and $\vee$ denotes the wedge product of topological spaces. 
A picture of this homotopy equivalence for $\PP^1 \setminus \lbrace 0,1,\infty\rbrace$ is given in Figure~\ref{F:bouquet}.
\begin{figure}[h]\label{F:bouquet}
	\begin{center}
		\includegraphics[scale=0.5]{bouquet.eps}
	\end{center}
	\caption{The figure shows $\PP^1\setminus \lbrace 0, 1, \infty\rbrace$ being deformed into $S^1 \vee S^1$. 
		The first step is the increase the size of the holes to make it look like a bowling ball. 
		We then wrap one of the holes completely around to get a disc with two interior discs removed. 
		This is then seen to be equivalent to a circle with a line through it. 
		After contracting the middle line one gets the bouquet of circles. 
	}
\end{figure}
The convenient description allows us to see that the fundamental group is just the free group on $(n-1)$ generators
$$\pi_1(\PP^1\setminus\lbrace t_1,\ldots,t_n\rbrace) \cong F_{n-1} \cong \langle \gamma_1,\gamma_2,\ldots,\gamma_{n-1} \rangle,$$ 
the generators then can be taken to be homotopy classes of loops around each of the points $t_1,\ldots, t_{n-1}$. 
The last loop $\gamma_n$ around $t_n$ satisfies the relation 
$$ \gamma_n \cdots \gamma_2 \gamma_1  = 1.$$
You can actually see this loop is trivial if you think about it a little bit. 

Anyway, with this description the representations $\pi_1(\PP^1\setminus T) \to \GL_n(\CC)$ are determined by tuples $(M_1,M_2,\ldots,M_{n-1}) \in \GL_n(\CC)^{n-1}$ modulo simultaneous conjugation by an element in $\GL_n(\CC)$.
Here $M_j = \rho(\gamma_j)$. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gauge Transformations}
%%%%%%%%%%%%%%%%%%%%%%%
It remain to describe the equivalence relation of differential equations that we are using in the Hilbert's 21st problem.
Let $(R,\partial)$ be a $\partial$-ring and consider the equation
\begin{equation}\label{E:original-equation}
Y' = AY
\end{equation}
where $Y = (y_1,\ldots,y_n)$ and $Y' =(y_1',\ldots,y_n')$ and $A \in M_n(R)$. 
One can change coordinate in this differential equation and suppose that 
\begin{equation}\label{E:gauge-trans-one}
Y = \Phi \widetilde{Y}
\end{equation}
for some $\Phi \in \GL_n(R)$. 
In this situation we get a new equation 
\begin{equation}\label{E:gauge-transformed-equation}
\widetilde{Y}' = \widetilde{A} \widetilde{Y} 
\end{equation}
which is said to be \emph{gauge equivalent} to the previous equation. 
We will now compute what $\widetilde{A}$ is by plugging $Y = \Phi \widetilde{Y}$ into $Y'=AY$. 
We obtain $Y' = (\Phi \widetilde{Y})' = \Phi' \widetilde{Y} + \Phi \widetilde{Y}'$. 
We also obtain $AY = A\Phi \widetilde{Y}$. 
Putting these together gives $\Phi \widetilde{Y}' = A\Phi \widetilde{Y} - \Phi' \widetilde{Y}$ or 
$$ \widetilde{Y}' = \widetilde{A} Y, \qquad \widetilde{A} = \Phi^{-1} A \Phi - \Phi^{-1} \Phi '.$$
Both $Y \mapsto \Phi^{-1} Y$ and $A \mapsto A^{\Phi} := \Phi^{-1} A \Phi + \Phi^{-1} \Phi'$ are called \emph{gauge transformations} and define right group actions of $\GL_n(R)$ on $R^{\oplus n}$ and $M_n(R)$.
The equations \eqref{E:original-equation} and \eqref{E:gauge-transformed-equation} are called \emph{gauge equivalent}.

For holomorphic and meromorphic linear systems we can consider holomorphic and meromorphic gauge transformations. 
These gauge transformations can be local or global. 
What is interesting is that sometimes we can take a meromorphic linear systems and then convert it into a holomorphic linear systems by some meromorphic gauge transformation. 
In the case that we can do this the singularities of the original linear system are called \emph{apparent singularities}. 

\begin{example}
	Consider the linear system 
	\begin{equation}\label{E:apparent-singularity} 
	Y' = \begin{pmatrix}1 & \frac{1}{t^2} -\frac{2}{t} \\ t^2 & 0 \end{pmatrix} Y
	\end{equation}
	which is holomorphic on $\CC\setminus \lbrace 0 \rbrace$. 
	The singularty at $t=0$ is actually just apparent as it is gauge equivalent to the system
	$$ \widetilde{Y}' = \begin{pmatrix} 1 & 1 \\
	1 & 1 
	\end{pmatrix} \widetilde{Y}.$$
	To see this one uses a meromorphic gauge transformation.
	The point here is that the singularities of \eqref{E:apparent-singularity} are just apparent and that they can be removed by using 
	$$ Y =\begin{pmatrix}t^2 & 0 \\ 0 & 1 \end{pmatrix} \widetilde{Y}. $$
	As an exercise one needs to compute 
	$$ \widetilde{A} = \begin{pmatrix} \frac{1}{t^2} & 0 \\
	0 & 1\end{pmatrix} \begin{pmatrix}1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} t^2 & 0 \\ 0 & 1 \end{pmatrix} - \begin{pmatrix} \frac{1}{t^2} & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 2t & 0 \\
	0 & 0 \end{pmatrix}.$$
	which comes from the formula for gauge transformations.
\end{example}

The most important example is the case where $\Phi$ is a fundamental matrix. 
\begin{example}[Equivalence to Trivial Equation]\label{EX:equivalence-to-trivial}
	Consider a linear differential system $Y'=AY$ over a differential ring $(R,\partial)$. 
	Suppose that the system admits a fundamental matrix $\Phi \in \GL_n(R)$. 
	Then setting $Y = \Phi \widetilde{Y}$ we find that 
	$$ A^{\Phi} = \Phi^{-1} A \Phi - \Phi^{-1} \Phi' = \Phi^{-1}( A\Phi - \Phi') =0 $$
	and so that system is gauge equivalent to the trivial system 
	$$ \widetilde{Y}' =0.$$
\end{example}
It is important to observe that there usually aren't \emph{global} fundamental matrices.
This is what prevents us from trivializing all differential equations.

%%%%%%%%%%%%%%%%%%
\section[Classification of Fuchsian Equations]{Classification of Fuchsian Differential Equations on $\PP^1$}
%%%%%%%%%%%%%%%%%%
We are going to show that every Fuchsian differential system on $\PP^1$ with polar locus $S = \lbrace a_1,a_2,\ldots, a_m\rbrace \subset \PP^1\setminus \lbrace \infty \rbrace$ takes the form  
 $$ Y' = A(t) Y, \qquad A(t) = \frac{A_1}{t-a_1} + \cdots + \frac{A_m}{t-a_m},$$
where $A_1,A_2,\ldots, A_m \in M_n(\CC)$ are constant matrices.
To do this we first need to review some facts about residues and Riemann surfaces.


%%%%%%%%%%%%%%%%
\subsection{Some Reminders About Riemann Surfaces}
%%%%%%%%%%%%%%%%
Riemann Surfaces are just topological spaces equipped with a system of holomorphic charts that make them locally isomorphic to open subsets of $\CC$. 
A description of these charts for $\PP^1$ is given in Figure~\ref{F:projective-line}.
This allows us to make sense of what a holomorphic map is and make sense of what computations ``at infinity'' are. 

\begin{figure}[h]\label{F:projective-line}
	\begin{center}
		\includegraphics[scale=0.33]{projective-line.eps}
	\end{center}
	\caption{The projective line $\PP^1$ is isomorphic to the Riemann sphere $S^2$ and is composed of two coordinate charts. 
		The first chart we think of as the ``usual'' copy of $\CC$ (which algebraic geometers upgrade to the affine line $\AA^1$) which has coordinate $t$. 
		Then when we want to set $t=\infty$ we use another copy of $\CC$ with coordinate $s$ where $s=1/t$.
		The point $s=0$ corresponds to the points $t=\infty$ and we use this $s$ coordinate to do all of our computations at infinity. }
\end{figure}

One can upgrade this $\CC$ to $\CC^n$ an get a category of complex manifolds. 
It turns out that the category of compact Riemann surfaces equivalent to the category of smooth projective algebraic curves over $\CC$.
These are curves which are cut out by homogeneous polynomial equations in some complex projective space $\PP^n$. 
The same is not true for higher dimensional compact complex manifolds, there exists compact complex manifolds which aren't projective varieties (see for example \cite[pg 161]{Shafarevich2013}). 
It is true however, a theorem called Chow's theorem, that every compact complex manifold embedded into complex projective space is a projective variety (i.e. it is cut how by homogeneous equations). 

The case of complex projective curves (or equivalently Riemann Surfaces) is especially nice because this category is equivalent to the category of fields $K/\CC(t)$ which are algebraic. 
In the case of connected Riemann surfaces $X$the naturally assocaited field is the field of meromorphic functions on $X$ which we denote by $\Mer(X)$. 
In the case of projective curves $C$ the naturally associated field is the function field $\kappa(C)$.
Miraculously they are isomorphic even though they have drastically different descriptions away from the case of $X=C=\PP^1$. 
This case is rather easy to describe. 

We will prove that $\Mer(\PP^1) = \CC(t)$ which is easily seen to be the fraction field $\CC[t]$ of the polynomial functions on one of its open sets. 
We write $\PP^1 = U_0 \cup U_{\infty}$ and note that some $f \in \Mer(\PP^1)$ has finitely many poles on $U_0$. 
This means there exists a polynomial $g(t)$ such that $f(t)g(t)$ is entire. 
Since $f(t)$ is meromorphic and $g(t)$ is meromorphic the order of vanishing at infinity is finite. 
Here $\ord_{t=\infty} ( f(t) g(t)) = \ord_{s=0}(f(1/s)g(1/s) )$. 
This means that $f(t)g(t) = g(t) \in \CC[t]$. 
Hence $f(t) = h(t)/g(t)$ which proves that every Meromorphic function is rational.

In general $\Mer(X)$ is a finite algebraic extension of $\CC(t)$. 
To give an idea of how different-looking $\Mer(X)$ and $\kappa(X)$ can be consider the case of an elliptic curve $E$. 
As a Riemann surface we like to describe this as $\CC/\Lambda$ for some lattice $\Lambda \subset \CC$. 
In this situation, we have $\Mer(E) = \CC(\wp_{\Lambda}(t), \wp_{\Lambda}'(t))$ where $\wp_{\Lambda}(t)$ is the Weierstrass $\wp$-function associated to the lattice $\Lambda$. 
In the case where we want to present $E$ algebraically, then away from $\infty$ (some curves may have more than one ``point at infinity'' just not the traditional presentations of $\PP^1$ and $E$) we have $E \subset \CC^2$ given by the equation $y^2 = x^3+ax+b$ for some $a,b\in \CC$. 
Here we are using $(x,y)$ for complex coordinates. 
The crazy part is that there is a map $\CC/\Lambda \to E$ given by $x = \wp(t)$ and $y=\wp'(t)$ which gives the isomorphism. 
Here the point $0 \in \CC/\Lambda$ maps to $\infty$ in the projective model of the elliptic curve. 

\subsection{The Residue Theorem For Meromorphic Differential Forms}\label{S:residue-theorem}
We will need the following theorem about the sum of residues being zero later as we try to classify Fuchsian equations.
Here we briefly recall that for any meromorphic differential $\omega$ on a compact Riemann surface $X$ we can find a local parameter $t=t_b$ at $b\in X$ and then write $\omega$ as $f(t)dt$. \footnote{In coordinates on say $\CC$ the local parameter for $b\in  \CC$ is $t_b=t-b$ where $t$ is the usual complex variable. }
We can then develop $f(t)$ in a Laurent series to get 
$$ \omega =  \left(\frac{a_{-n} }{t^n} + \cdots + \frac{a_{-1}}{t} + a_0 + a_1 t + \cdots  \right) dt$$
and define the residue at $b$ by the usual formula 
$$ \res_{t=b}(\omega) = a_{-1}.$$
We will extend this to vector valued differential forms $A(t)dt$  by doing this component by component and taking the residues there.
\begin{theorem}[Residue Theorem]\label{T:residue-theorem}
	Let $\omega$ be a meromorphic differential on a compact Riemann surface $X$. 
	Then $ \sum_{a \in X} \res_{t=a}(\omega) =0.$
\end{theorem}
\begin{proof}
	We give a proof in the case that $X=\PP^1$. 
	A complete proof can be found at \cite[Proposition 6.6]{Schlag2014} and those notes can be found online as of 2022 by a simple Google search.
	
	The basic idea as depicted in figure \ref{F:sum-of-residues} is to take a simple closed contour $\gamma_1$ and its opposite contour $\gamma_2$ and realize that on one hand 
	$$ \int_{\gamma_1}\omega + \int_{\gamma_2} \omega =0 $$
	while on the other hand we have the classic residue theorem from complex analysis for each of these integrals
	$$ \int_{\gamma_1}\omega + \int_{\gamma_2}\omega = 2\pi i \sum_{b \in \PP^1} \res_{t=b}(\omega).$$
	\begin{figure}[h]\label{F:sum-of-residues}
		\begin{center}
			\includegraphics[scale=0.5]{sum-of-residues.eps}
		\end{center}
		\caption{One uses the basic residue theorem on two simple integrals which are opposite of each other to prove the residue theorem on $\PP^1$. }
	\end{figure}
\end{proof}

For $A(t) = (a_{ij}(t)) \in M_n(\CC((t)))$ we will do Laurent series developments entry-by-entry and write 
$$ A(t) = \sum_{j=-\infty}^{\infty} A_j (t-t_0)^j, \qquad A_j \in M_n(\CC).$$
For entries which are truely meromorphic, then for closed curves $\gamma$ we will have  
$$\int_{\gamma} A(t)dt = (\int_{\gamma}a_{ij}(t)dt).$$ 
As above if $A(t)dt$ is a matrix of meromorphic differential forms with poles at $t_1,\ldots,t_m$ on a Riemann surface $X$ with residues $R_j$ for $1\leq j \leq m$ then we get
$$ \sum_{j=1}^m R_j =0.$$

\begin{corollary}
	The sum of the residues of meromorphic matrix valued differential forms is zero.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Classification of Fuchsian Differential Equations on $\PP^1 \setminus S$}
%%%%%%%%%%%%%%%%%%%%%%%
Fuchsian differential equations on $\PP^1$ have a very simple form.
For a polar locus $S = \lbrace s_1,\ldots,s_m\rbrace$ we will often assume that $S$ takes the form $S =\lbrace 0,1,\infty,s_4,\ldots,s_m \rbrace$ which we can do by using a M\"{o}bius transformation. 
We can also if we want assume that the polar locus not contain $\infty$. 
\begin{theorem}
	Consider a Fuchsian differential equation on $\PP^1$,
	$$ Y' = A(t) Y, \qquad A(t) \in M_n(\CC(t)).$$
	If $A(t)$ has polar locus $S=\lbrace s_1,\ldots,s_m\rbrace \subset \PP^1$ not containing infinity then 
	$$ A(t) = \frac{A_1}{t-s_1} + \cdots + \frac{A_m}{t-s_m} $$
	where $A_j \in M_n(\CC)$ and $A_1+\cdots + A_m=0$.
\end{theorem}
\begin{proof}
	We can suppose without loss of generality that there are no poles at infinity. 
	To prove this one needs to recall that by the Mittag-Leffler theorem \cite[Proposition 2.19]{Schlag2014 } if $f(z)$ is meromorphic on $\CC$ with poles at $s_1,\ldots,s_m$ then there exists $p_j(z) \in \CC[z]$ polynomials such that 
	$$ f(z) - \sum_{j=1}^m p_j( \frac{1}{z-s_j}) $$
	is entire and the degree of $p_j$ is the order of the pole of $f$ at $z=s_j$.
	In our application we have that $A(t)$ has at most a pole at each $s_j$. 
	Hence there exists some matrices $A_1,\ldots, A_m \in M_n(\CC)$ such that the components of  
	$$B(t) = A(t) - \frac{A_1}{t-s_1} - \cdots - \frac{A_m}{t-s_m} $$
	are holomorphic on $\CC$. 
	Also note that $A_j/(t-s_j) = A_js/(1-ss_j)$ is also holomorphic at $s=0$ or $t=\infty$. 
	This means that $B(t)$ is entire and bounded and hence constant. 
	But we know that $\lim_{t\to s_j}B(t) =0$ by construction which means that it must be the constant function zero. 
	
	The second part about the sum of the residues being zero follows from the Residue theorem (\S \ref{S:residue-theorem}) but doing it component by component in the matrix $A(t)dt$.
\end{proof}

The residue matrices are so important we give them a name.
They are called the \emph{local exponents} of the linear differential equation. 
We will see that if $\rho$ is a local exponent of $A(t)/t$ where  $A(t)\in M_n(\CC[[t-t_0]])$ at $t=t_0$ with the property that $\rho+r$ is not an eigenvalue for any integer $r>1$ then the system admits a solution of the form $Y(t) = (t-t_0)^{\rho}Z(t)$ where $Z(t) \in \CC[[t]]^n$. 
If $A(t)$ is holomorphic then $Z(t)$ will be holomorphic.

We conclude this subsection with a classification of equations with one, two, three, and four singular points. 
The case of three singular points will end up leading to the theory of hypergeometric differential equations. 
The case of four singular points ends up leading to the theory of isomonodromic deformations and $P_{VI}$ the 6th Painlev\'{e} equation.

In the following examples the singular locus $S = \lbrace s_1,\ldots,s_m \rbrace \subset \PP^1$ can be taken without loss of generality to be $S=\lbrace 0,1,\infty,s_4,\ldots,s_m\rbrace$ since any three points can map to any other three points by a M\"obius transformation. 
\begin{example}[one singular point]
	Consider a Fuchsian differential equations with $S = \lbrace 0 \rbrace$. 
	Then we have 
	 $$ \dfrac{dY}{dt}= \frac{A_0}{t} Y $$
	for some constant matrix $A_0 \in M_n(\CC)$. 
	We then can use the chart at infinity $\partial_t = -s^2 \partial_s$ to conclude that the equation becomes $-s^2 \dfrac{dY}{ds} = s A_0 Y$ which gives 
	 $$ \dfrac{dY}{ds} = -\frac{A_0}{s}Y,$$
	which is not holomorphic at $\infty$ unless $A_0 = 0$. 
	Hence every such system is equivalent to 
	$$Y'=0.$$
\end{example}

\begin{example}[two singular points]
	Consider a Fuchsian differential equation with polar locus $S = \lbrace 0, \infty \rbrace$. 
	From the previous example we see that it has the form 
	 $$ \frac{dY}{dt} = \frac{A_0}{t} $$
	and that $A_0 = -A_{\infty}$.
\end{example}

The case of three singular points is sometimes called the Gauss case of the hypergeometric case because of its connections to the hypergeometric differential equations.
\begin{example}[three singular points]
	Consider a Fuchsian differential equation with polar locus $S= \lbrace 0, 1,\infty\rbrace$. 
	Such an equation has the form
	 $$ \frac{dY}{dt} = \left ( \frac{A_0}{t} + \frac{A_1}{t-1}\right) Y$$
	where $A_0 + A_1 + A_{\infty} =0$.
	Explicitly after changing coordinates to the chart at infinity (letting $t=1/s$) we find
	 $$ -s^2 \dfrac{dY}{ds} = \left ( \frac{A_0}{1/s} + \frac{A_1}{1/s-1}\right) Y $$
	which implies 
	 $$ \dfrac{dY}{ds} =- \left ( \frac{A_0+A_1}{s} + \frac{A_1}{1-s}\right) Y,$$
	and we can see $A_{\infty} = -A_0-A_1$ explicitly. 
	
	In section \S\ref{S:hypergeometric} we will show every rank two Fuchsian equation with polar locus $S=\lbrace a,b,c\rbrace \subset \PP^1$ can be reduced to the Gauss hypergeometric equation in a single dependent variable 
	$$ t(t-1)y'' +(c-(a+b+1)t)y'-aby =0.$$
\end{example}

The case of four singular points is sometimes called the Painlev\'e case because of its connections to the Painlev\'e equations.
\begin{example}[four singular points]
	Every Fuchsian differential equation with polar locus containing four points now cannot be normalized to a standard set of points.
	We can bring the first three points of $S$ to $0,1,\infty$ but a third point $\lambda\in \CC$ remains. 
	We will have $S=\lbrace 0,1,\infty, \lambda\rbrace$ and the differential equation will take the form 
	 $$ \dfrac{dY}{dt} = \left( \frac{A_0}{t} + \frac{A_1}{t-1} + \frac{A_{\lambda}}{t-\lambda}\right) Y $$
	where $A_0,A_1,A_{\lambda}\in M_n(\CC)$ and we define $A_{\infty}$ by the sum of the residues being zero $A_0+A_1+A_{\lambda}+A_{\infty}=0$. 
	
	A fun game to play here will be to determine the conditions under which we may vary $A_{\lambda}$ as a function of $\lambda$ and preserve the monodromy representation. 
	Note that $\pi_1(\PP^1\setminus \lbrace 0,1,\infty,\lambda_1\rbrace) \cong \pi_1(\PP^1\setminus \lbrace 0,1,\infty,\lambda_2\rbrace)$ for every pair of $\lambda_1$ and $\lambda_2$ so it makes sense to ask for monodromy representations to change. 
	Such deformations are called \emph{isomonodromic}.
	The criterian for deformations to be isomonodromic are given by Schlesinger's equations for matrices which give rise to the Painlev\'e equations. 
\end{example}


\section[Hypergeometric Equations]{Hypergeometric Differential Equations: Fuchsian Differential Equations of rank two on $\PP^1\setminus \lbrace 0,1,\infty\rbrace$}

 \label{S:hypergeometric}

The Gauss hypergeometric equation is the following homogeneous ordinary differential equation
\begin{equation}
   y'' + \frac{(a+b-1)t-c}{t(1-t)} y' + \frac{ab}{t(1-t)} y =0 .
\end{equation}
It solutions are so-called hypergeometric functions and has the remarkable property that any rank two Fuchsian differential equations on $\PP^1$ can be reduced to this equation for some collection of parameters $(a,b,c)$. 
The $(a,b,c)$ really encode eigenvalues of the residue matrices (=local exponents) and the form of the equation is really a consequence of the restriction of these local exponents in part due to Fuch's theorem which says that the sum of the local exponents in this rank 2 case with three singularities must be equal to one. 

\begin{exercise}[Hypergeometric Exponents]\label{E:exponents-for-hypergeometric}
	Show that the local exponents of the hypergeometric differential equation with parameters $(a,b,c)$ fall into the following table:
	$$\begin{tabular}{ccc}
		$0$ & $1$ & $\infty$  \\
	\hline \hline	$0$ & $0$ & $a$ \\
	$1-c$ & $c-a-b$& $b$
	\end{tabular}.	$$
	This is sort of a hard computation now but gets easier once more tools are developed in the rest of the section. 
	I would try it now for 30 minutes, then try it again once you have the indicial equation, then revisit it once more once you have Fuchs' relation.
\end{exercise}

The first mystery is showing that $n\times n$ first order linear systems can actually be reduced to an order $n$ equation in one dependent variable.
This is sort of the opposite of taking an equation of high order and reducing it to an equation a low order but in more variables. 
The key to this is the theory of $D$-modules. 
We will show that linear systems gives rise to a $D$-modules and conversely any $D$-module with a choice of basis gives a linear differential equation. 
Changing the basis will changes the differential equation by a gauge transformation. 
Now knowing that $D$-modules encode linear differential equations we apply Katz's theorem and show that $D$-modules will admit so called cyclic vectors.
In the case of rank two Fuchsian differential systems with three poles this reduces our equation to an order two equation linear equation in one dependent variable.

Finally, once we are in the order two case we need to show that all of our equations are determined by the local exponents and that we can manipulate these exponents by a series of gauge transformations and automorphisms of $\PP^1$ to bring our general equations into the Gauss hypergeometric case.
This involves a basic lemma about how local exponents change under Gauge transformations of the form $Y(t) = t^{\rho} \widetilde{Y}(t)$. 

\subsection{Weyl Algebras and $D$-Modules}\label{S:weyl-algebras}

Let $(R,\Delta)$ be a $\Delta$-algebra. 
\begin{definition}
	A \emph{Weyl algebra} associated to $(R,\Delta)$ is the ring $R[\Delta]$ of linear operators on associated to $(R,\Delta)$. 
	It is the non-commutative ring $R[\partial \colon \partial \in \Delta]$ where one has
	$$ \partial a = a \partial  + \partial(a), \qquad a \in R, \partial \in \Delta.$$
	One also has $\partial_1 \partial_2 = \delta_2 \delta_1$ for $\partial_1,\partial_2 \in \Delta$.
\end{definition}

The idea behind the formula $\partial a = a \partial + \partial(a)$ comes from looking a $a \in R$ when viewed as an element $a \in R[\Delta]$ as the linear operator ``multiplication by $a$''.
In this situation we have 
$$ (\partial a) \cdot f = \partial( af) = \partial(a)f + a \partial(f) = [\partial(a) + a \partial]\cdot f,$$
which justifies the rule. 

\begin{definition}
	A \emph{$D$-module} is a $R[\Delta]$-module. 
	We will simply call these $R[\Delta]$-modules. 
\end{definition}

Authors like to get cutesy with the above definition and it is worth pointing some things out. 
First, many authors define $\mathcal{D} = R[\Delta]$ and then talk about $\mathcal{D}$-modules. See for example Singer and van der Put. 
Some authors only define Weyl-algebras are for polynomial rings and refer to this particular Weyl algebra as \emph{the} Weyl algebra. 
In this case they take $R = \CC[x_1,\ldots,x_m]$ with $\Delta = \lbrace \partial_{x_1},\ldots, \partial_{x_m} \rbrace$ and then only talks about Weyl algebras (as we have defined above) as the only Weyl algebras.
This is useful when searching the literature for propositions about Weyl algebras that you need.
Finally, many authors restrict to the case $\Delta = \lbrace \partial \rbrace$ which will be the case we are interested in mostly and call these $\partial$-modules. 
In this case some authors (like Nick Katz) like to define $D$ as the derivation operator on the module $V$ which satisfies $D(av) = \partial(a) v + a D(v)$ for $v\in V$ and $a \in R$.
I reserve the right to use a mixture of these perspectives (and you should too).


\subsection{Linear Systems and $D$-Modules}
There is a procedure for converting between linear differential equations and $D$-modules which will be useful that we will now explain. 
In this subsection we will restrict to the case of a single derivative.

Given a rank $n$ linear system over $(R,\partial)$ given by 
$$ Y' = A Y, \qquad A = (a_{ij}) \in M_n(R), $$
we can define a $D$-module structure on $V=R^{\oplus n}$.
Let $e_1,\ldots,e_n$ be a standard basis for $V$.
Then we define 
$$ D(e_j) = \sum_{i=1}^n a_{ij} e_i.$$
Let $V_0 = (R^{\partial})^{\oplus n}$. 
We now have a $R^{\partial}$-linear operator on $V_0$ and we extend this to all of $V$ by specifying 
$$ D(b v_0) = \partial(b)v_0 + b D(v_0), \quad v_0 \in V_0, b \in R. $$
\begin{exercise}
	Check that this is well defined.
	This means that if $bv_0 = cw_0$ for some other $c \in R$ and $w_0 \in V_0$ then $D(bv_0) = D(cw_0)$. [This is a silly easy problem.]
\end{exercise}

Conversely, given a $D$-module structure on $V = R^{\oplus n}$ one then takes a basis $v_1,\ldots, v_n$ and finds that 
$$ D(v_j) = \sum_{i=1}a_{ij}v_i $$
for some $a_{ij}\in R$. 
This allows us to set up a linear differential equation 
$$ Y' = AY, \qquad A = (a_{ij}) \in M_n(R).$$
One then finds that the linear differential equation associated to the $D$-module is again the $D$-module with $v_1,\ldots,v_n$ identifying with the standard basis vectors. 

If instead we had chosen a different basis one can check that one will obtain a new differential equation
$$ \widetilde{Y}' = \widetilde{A} \widetilde{Y} $$
which is gauge equivalent to the first equation. 
This gives us a procedure for assigning a linear system of rank $n$ over $R$ (up to gauge equivalent) to every $R[\partial]$-module $V$ of finite rank $n$.
The point here is that change of basis of the $D$-module is gives rise to a gauge transformation of the associated linear system.

\begin{exercise}
	Show that indeed a change of coordinates on the $R$-module induces a gauge tranformations of the linear differential equation.
\end{exercise}

\subsection{Cyclic Vectors and Katz's Theorem}
In order to convert first order linear systems of rank $n$ into linear differential equations of  order $n$ in a single variable we need the notion of a cyclic vector. 
\begin{definition}
	An $R[\Delta]$-module $V$ is \emph{cyclic} if and only if there exists some $v \in V$ such that $V = R[\Delta]\cdot v$. 
	Such a vector $v \in V$ where $V = R[\Delta]\cdot v$ is called a \emph{cyclic vector}.
\end{definition}

In the case that $\Delta = \lbrace \partial \rbrace$ and $V \cong R^n$ a $R[\partial]$-module a vector $v \in V$ is cyclic if and only if 
$$ v, \partial(v), \ldots, \partial^{n-1}(v) $$
form a basis for $V$. 
This is probably the most important case. 
Before proving such cyclic vectors exist, lets take a moment to realize our goal reducing a first order linear system of rank n to an order n linear differential equation in a single dependent variable. 

Following our procedure we set $v_0 = v$ and $v_i = \partial^i(v)$ which gives use $\partial(v_i) = v_{i+1}$ for $0 \leq i \leq n-2$ and then $\partial(v_{n-1}) = b_0v_0 +b_1v_1 + \cdots + b_{n-1} v_{n-1}$ for $b_i \in R$ and we get the linear systems 
$$ Y' = BY, \qquad B=\begin{pmatrix} 0 & 1 & 0&\cdots & 0 \\
0 & 0 & 1 &\cdots & 0 \\
\vdots & \vdots &\vdots & \ddots & \vdots \\
0& 0 & 0 & \cdots & 1 \\
b_0 & b_1 & b_2 & \cdots & b_{r-1} \\
\end{pmatrix}$$
which gives the linear differential equation 
$$ y^{(n)} = b_0 y+b_1y' + b_2 y''+ \cdots + b_{n-1}y^{(n-1)}.$$

The following Theorem can be found in \cite{Katz1987} (which is just five pages including citations). 
\begin{theorem}[Katz's Theorem]\label{T:katz}
	Let $(R,\partial)$ be a $\partial$-ring with $t\in R$ satisfying $\partial(t)=1$.
	Let $V$ be a free $R$-module of finite rank $n$ which has the structure of a $R[\partial]$-module. 
	Then if $R$ is local, and $(n-1)!$ is invertible in $R$ then $V$ admits a cyclic vector of the form
	$$ v = \sum_{j=0}^{n-1} \frac{(t-a)^j}{j!} \sum_{i=0}^j { j \choose i } D^i(e_{j-i}) $$
	where $a \in R^{\partial}$ and $e_1,\ldots,e_n$ is the standard elementary basis for $R^n$.
\end{theorem}
We will prove this for $R=\CC[[t]]$. 

\begin{lemma}\label{L:katz}
	Let $V=\CC[[t]]^{\oplus n}$ be a $D$-module. 
	\begin{enumerate}
		\item Let $h_0,\ldots,h_{n-1} \in V$ be horizontal (i.e. suppose $\partial(h_j)=0$). 
		Consider  $v=\sum_{j=0}^n \dfrac{t^j}{j!}h_j.$
		The vector $v$ is cyclic. 
		\item For each $v_0\in V$ consider the system 
		\begin{equation}\label{E:deligne-equation}
		\begin{cases}
		v \equiv v_0 \mod tV \\
		\partial(v) =0 
		\end{cases}
		\end{equation}
		The element  $v:= e^{-t\partial}v_0 = \sum_{j\geq 0}(-1)^j \frac{t^j}{j!}\partial^j(v_0)$
		is $t$-adically convergent and is the unique element in $V$ satisfying \eqref{E:deligne-equation}.
	\end{enumerate}
\end{lemma}
\begin{proof}
	Taking derivatives we have 
	\begin{align*}
	v=& h_0 + t h_1 + \frac{t^2}{2!} h_2 + \cdots + \frac{t^{n-1}}{(n-1)!}h_{n-1}\\
	\partial(v) =& h_1 + t h_2 + \frac{t^2}{2!} h_3+ \cdots + \frac{t^{n-2}}{(n-2)!}h_{n-2} \\
	\vdots & \\
	\partial^{n-1}(v) =& h_{n-1}
	\end{align*}
	starting from the bottom of the list and going up one can see linear independence as they each introduce a new $h_j$.
	
	To prove the second part we just compute the derivative of $v$ and expand using the product rule term by term. 
	For uniqueness, suppose that $w$ is another solution. 
	One then has $w=v+tu$ for some $u\in V$. 
	We then get $u+t\partial(u)=0$, by $0 = \partial(w) = \partial(v) + \partial(tu) = \partial(tu)$. 
	We can expand $u$ in a power series to get $u(t) = \sum_j a_j t^j$ and we find that $\partial(u(t)) = u^{\partial}(t) +u'(t)$ which gives $a_0=0$ and then $a_j^{\partial}+(j+1)a_{j+1} +a_{j+1}=0$. 
	This allows us to conclude all of the $a_j=0$ inductively. 
\end{proof}

The proof of the following theorem will use Nakayama's Lemma which can be found in Atiyah-MacDonald \cite[pg 21]{Atiyah2016}.
\begin{proof}[Proof of Katz's Theorem for Formal Power Series]
	Let $V = R^n$. 
	Let $e_0,\ldots,e_{n-1}$ be a basis, then it is a basis modulo $tV$. 
	Hence by Nakayama, $\widetilde{e}_j:=e^{-t\partial }e_j$ is also a basis for $V$ since it is a basis modulo $tV$. 
	Furthermore, by the Lemma $\partial(\widetilde{e}_j)=0$. 
	We now apply part one of Lemma~\ref{L:katz} to get 
	\begin{align*}
	\sum_{j=0}^{n-1} \frac{t^j}{j!}\widetilde{e}_j =& \sum_{j=0}^{n-1}\frac{t^j}{j!} \sum_{i\geq 0}\frac{t^i}{i!} \partial^i(e_j) \\
	=& \sum_{j=0}^{n-1}\sum_{i\geq 0}\frac{t^{i+j}}{i!j!} \partial^i(e_j).
	\end{align*}
	We can trim this down (using Nakayama again). 
	If $v$ is cyclic then $v+t^n c$ is also cyclic. 
	The ``large'' power $t^n$ ensures that it remains a basis after $n$ derivatives. 
	This allows us to kill off terms with $j+k \geq n$. 
	Hence 
	$$ \sum_{j=0}^{n-1}\sum_{i=0}^{n-1-j}(-1)^i \frac{t^i}{i!}\partial(e_j) $$
	gives a cyclic vector.
\end{proof}

%%%%%%%%%%%%%%%%
\subsection{Local Exponents}
%%%%%%%%%%%%%%%%
Consider a first order meromorphic system of rank $n$ on $\PP^1$ given by 
$$ Y' = A(t) Y, \qquad A(t) \in M_n(\CC(t)).$$
The eigenvalues of residue matrices play such an important role in the local behavior of solutions of differential equations we give them a name.
\begin{definition}
	Let $R= \Res_{t=t_0}(A(t)) \in M_n(\CC)$ be a residue at $t=t_0$.
	An eigenvalue of $R$ is called a \emph{local exponent} of the system at $t=t_0$.
\end{definition}
If $S=\lbrace s_1,s_2,\ldots,s_m \rbrace$ is the polar locus for a differential equation of rank $n$ with eigenvalues $\rho_1(s_j),\rho_2(s_j),\ldots, \rho_n(s_j)$ at the points $j$ we will often write down a so-called \emph{Riemann table} in the form
$$\begin{array}{cccc}
	s_1 & s_2 & \cdots & s_m \\
	\hline \hline 
	\rho_1(s_1) & \rho_1(s_2) & \cdots & \rho_1(s_m) \\
	\rho_2(s_1) & \rho_2(s_2) & \cdots & \rho_2(s_m) \\
	\vdots & \vdots & \ddots &\vdots \\
	\rho_n(s_1) & \rho_r(s_2) & \cdots & \rho_n(s_m)
\end{array}$$

We will now go on to show that solutions of $Y(t)$ locally have the form $t^{\rho} Z(t)$ for $\rho$ ``non-resonant'' local exponents. 
We say that an eigenvalue $\rho$ of $R$ is non-resonant provided there doesn't exist another eigenvalue $\mu$ of $R$ such that $\rho-\mu \in \ZZ$. 

\subsection{Theta Operator and Indicial Equations}
Consider a linear differential equation in one variable  
$$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) =0 $$
which is formally Fuchsian at $t=0$ so that 
$$b_j(t) := t^{n-1}a_j(t) \in \CC[[t]]. $$
We wish to derive an equation for the local exponents of this equation at $t=0$. 
To do this it will be convenient to write our operator (which we view as an element of the Weyl algebra)
$$L = \partial^n  + a_{n-1}(t) \partial^{n-1} + \cdots + a_0(t) \in \CC[[t]][\partial]$$
in terms of the \emph{theta operator}
$$\theta = t \partial_t \in \CC[[t]][\partial].$$
We remark that this operator is called the \emph{Euler operator} in \cite{Iwasaki1991} and is denoted by $\delta$.

The following basic identities will be useful. 
\begin{exercise}
	In this problem $\partial = \partial_t$. 
	Show that 
	\begin{enumerate}
		\item $t^n\partial^n = \theta(\theta-1)\cdots(\theta - n+1)$
		\item $\theta(t^{\rho} f) =t^{\rho}(\theta+m)f$. 
		\item $(\theta+\rho)t^j = (j+\rho)t^j$ for all $j\geq 0$
	\end{enumerate}
\end{exercise}
If we let $M = t^n L$ then we see that 
 $$ M = \sum_{j=0}^n a_j t^{n-j}t^j \partial^j = \sum_{j=0}^n b_j \theta(\theta-1) \cdots (\theta -j +1).$$
For concreteness we write out the order two case.
\begin{example}\label{E:order-two-theta}
	We have $L = a_0 + a_1 \partial + \partial^2$ and $M = b_0 + b_1\theta +\theta(\theta-1)= b_0 + (b_1-1)\theta + \theta^2$. 
	We can be even more explicit with $b_0 = t^2a_0$ and $b_1 = ta_1$ so that 
	 $$M = t^2a_0 + (ta_1-1)\theta + \theta^2.$$
\end{example}
One also has a cute form of the hypergeometric differential equation.
\begin{exercise}
	Check that the hypergeometric equation has the form 
	$$ \theta(\theta+1-c)y -t(\theta+a)(\theta+b)y =0. $$
\end{exercise}


Now in order to derive the indicial equation for the local exponents of a linear differential operator we will seek solutions of $My=0$ in the form 
 $$ f(t) = t^{\rho} \sum_{j=0}^{\infty} c_j t^j $$
and conclude a necessary identity about the exponent $\rho \in \CC$. 
To proceed we write each $b_i(t)$ for $1\leq i \leq n$ as $b_i(t) = \sum_{j=0}^{\infty} b_{ij}t^j.$.
We then just proceed with a computation
\begin{align*}
Mf =& \left( \sum_{i=0}^n \sum_{j=0}^{\infty} b_{ij}t^j \theta^i \right) \left( t^{\rho} \sum_{k=0}^{\infty} c_k t^k \right) \\
=&t^{\rho} \sum_{i=0}^n \sum_{j=0}^{\infty} b_{ij} t^j (\theta+\rho)^i \sum_{k=0}^{\infty} c_k t^k \\
=&t^{\rho} \sum_{i=0}^n \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} b_{ij} c_k t^j (k+\rho)^i  t^{k+j} \\
=& t^{\rho} \sum_{m=0}^{\infty}\left( \sum_{i=0}^n \sum_{j=0}^m b_{ij} c_{m-j}(\rho+m-j)^i\right)t^m.
\end{align*}
If $Mf=0$ as an element of $\CC[[t]][t^{\rho}]$ then by the linear independence of $t^m$ this gives a system of  equations for each $m$ given by 
 $$   \sum_{i=0}^n \sum_{j=0}^m b_{ij} c_{m-j}(\rho+m-j)^i=0.$$
In the case $m=0$ we can pull out $c_0$ and use that $b_{i0} = b_0(0)$ and the indicial equation.

\begin{theorem}[Indicial Equation]
	If $t^{\rho}f(t)$ is a formal solution of $L$ then $\rho$ is a solution of 
	 \begin{equation}\label{E:indicial-equation}
	  b_0(0) + b_1(0) \rho + \cdots + b_{n-1}(0)\rho^{n-1} + \rho^n =0.
	 \end{equation}
\end{theorem}
Equation~\ref{E:indicial-equation} is called the indicial equation for the differential equation at $t=0$. 
It is an exercise to compute derive the indicial equation at other points. 
The basic idea is to use $\theta = (t-t_0) \dfrac{d}{d(t-t_0)}$ rather than $t\dfrac{d}{dt}$. 
Similarly, for an equation at infinity one needs to change coordinates to $s$ where $t=1/s$ and $-s^2\dfrac{d}{ds}=\dfrac{d}{dt}$.

For a later application to Fuch's relation it will be useful to compute $b_{n-1}(0)$ explicitly the second to top coefficient is always the sum of the roots:
 $$ b_{n-1}(0) = -\rho_1 -\rho_2 - \cdots - \rho_n.$$
This formula is related to a residue and we will later apply the Residue theorem.
\begin{corollary}\label{C:residue-formula}
	 $ b_{n-1}(0) = \res_{t=0}( a_{n-1}(t)dt ) - {n \choose 2}.$
\end{corollary}
\begin{proof}
One sees that 
\begin{align*}
M =& \sum_{j=0}^n a_j t^{n-j} \theta(\theta-1)\cdots(\theta-j+1) \\
=& \theta^n + (-1-2-\cdots-(n-1))\theta^{n-1} + ta_{n-1}(t) \theta^{n-1} + \cdots \\
=& \theta^n + \left( a_{n-1}(t)t - {n \choose 2} \right)\theta^{n-1} + \cdots  
\end{align*}
and hence the statement follows.
\end{proof}

\begin{exercise}
	For this problem consider a Fuchsian differential equation 
	 $$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) y =0,$$
	where $a_j(t) \in \CC(t)$ for $0\leq j \leq n-1$.
	Let $S\subset \PP^1$ be the polar locus of this equation. 
	\begin{enumerate}
		\item Show that the general indicial equation at $t=t_0\neq \infty$ takes the form
		 $$ \sum_{j=0}^n c_j \rho(\rho-1)\cdots(\rho-j+1) =0 $$
		where $c_j = \lim_{t\to t_0} (t-t_0)^{n-j}a_j(t).$
		\item Show that if $t_0=\infty \in S$ then the indicial equation becomes 
		 $$ \sum_{j=0}^n (-1)^j c_j \rho(\rho-1)\cdots(\rho-j+1) =0 $$
		 where $c_j = \lim_{t\to \infty} t^{n-j} a_j(t)$. 
	\end{enumerate}
\end{exercise}
Using the formulas above one now has a more systematic approach to computing the local exponents for the hypergeometric functions. 
\begin{exercise}
	Compute the local exponents of the hypergeometric equation 
	 $$ y'' + \dfrac{(a+b-1)t -c}{t(1-t)}y' + \dfrac{ab}{t(1-t)} y =0, $$
	for each $t_0 \in S = \lbrace 0,1,\infty\rbrace$ using the indicial formulas. 
\end{exercise}


\subsection{Fuchs' Relation}

We now state Fuchs relation which tells us more about the local exponents of the hypergeometric function (for example).
\begin{theorem}[Fuchs' Relation]\label{T:fuchs-relation}
Consider a Fuchsian linear differential equation on $\PP^1$ of the form 
$$ y^{(n)} + a_{n-1}(t) y^{(n-1)} + \cdots + a_0(t) y =0.$$
Let $S \subset \PP^1$ denote the polar locus of the differential equation and assume that $\infty \in S$. 
If $\rho_1(a),\ldots,\rho_n(a)$ denote the local exponents at $a \in S$ then
$$ \sum_{a\in S} \left( \rho_1(a) + \cdots + \rho_n(a) \right) = (\#S-2){n \choose 2}.$$	
\end{theorem}

Fuchs' relation mposes an extra constraint on the possible eigenvalues of matrices. 
Note that in the case $n=2$ and $\# S=3$ (the hypergeometric case) we have 
$$ \sum_{a\in S}\left( \rho_1(a) + \rho_2(a) \right) =1.$$
\begin{exercise}
	Check that the exponents in the Riemann table in Exercise~\ref{E:exponents-for-hypergeometric} satisfy Fuchs' relation.
\end{exercise} 
We now give the proof of Fuchs' relation.

\begin{proof}
	We know that for $a \in S\setminus \infty$ we have 
	 $$ \rho_1(a) + \rho_2(a) + \cdots + \rho_n(a) = {n \choose 2} - \res_{t=a}(a_{n-1}(t)dt), $$
	similarly for $a =\infty$ we have 
	 $$ \rho_1(\infty) + \rho_2(\infty) + \cdots + \rho_n(\infty) = -{n \choose 2} - \res_{t=\infty}(a_{n-1}(t)dt).$$
	Using the residue formula (Theorem~\ref{T:residue-theorem}) we have 
	\begin{align*}
	  0=&- \sum_{a \in S} \res_{t=a}( a_{n-1}(t)dt) \\
	  =& \sum_{a\in S\setminus \infty}\left( \rho_1(a)+\rho_2(a) + \cdots + \rho_n(a) - {n\choose 2} \right) \\
	  & + \rho_1(\infty) + \rho_2(\infty) + \cdots + \rho_n(\infty) + {n \choose 2} \\
	  &= - (\#S -2){n \choose 2} + \sum_{a\in S}\left( \rho_1(a)+\rho_2(a) + \cdots + \rho_n(a) \right), 
	  \end{align*}
	  which proves the result. 
\end{proof}

\subsection{Local Solutions of Exponent $\rho$}
Consider a first order system of rank $n$ which is formally Fuchsian at $t=0$. 
We will write 
$$ Y' = \frac{A(t)}{t} Y, \qquad A(t) \in M_n(\CC[[t]]).$$
Note that this system is equivalent to $\theta(Y) = A(t) Y$ where $\theta$ operators component-by-component. 
We will let expand $A(t)$ in a power series
$$ A(t) = A_0 + A_1 t + \cdots, $$
and then consider power series solutions of the form $Y(t) = t^{\rho}Z(t)$ and develop $Z(t)$ as a power series 
$$ Z(t) = Z_0 + Z_1 t + \cdots. $$
We then find that 
$$ \theta(Y) = \theta( t^\rho Z) = t^{\rho}(\theta+\rho)Z, \quad AY =  t^\rho A Z, $$
which leads us to  
\begin{align*}
(\theta + \rho)Z(t) &= \rho Z_0 + (\rho Z_1 + Z_1)t + (\rho Z_2 + 2 Z_2)t^2 +\cdots, \\
A(t) Z(t) &= A_0 Z_0 + (A_1Z_0+A_0Z_1)t + (A_2 Z_0 + A_1Z_1+A_0 Z_2)t^2+\cdots 
\end{align*}
which when we equate coefficients tells us that $Z_0$ is an eigenvector of $A_0$ with eigenvalue $\rho$ and that for $n\geq 1$ we have the equation
$$ nZ_n + \rho Z_n = A_0 Z_n + A_1 Z_{n-1} + \cdots + A_n Z_0.$$
This equation allows us to solve inductively as long as $(\rho+n)$ is not an eigenvalue of $A_0$ for $n\geq 1$ since we have the expression
$$( \rho + n + A_0) Z_n = A_1 Z_{n-1} + \cdots + A_n Z_0,$$
and $\rho+n$ not being an eigenvalue puts $(A_0 - \rho-n)$ invertible. 
The fancy word for this is that $\rho+n$ is in the resolvent set of the operator $A_0$ (the resolvent set of a linear operator $L$ is precisely the set of $\lambda$ such that $\lambda-L_0$ is invertible). 
We will omit the proof of convergence. 
This has to do with estimating the operator norm of $(x-A_0)^{-1}$ for $x$ in the resolvent set. 

This proves the following.
\begin{theorem}
	Consider the formal Fuchsian system 
	\begin{equation}\label{E:formal-fuchsian}
	Y' = \frac{A(t)}{t} Y, \qquad A(t) \in M_n(\CC[[t]]).
	\end{equation}
	Let $A_0\in M_n(\CC)$ be the residue of $A(t)/t$ at $t=0$ and let $\rho$ be an eigenvalue such that $\rho+n$ is not an eigenvalue of $A_0$ for any integer $n\geq 1$. 
	Then \eqref{E:formal-fuchsian} admits a formal solution $Y(t) \in \CC[[t]]^n$ of the form 
	$$ Y(t) = t^{\rho}(Y_0 + Y_1 t + \cdots ) $$
	where $Y_0$ is an eigenvector of $A_0$. 
	Moreover the series is convergent is the series for $A(t)$ is. 
\end{theorem}
Note that even the resonant case where there are eigenvalues $\rho$ and $\mu$ with  $\rho-\mu \in \ZZ$ then still one of these admits a solution of the type above. 
One just needs some eigenvalue such that there is no positive integer that gives another. 
If $\rho$ and $\mu$ are equal then we don't need to worry about this. 
If $\rho-\mu$ is negative then we don't need to worry about this. 
If $\rho-\mu$ is positive then we can switch the role of $\rho$ and $\mu$ and again not worry about this. 

We record the following for later use. 
\begin{theorem}\taylor{Check and add. We might need to add more about formal power series solutions to do Stokes stuff later.}
	More generally, in the non-resonant case one has a fundamental matrix of the form 
	$$ \Phi(t) = \Psi(t) t^{A_0} $$
	where $\Psi(t)$ is a matrix of formal power series. 
	The matrix $\Psi(t)$ is convergent if $A(t)$ is convergent.
\end{theorem}

\subsection{Exponent Shifting}
We record the following useful fact.
\begin{lemma}[Exponent Shifting]\label{L:exponent-shifting}
	The gauge transformation $Y=(t-t_0)^{\mu}\widetilde{Y}$ has the effect of $A(t)/(t-t_0)\mapsto \widetilde{A}(t)/(t-t_0)$ where $\widetilde{A}(t) = A(t)-\mu$.
\end{lemma}
\begin{proof}
	Without loss of generality we can suppose that $t_0=0$ since $\infty$ is invariant under the transfomation $t\mapsto t-t_0$. 
	The system has the form 
	 $$ \theta Y = A(t) Y' $$
	where $A(t) = A_0 + A_1 t + \cdots \in M_n(\CC[[t]])$ and $A_j \in M_n(\CC)$ for $j\geq 0$. 
	Then $\theta(t^{\mu}\widetilde{Y}) = t^{\mu}(\theta+\mu)\widetilde{Y}$ and $A(t)t^{\mu}\widetilde{Y} = t^{\mu}A(t)\widetilde{Y}$ which gives the equation
	 $$ \theta \widetilde{Y} = (A(t)-\mu) \widetilde{Y}.$$
	One can check that $\sigma_p(A_0-\mu) = \sigma_p(A_0)-\mu$ where $\sigma_p(B)$ denotes the eigenvalues of a matrix $B$.
\end{proof}

\subsection[Exponents Determine Equations]{Local Exponents Determine Equations in Hypergeometric Case}
In this subsection we work in the Fuchsian case where $\#S=3$ and rank two. 
In particular we work with single ordinary differential equations of the form
 $$ y'' + a_1(t)y' + a_0(t) y =0 $$
with $a_1(t),a_0(t) \in \CC(t)$ since all rank two order one systems are equivalent to order two differential equations in one variable.

\begin{theorem}\label{T:exponents-determine-equation}
Let $S = \lbrace t_1,t_2,t_3 \rbrace \subset \PP^1$ and fix a table of local exponents satisfying Fuchs' relation  
$$\begin{array}{ccc}
t_1& t_2 & t_3 \\
\hline \hline \alpha & \beta & \gamma \\
\alpha' & \beta' & \gamma' 
\end{array}.$$
There exists a unique $a_1(t),a_2(t) \in \CC(t)$ such that 
\begin{equation}\label{E:fixed-exponents}
  y''+a_1(t) y' + a_0(t) y =0
 \end{equation}
is a Fuchsian differential equation with polar locus $S$ and exponents as given in the table. 
\end{theorem}
\begin{proof}
	The proof is a partial fraction expansion computation and follows \cite[Chapter 2, Proposition 1.1.1]{Iwasaki1991} closely.
	We assume without loss of generality that $t_3=\infty$. 
\end{proof}

\begin{theorem}
	The equation~\ref{E:fixed-exponents} reduces to the hypergeometric differential equation.
\end{theorem}
\begin{proof}
Using a M\"obius transformation we can transform $(t_1,t_2,t_3)$ to $(0,1,\infty)$ giving a new order two differential equation with $S = \lbrace 0,1,\infty\rbrace$.
This gives a new exponent table
	$$\begin{array}{ccc}
	t_1 & t_2 & t_3 \\
	\hline \hline \alpha & \beta & \gamma \\
	\alpha' & \beta' & \gamma' 
	\end{array} \mapsto \begin{array}{ccc}
		0& 1 & \infty \\
		\hline \hline \alpha & \beta & \gamma \\
		\alpha' & \beta' & \gamma' 
	\end{array}.$$
We next apply the exponent shifting lemma (Lemma~\ref{L:exponent-shifting}).
Making the gauge transformation $y = t^{-\alpha}(t-1)^{-\beta}\widetilde{y}$ to tranform the exponent table again to
 	$$\begin{array}{ccc}
 	0& 1 & \infty \\
 	\hline \hline \alpha & \beta & \gamma \\
 	\alpha' & \beta' & \gamma' 
 	\end{array} \mapsto \begin{array}{ccc}
 0& 1 & \infty \\
 \hline \hline 0& 0 & \gamma +\alpha +\beta \\
 \alpha' -\alpha& \beta'-\beta & \gamma' +\alpha+\beta
 \end{array}.$$
 We then just relabel the exponents:
   \begin{align*}
   a &= \gamma +\alpha +\beta\\
   b &= \gamma' +\alpha+\beta\\
   1-c &= \alpha' -\alpha 
   \end{align*}
  and finally Fuchs' relation (Theorem~\ref{T:fuchs-relation}) forces $\beta'-\beta=c-a-b$.
  Since the exponents determine the equation (Theorem~\ref{T:exponents-determine-equation}) the transformed equation must be a hypergeometric equation with the given exponents. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Riemann-Hilbert]{Connections and Riemann-Hilbert Correspondences}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
I started writing this section to present a proof of Plemelj's theorem, which at the time  (and the 70 years that follows) was thought to be a solution to Hilbert's 21st problem in a modern form. 
To do this we need to introduce vector bundles with connections which then will later be used again for constructing isomonodromic deformations. 

The basic strategy of Plemelj's proof is to glue a bunch of local equations together. 
We now understand this technique to be the part of what is called ``descent theory''. 
This is just a fancy word for ``the theory of gluing things together''.  
In order to glue things we need to define the things that we are gluing. 
These ``things'' are connections on vector bundles $(E,\nabla)$ over a Riemann surface (or complex manifold). 
The word ``connections'' is just a fancy word for ``locally a $D$-module''. 
 
The idea of a connection leads to a bunch of interesting mathematics and physics including the notion of curvature. 
The basic motto is 
\begin{center}
Force = Curvature
\end{center}
Physically, all of the basic forces in the standard model of physics the weak, strong, electromagnetic are all Yang-Mills theories which involve this concept.

We will follow the chapters of Haeflinger and Malgrange the book on $D$-modules  \cite[Chapters III, IV]{Borel1987} which largely follows \cite{Deligne1970}.
There some nice material from the \emph{Holomorphic Foliations and Algebraic Geometry} Summer School in Mathematics in 2019 which has excellent YouTube videos and notes. Viktoria Heu's \href{https://if-summer2019.sciencesconf.org/resource/page/id/1}{Notes} are brief and excellent. 
Also see Frank Loray's second lecture from the same Summer School \href{https://www.youtube.com/watch?v=qCujE4nU8bc}{here}.\footnote{There are actually many great videos of Frank Loray on YouTube if you do a quick search.}


\section{Vector Bundles and Connections}
In this section we define vector bundles and connections. 
They are mainly a global language for linear differential equations and give a formalism in which we can talk about complicated changes of coordinates.
The vector bundle encodes all possible changes of coordinates of the $D$-module and the connection is the derivation part of the $D$-module. 
More precisely it is the equation. 
Most importantly this global language allows us to glue together local information in order to solve (or show we can't solve) Hilbert's 21st problem.

Next given a vector bundle and a connection we get to talk about curvature and things like geodesics. 
A connection is a way to convert a derivative into a $D$-module structure.
If we think of derivatives as vector fields on space, as one does with tangent bundles, connections are telling us how to associate a direction on our manifold to a direction in our vector bundle. 
In particular for each direction one gets a differential equation and solving this differential equations tells us how to move things around in the vector bundle. 
This is just solving an initial value problem in ODEs.
One issue is that while moving around in our base space commutes, it doesn't necessarily translate to an commutative procedure for moving around in the vector bundle. 
This leads to a notion of curvature. 

Consider Figure~\ref{F:curvature}. 
In this picture we have our base space being a sphere $S^2$ and the vector bundle being the tangent bundle itself -- so note in particular that in this setup the tangent bundle is appearing twice: first as an object parametrizing directions and second as the object on which the directions act through covariant derivatives/assigning linear differential equations.
One can see that if we move a vector from the north pole down a longitude, then along a lattitude, then back up to the north pole around a longitude that we arrive with a vector which is displaced from the original one. 
This is what curvature is.
\begin{figure}[h]\label{F:curvature}
	\begin{center}
		\includegraphics[scale=0.5]{curvature.eps}
	\end{center}
	\caption{Curvature of the tangent bundle of $S^2$ associated to the Levi-Civita connection is evidenced by transporting tangent vectors around the the sphere.}
\end{figure}

Finally, while curvature is an interesting concept in itself, the conditions for curvature give interesting differential equations. 
In fact all the gauge theories in particle physics (strong, weak, electro-magnetic), all of the fundamental forces are encoded by curvatures. 
Later, we are going to need curvature to vanish in order for an overdetermined collection of partial differential equations to be well-defined.
This is needed for example in order to derive the Schlesinger equations for isomonodromic flows (this is an equation for equations!).
These conditions are what give rise to the $P_{VI}$, Painlev\'e six.
This is essentially a condition on equality of mixed partial derivatives for solutions of differential equations. 

\subsection{Vector Bundles (for the uninitiated)}
There are two objects that we will often conflate: vector bundles and locally free sheaves. 
Locally free sheaves are essentially modules that we associate to open sets. 
They are modules over rings of holomorphic functions and they ``glue'' together nicely. 
Vector bundles are spaces over another space which have the property that local sections of the vector bundle form a locally free sheaf. 
Later we will conflate the two since for every vector bundle there is a locally free sheaf and conversely, to every locally free sheaf we can construct a vector bundle. 
If you know what these words mean, this section probably isn't for you. 
I'm going to begin an introduction into these two fundamental objects with a discussion of coordinates of free modules. 
I will then extend this idea to describe the data of vector bundles given from two open sets. 
I will then describe the general definition and say a little bit about what it means to be a sheaf. 
After this section I'm going to assume everyone is familiar with these objects since a detailed discussion will lead us two far afield. 
We recommend \cite{Vakil2017} for a more detailed discussion of vector  bundles on schemes.

Let $R$ be a commutative ring and let $E$ be a free $R$-module of rank $n$. 
We have talked about coordinate isomorphisms $\psi: E \to R^{\oplus n}$ given by $v=f_1v_1 + \cdots + f_n v_n \mapsto (f_1,\ldots,f_n)$ where $v_1,\ldots,v_n$ is a basis for $E$ and $f_1,\ldots,f_n\in R$ are called the coordinates. 
The map $\psi$ is called a trivialization. 
We have also talked about how a change in choice of basis for $E$ transforms the coordinates by some element of $\GL_n(R)$. 
The element of $\GL_n(R)$ transitions from one set of coordinates in one basis to another set of coordinates in another basis. 

A vector bundle is sort of like this naive change of coordinates but we have a varying collection of $R_i$ and $E_i$ for $i$ in some index set $I$ and they need to satisfy compatibility conditions.
The $R_i$ are the functions on some open set of some space and the $E_i$ are local sections of the vector bundle. 

\begin{example}[Vector Bundles With Two Charts]
	The data for this is some $E_i$ $R_i$-modules for $i=1,2$ and an additional $R_{12}$-module $E_{12}$.
	
	There are ring homomorphisms 
	$$ \begin{tikzcd}
	R_1 \arrow[r] & R_{12} & \arrow[l] R_2 
	\end{tikzcd}$$
	and morphisms of abelian groups
	$$ \begin{tikzcd}
	E_1 \arrow[r] & E_{12} & \arrow[l] E_2 
	\end{tikzcd}$$
	which respects the module actions. 
	Moreover these have the property that a basis for $E_1$ or $E_2$ induce a basis for $E_{12}$ and hence trivializations for $E_1$ or $E_2$ induce trivializations for $E_{12}$. 
	There are additional glueing properties, but I will state those when I state the official definition.
\end{example}

\begin{example}[Vector Bundles on $\PP^1$]\label{E:charts-on-p1}
	In the case of $\PP^1$ we have two open sets $U_0$ and $U_{\infty}$ which cover $\PP^1 = U_0 \cup U_{\infty}$. 
	\begin{figure}[h]
		\begin{center}
			\includegraphics[scale=0.5]{charts-for-p1.eps}
		\end{center}
		\caption{$\PP^1$ is covered by $U_0$ and $U_{\infty}$.
			Here $U_0$ uses the standard $t$-coordinate and  $U_{\infty}$ uses the coordinate at infinity $s$ given by $s=1/t$. 
			On their intersection $U_0 \cap U_{\infty}$ you can use either coordinate. 
		}
	\end{figure}
	
	In the previous example (with indexing $0,\infty$ instead of $1,2$) we have 
	$$ \begin{tikzcd}
	R_0 =\hol(U_0) \arrow[r] & R_{0\infty} = \hol(U_0\cap U_{\infty}) & \arrow[l] R_{\infty} = \hol(U_{\infty}) 
	\end{tikzcd}$$
	where the maps are ``restriction of the domain''.
	Then to specify a vector bundle one can specify three modules $E_0$, $E_{\infty}$, and $E_{0\infty}$ which are free $R_0,R_{\infty},$ and $R_{0\infty}$-modules respectively. 
\end{example}

While the basic data above is correct it is terrible to define vector bundles this way. 
We want a definition that doesn't depend on the choice of cover, works for all spaces, and allows us to glue local objects together. 
The technical thing we want to say is that a vector bundles $E$ on $X$ is a \emph{locally free sheaf of $\Ocal_X$-modules}.
There are two categories in which we want to formulate this notion: the category of schemes and the category of complex manifolds. 


\begin{itemize} 
	\item ($\Ocal_X$-modules) 
	In the category of complex manifolds for an open set $U$ one has $\Ocal_X(U) = \hol(U)$ the set of holomorphic functions on $U$ and in the category of schemes $\Ocal_X(U)$ is the structure sheaf. 
	
	\item (Sheaves) For $E$ to be a sheaf of $\Ocal_X$-modules we need that $E(U)$ to be a $\Ocal_X(U)$-module for every $U\subset X$ and it needs to satisfy sheaf axioms. 
	Elements of $E(U)$ are called sections over $U$. 
	The first axiom says that if you have bunch of open sets and sections on those open sets that agree on the intersections then there exists a section over the union of the open sets that restricts to each of those sections. 
	The second axiom says that such a lifting is unique: if you have two sections which agree on a collection of open sets that cover the set it is a section over then the two sections must be the same. 
	\item (Locally free of rank $n$)
	Finally, for $E$ to be locally free of rank $n$ that means that for every $x\in X$ there exists some $U$ open containing $x$ and an isomorphism $\psi_U: E(U) \to \Ocal_X(U)^{\oplus n}$. 
\end{itemize}
The nice thing about vector bundles is that they satisfy effective descent. 
This is sort of like the sheaf axiom but for objects of the category themselves. 
If $E_i$ are vector bundles over $U_i$ and they $E_i \vert U_i \cap U_j \cong E_j \vert U_1 \cap U_j$ and these isomorphisms satisfy some compatibility conditions, then there exists a vector bundle over $\bigcup_{i} U_i$.
The correct way of talking about this now is to say that fibered category of vector bundles over the category of spaces you are considering is a stack. 
We aren't going to review this here, this would take an entire class. 
The take-away is that you can build up vector bundles from local data.

Let's do a simple example of a line bundle on $\PP^1$.
A line bundle is just a vector bundle of rank one. 
\begin{example}
	Lets consider the sheaf of holomorphic differentials on $\PP^1$. 
	This is the sheaf we denote by $E = \Omega_{\PP^1}^1$. 
	In our usual coordinates we have 
	$$ E(U_0) = \Ocal_{\PP^1}(U_0) dt, \quad E(U_{\infty}) = \Ocal_{\PP^1}(U_{\infty})ds.$$
	There are trivializations 
	$$ \psi_0\colon E(U_0) \to \Ocal_{\PP^1}(U_0), \quad f(t) dt \mapsto f(t) $$
	$$ \psi_{\infty} \colon E(U_{\infty}) \to \Ocal_{\PP^1}(U_{\infty}), \quad g(s)ds \mapsto g(s) $$
	Both of these trivializations are valid on $E(U_0 \cap U_{\infty})$ and over $U_0\cap U_{\infty}$ we have
	$$ \psi_{\infty}\psi_0^{-1}(1) = \psi_{\infty}(dt) = \psi_{\infty}(\frac{-ds}{s^2}) = \frac{-1}{s^2}\psi_{\infty}(ds) = \frac{-1}{s^2}.$$
\end{example}

In the above example we see that the transition map was given by multiplication by $-1/s^2$. 
It turns out that every line bundle on $\PP^1$ and admits trivializations over $U_0$ and $U_{\infty}$ with transition maps of  the form $f \mapsto -s^{-d} f$ for some integer $d$. 
The integer $d$ characterizes the line bundle up to isomorphism and we call the one with integer $d$, $\Ocal_{\PP^1}(-d)$. 
So for example, $\Omega_{\PP^1} \cong \Ocal_{\PP^1}(-2)$. 
There are a couple interpretation of these line bundles, one being sheaves of meromorphic functions with poles at more order $d$ at infinity. 
The notation is a little wonky too. 
If $U$ is an open subset of $\PP^1$ then to take sections of this vector bundle we write $\Ocal_{\PP^1}(d)(U)$ so the $d$ has nothing to do with the open sets we were plugging in earlier. 
\begin{remark}
	Readers already familiar with algebraic or complex geometry will recognize $-s^{-d} \in \Ocal^{\times}_{\PP^1}(U_0 \cap U_{\infty})$ as a representative of the cohomology class  in $H^1(\PP^1,\Ocal^{\times}) = \Pic(\PP^1)$ given in terms of a \v{C}ech cocycle with two open sets. 
\end{remark}
The comment about line bundles extends to vector bundles. 
The obvious vector bundles we can think of are direct sums of line bundles. 
These have transition matrices which are diagonal of the form $\operatorname{diag}(t^{d_1},t^{d_2},\ldots,t^{d_n}).$
These turn out to be all of them.

\begin{theorem}[Birkoff-Grothendieck]
	All vector bundles on $\PP^1$ are isomorphic to 
	$$ \Ocal_{\PP^1}(d_1) \oplus \cdots \Ocal_{\PP^1}(d_n) $$
	for some $d_1,d_2,\ldots,d_n \in \ZZ$.
\end{theorem}
\begin{proof}[Proof Reference and Sketch]
	This can be found in \cite{Hazewinkel1982} which works algebraically over a general ring. 
	By GAGA, that every algebraic vector bundle on $\PP^1$ (considered as a scheme) is equivalent to proving every holomorphic vector bundles on $\PP^1$ has this form.
	
	The prove relies on a matrix factorization theorem of Birkoff.
	If $A \in \GL_n(\CC[t,t^{-1}])$ then there $BAC=D$ where $C=C(t)$ and $B=B(t^{-1})$  have entries in $\CC[t]$ and $\CC[t^{-1}]$ respectively and $D$ is diagonal with each entry of the form $t^s$ for some integer $s$.
	
	One works on two charts of $\PP^1$ and then factors the transition data using this theorem. 
	This is a \v{C}ech cocycle classifying the vector bundle and has the appropriate diagonal form.
	This proves the results.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Systems vs Connections}
%%%%%%%%%%%%%%%%%%%%%%%
Consider a vector bundle of rank $n$ with connection $(E,\nabla)$ on complex manifold $X$.
In the special case that $E\cong \Ocal_X^{\oplus n}$ we often speak of a the connection as a \emph{system} since there is really no extra global information. 
For non-compact Riemann surfaces all connections are really just systems.
\begin{theorem}[Grauert-R√∂hrl Theorem]
	Every holomorphic vector bundle of rank $n$ on a non-compact Riemann surface $X$ is isomorphic to $\Ocal_X^{\oplus n}$.
\end{theorem}
\begin{proof}[Proof Reference and Sketch]
	This is \cite[Theorem 30.4]{Forster1981}.
	The proof is by induction on the rank of the vector bundle. 
	They show first that line bundles are trivial using the so-called Runge approximation theorem.
	Then they do an explicit computation with \v{C}ech cocycles after appling the inductive hypothesis to reduce the transition functions to unipotent matrices. 
	They then reduce further arguing about an additive \v{C}ech cocycle.
\end{proof}

This will mean that we don't need to worry about global information coming from the vector bundle when trying to establish a Riemann-Hilbert correspondence for log-connections on the projective line minus a finite set of points. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Babymost case: Covariant Derivatives and Connections Associated to ODEs}
%%%%%%%%%%%%%%%%%%%%%%%
Since connections can get abstract, before proceedings with the full definition, I'm going to explain what everything is for an ODE. 
The main idea is that we can convert ``$Y$ is a solution of a differential equations' into ``$Y$ is horizontal for a connection''.
$$
\dfrac{dY}{dt}=B(t)Y \quad \iff \quad \nabla_{\frac{\partial}{\partial t}}(Y) =0.
$$
The covariant derivative in this example is the $\CC$-linear operator 
$$ \nabla_{\frac{\partial}{\partial t}} = \dfrac{\partial}{\partial t} - B(t).$$
If, say, $B(t)$ is a holomorphic on $U$ some neighborhood of a points in $\PP^1$ then this defined an operator $R^n \to R^n$ whree $R=\hol(U)$.
The connection in this situation is a map
$$ \nabla = d-B(t)dt $$
where $d$ is the exterior differential acting on column vectors in $R^n$ and $-B(t)dt$ is a matric of differential 1-forms. 
This defined a map $\nabla: R^n \to \Omega^1_{\PP^1}(U) \otimes R^n$.
Explicitly 
$$ \nabla \begin{pmatrix}
y_1 \\
y_2\\
\vdots \\
y_n
\end{pmatrix} = \begin{pmatrix}
dy_1 \\
dy_2\\
\vdots \\
dy_n
\end{pmatrix} - B(t) \begin{pmatrix}
y_1 \\
y_2\\
\vdots \\
y_n
\end{pmatrix} dt. $$
Sometimes the matrix $-B(t)dt$ is called the connection 1-form and usually denoted by $\omega$ or $A(t)dt$ (so that $A(t)=-B(t)$). 
Before proceeding to the abstract theory we remark that $\nabla$ and $\nabla_{\frac{\partial}{\partial t}}$ are related by the pairing 
$$ \nabla_{\frac{\partial}{\partial t}}(Y) = \langle \nabla(Y), \dfrac{\partial}{\partial t} \rangle.$$
Usually this pairing is just defined differential forms and derivations, i.e. between $\Omega_{\PP^1}^1(U)$ and $T_{\PP^1}(U)$ but if $W$ is a vector of $1$-forms given by we extend the pairing to $W$ by pairing with each entry of $W$.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Connections}
%%%%%%%%%%%%%%%%%%%%%%%
There exists definitions of connections for schemes but for concreteness we will work with complex manifolds.\footnote{I'm really working with ringed spaces here.}
Let $X$ be a complex manifold and let $E$ be a vector bundle on $X$. 
\begin{definition}
	A \emph{connection} on $E$ is a $\CC$-linear map 
	$$ \nabla: E \to \Omega_X^1 \otimes_{\Ocal_X} E $$
	satisfying 
	\begin{enumerate}
		\item For all $f \in \Ocal_X$ and all $s\in E$, $\nabla(fs) = df\otimes s + f\nabla(s).$ 
		\item For all $s_1,s_2 \in E$, we have $\nabla(s_1+s_2)=\nabla(s_1) + \nabla(s_2)$.
	\end{enumerate}
\end{definition}
We now can extract a more general definition of local system.
\begin{definition}
	The space of \emph{horizontal sections} of $(E,\nabla)$ is defined by 
	$$ U \mapsto E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s) =0 \rbrace.$$
\end{definition}
This will play an important role in the Riemann-Hilbert correspondence in the case that $\nabla$ is a so-called integrable connection.

We remark that every connection $\nabla$ can be extended to $\Omega^i_X \otimes E$.
This means that given $\omega \otimes s \in \Omega^i_X \otimes E$ we define
$$ \nabla(\omega \otimes s) = d(\omega) \otimes s + (-1)^i \omega \wedge \nabla(S).$$


\subsection{Christoffel Symbols}
Let $X$ be a complex manifold of dimension $n$ and let $E$ be a vector bundle of rank $n$ on $X$.
Locally (on some open set $U\subset X$) we can fix coordinates $t_1,\ldots,t_m$ of $X$ and a basis $s_1,\ldots,s_n$ of $E$. 
This means that on $U$ we have 
$$ E(U) = \Ocal_X(U)s_1 + \cdots + \Ocal_X(U)s_n,$$
$$ T_X(U) = \Ocal_X(U) \dfrac{\partial }{\partial t_1} + \cdots + \Ocal_X(U) \dfrac{\partial}{\partial t_m},$$
$$ \Omega_X(U) = \Ocal_X(U)dt_1 + \cdots + \Ocal_X(U)dt_m.$$
In each of the above expression the sum are direct -- so each $\Ocal_X(U)$-module is free with the given basis. 
Now to get the Christoffel symbols we can just write down what $\nabla(s_j) \in \Omega^1_X(U) \otimes E(U)$ must look like. 
It must have the form
\begin{equation}\label{E:structure-constants}
\nabla(s_j) = \sum_{i=1}^n\sum_{\alpha=1}^m \Gamma^i_{\ j \alpha} dt_{\alpha} \otimes s_i.
\end{equation}
The Christoffel symbols are just the structure ``constants'' for the connection.
\begin{definition}
	The elements $\Gamma^i_{\ j \alpha} \in \Ocal_X(U)$ are called the \emph{Christoffel symbols} of $\nabla$ with respect to the local basis $s_1,\ldots,s_n$ of $E$ and local coordinates $t_1,\ldots,t_m$ of $X$.
\end{definition}
The is a convenient way to write this down using Einstein notation. 
If we write $t^{\alpha}$ instead of $t_{\alpha}$ then we can write \eqref{E:structure-constants} in the simple form
$$\nabla(s_j) = \Gamma^i_{j\alpha} dt^{\alpha}\otimes s_i.$$
In Einstein summation notation a upper index followed by a repeated lower index implies summation over that variable. 
So, in this expression, there is an implied sum over $\alpha$ and over $i$. 
We will make use of Einstein notation freely. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Covariant Derivatives}
%%%%%%%%%%%%%%%%%%%%%%%
We now given the definition of a covariant derivative.
\begin{definition}
	Let $\theta \in T_X(U)$ be a derivation.
	We define the \emph{covariant derivative associated to $\theta$} to be the operator 
	$$ \nabla_{\theta}\colon E(U) \to E(U), \quad s\mapsto \nabla_{\theta}(s) = \langle \nabla(s),\theta\rangle.$$
\end{definition}
In the above expression the pairing $\Omega_X^1 \times T_X\to \Ocal_X$ (which we can write as $\Omega_X^1\otimes T_X \to \Ocal_X$) is extended to $(E\otimes \Omega_X^1)\otimes T_X \to E\otimes \Ocal_X= E$ by tensoring up to $E$.
All tensors here are over $\Ocal_X$.
Note that this pairing just means that we pair each component of $E\otimes \Omega^1_X$ with a tangent vector and take the element of $E$ obtained from the result. 
Also, since $\nabla$ satisfies a product rule and sum rule we will have 
$$ \nabla_{\theta}(fs) = \theta(f) s + f \nabla_{\theta}(s), \quad \forall f \in \Ocal_x, \forall s \in E,$$
$$\nabla_{\theta}(s_1+s_2) = \nabla_{\theta}(s_1) + \nabla_{\theta}(s_2), \quad \forall s_1,s_2 \in E.$$


In local coordinates, we have an explicit expression for covariant derivatives using Christoffel symbols.
Before proceedding I will make some remarks on notation.
First, derivatives $\dfrac{\partial}{\partial t^{\beta}}$ are ``naturally lowered'' in so Einstein notation and for convenience we often write them as $\partial_{\beta} = \dfrac{\partial}{\partial t^{\beta}}$.
In this notation, with these local coordinates, a general derivative $\theta$ is written as $\theta = a^{\beta}\partial_{\beta}$. 
Also, it is annoying to write $\nabla_{\frac{\partial}{\partial t^\beta}}$ all of the time.
We will write 
$$ \nabla_{\beta} = \nabla_{\partial_{\beta}} = \nabla_{\frac{\partial}{\partial t^{\beta}}}$$
to simplify the notation.
\begin{example}
	Let $t^1,\ldots,t^m$ be local coordinates for $X$ and let $s_1,\ldots, s_n$ be a local basis for $E$. 
	Then if $\theta = \dfrac{\partial}{\partial t^{\beta}}$ we have
	$$\nabla_{\frac{\partial}{\partial t^{\beta}}}(s_j) = \langle \Gamma^i_{\ j \alpha} dt^{\alpha} \otimes s_i, \dfrac{\partial}{\partial t^{\beta}} \rangle 
	= \Gamma^i_{\ j \alpha} \delta^{\alpha}_{\ \beta} s_i 
	= \Gamma^i_{\ j \beta} s_i.$$
	In particular if $s = f^js_j\in E$ then 
	\begin{align*}
	\nabla_{\beta}(f^js_j) &=\partial_{\beta}(f^j)s_j + f^j \nabla_{\beta}(s_j) \\
	&= \partial^{\beta}(f^j)s_j + f^j \Gamma^i_{\ j \beta} s_i\\
	&= \left(\partial^{\beta}(f^j) + f^i \Gamma^j_{\ i \beta} \right) s_j 
	\end{align*}
	
	
	A general $\theta$ we can write as $\theta = a^{\beta}\partial_{\beta}$ and
	$$\nabla_{\theta}(s_j) = \nabla_{a^{\beta} \partial_{\beta}}(s_j) = a^{\beta}\nabla_{\beta}(s_j)=a^{\beta}\Gamma^i_{\ j \beta}s_i. $$
\end{example}

\subsection{Connection 1-forms}
The purpose of this section is the show that after fixing a local basis $s_1,\ldots,s_n$ for $E(U)$, that locally 
$$ \nabla = d + \omega, \quad \mbox{ on $\Ocal_X(U)^{\oplus n}$} $$
where $\omega \in M_n(\Omega_X^1(U))$ is a matrix of $1$-forms.
Before proceeding, I need to explain how to multiply matrices of one forms and how the exterior derivative $d$ works.

\subsubsection{Calculus of Connection 1-forms}
For the purpose of iterating the connection later, we remark that tje exterior algebra $\Omega_X^{\bullet} = \bigoplus_{d=0}^m \Omega_X^{\bullet}$ is a sheaf of skew commutative ring satisfying 
$$\eta_1 \wedge \eta_2 = (-1)^{d_2} \eta_2 \wedge \eta_1, \quad 
\eta_j \in \Omega^{d_j}_{X}.$$
Matrices $\omega,\eta \in M_2(\Omega^{\bullet})$ are then multiplied by using the formula 
$$ \omega \wedge \eta = ( \sum_{l=1}^n \omega_{i l} \wedge \eta_{l j} ),$$
if $\omega = (\omega_{ij})$ and $\eta = (\eta_{ij})$.

One more notational remark before proceeding: If $s_i \in E(U)$ form a local basis we will let $s^i$ denote the dueal basis in $E^{\vee}(U)$ where $E^{\vee}$ is the dual vector bundle. 
It is defined by 
$$E^{\vee}(V) := E(V)^{\vee} = \Hom_{\Ocal_X(V)}(E(V),\Ocal_X(V))$$ 
for $V$ an open subset of $X$. 
The second $\vee$ is just usual $\Ocal_X(V)$-module duality as defined by the last equality.

\begin{exercise}
	Suppose that $t^{\alpha}$ are local coordinates of $X$ and that $s_i$ is a local basis for $E$. 
	We can write $\nabla$ as an element of $\Omega_X^1\otimes \End(E)$ by 
	$$ \nabla = \Gamma^i_{\ j \alpha} dt^{\alpha} \otimes s_i \otimes s^j.$$
	Here $\End(E)$ is the sheaf of $\Ocal_X$-linear maps from $E$ to itself (endomorphisms) and $\End(E) = E \otimes E^{\vee}$.
\end{exercise}

We now explain how to work with $\nabla=d+\omega$ as an element of a Weyl algebra.
The thing to keep in mind here when working with $\nabla =d+\omega$ is that $\omega$ is a matrix over a non-commutative ring i.e. $\omega \in M_n(\Omega_X^{\bullet})$ (so it is like super noncommutative) and $d$ is an operator on this ring. 
This means we need to do computations in a very weird looking Weyl algebra $M_n(\Omega_X^{\bullet})[d]$ where $d$ here is the exterior derivative. 
If $\omega \in M_n(\Omega_X^{\bullet})$ we need to understand how $d\omega$ acts on $\eta\in M_n(\Omega_X^{\bullet})$ when $\omega$ is homogenous. 
The key thing to keep in mind is that  $\omega$ act by wedge-matrix-multiplication:
$$(d\omega)(\eta) = d(\omega\wedge \eta) = d(\omega) \wedge \eta + (-1)^{\deg(\omega)} \omega \wedge d(\eta).$$
This proves the following:
\begin{lemma}[Basic Weyl Algebra Rules]\label{L:exterior-weyl-algebra}
In the Weyl algebra	$M_n(\Omega_X^{\bullet})[d]$ for $\omega \in M_n(\Omega_X^{\bullet})$ homogeneous we have 
	\begin{equation}
	d \wedge \omega = (-1)^{\deg(\omega)} \omega \wedge d + d(\omega).
	\end{equation}
\end{lemma}
This is going to be used when computing our formulas for curvature (Theorem~\ref{T:integrability-conditions}).

\subsubsection{The formula: $\nabla = d +\omega$}

We now verify the claim about the description in local coordinates.
Let $U$ be an open subset for which $s_1,\ldots,s_n$ is a basis for $E(U)$. 
Let $\psi: E(U) \to \Ocal_X(U)^{\oplus n}$ be the trivialization given by $\psi(f^is_i) = f^ie_i$ where $e_i$ is the standard basis vector on $\Ocal_X^{\oplus n}$ (say viewed as column vectors).
The trivialization extends to $E(U) \otimes \Omega_X(U) \to \Ocal_X(U) \otimes \Omega_X^1(U)$ by functorality of the tensor product and we will abusively also denote this isomorphism by $\psi$. 
We now have a square,
$$\begin{tikzcd}[contains/.style = {draw=none,"\in" description,sloped}]
s=f^is_i \arrow[dd,mapsto] \arrow[dr, contains] \arrow[rr,mapsto]& & \nabla(s) \arrow[d,contains] \\ 
& E(U) \arrow[r, "\nabla"] \arrow[d,above,"\psi"]& \Omega_X(U) \otimes E(U)  \arrow[d, "\psi"]\\
\begin{pmatrix}
f^1 \\
\vdots \\
f^n 
\end{pmatrix}\arrow[r,contains]& \Ocal_X(U)^{\oplus n} \arrow[r, "\nabla^{\psi}"] & \Omega_X(U) \otimes \Ocal_X(U)^{\oplus n}
\end{tikzcd}$$
where $\nabla^{\psi}$ denotes the connection in trivialized coordinates. 
Examining the diagram we see that we have 
$$ \psi(\nabla s) = \nabla^{\psi} \begin{pmatrix}
f^1 \\
\vdots \\
f^n 
\end{pmatrix},$$
so it remains to compute what $\psi(\nabla(s))$ is.
We get 
\begin{align*}
\nabla(s) &= df^i\otimes s_i + f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes s_j \\
&\mapsto \psi(\nabla(s))  =df^i \otimes e_i + f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes e_j = \begin{pmatrix}
df^1 \\
\vdots \\
df^n
\end{pmatrix} + \omega \begin{pmatrix}
f^1 \\
\vdots \\
f^n
\end{pmatrix}
\end{align*}
where we have written
$$ f^i \Gamma^j_{\ i \alpha} dt^{\alpha} \otimes e_j= \omega \begin{pmatrix}
f^1 \\
\vdots \\
f^n
\end{pmatrix}, \quad \omega = A_{\alpha} dt^{\alpha}, \quad A_{\alpha} = (\Gamma^j_{\ i \alpha}).$$
We summarize the above discussion with the following lemma.
\begin{lemma}
	If $E$ is a rank $n$ vector bundle on an $m$-dimensional complex manifold and $U \subset X$ is an open subset such that $X$ has local coordinates $t^1,\ldots,t^m$ and local basis $s_1,\ldots,s_n$ then in local coordinates  $\nabla_{\alpha}:\Ocal_X(U)^{\oplus n} \to \Ocal_X(U)^{\oplus n}$ takes the form
	$$ \nabla_{\alpha} = \partial_{\alpha} + A_{\alpha},$$
	where $A_{\alpha} = (\Gamma^j_{\ i \alpha})$ and elements of $\Ocal_X(U)^{\oplus n}$ are viewed as column vectors.
	In Einstein notation
	$$ \nabla_{\alpha} f^i = \partial_{\alpha}(f^i) + \Gamma^i_{\ j \alpha}f^j.$$
\end{lemma}

The matrix of $1$-forms $\omega$ is called the connection 1-form and we record this in a definition environment for those browsing looking for the definition.
\begin{definition}
	The matrix $\omega = A_{\alpha}dt^{\alpha} \in M_n(\Omega_X(U))$ is called a \emph{connection 1-form}.
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Curvature}
%%%%%%%%%%%%%%%%%%%%%%%
We keep our notation as in the previous section.
We will let $E$ be a rank $n$ vector bundle on an $m$-dimensional complex manifold and let $U$ be an open subset of $X$ which admits local coordinate $t^{\alpha}$ for $X$ and a local basis $s_j$ for $E$.

Recall that $\nabla$ tells us how to move on $E$ given movement on the base: to flow from $s_* \in E(U)$ to another point $s$ sufficiently near by along the direction of $\partial_{\alpha}$. 
See figure~\ref{F:parallel-transport} for a picture of paths in an open set $U\subset X$ being lifted to paths in the vector bundle by solving differential equations.
\begin{figure}[h]\label{F:parallel-transport}
	\begin{center}
		\includegraphics[scale=0.5]{parallel-transport.eps}
	\end{center}
	\caption{Movement in the base tells us how to move in the fibers by solving differential equations. The figure shows paths in the space being transported to the vector bundle locally by solving differential equations.}
\end{figure}

We describe these equations.
If $s_* = f^js_j$ then to we just solve 
\begin{equation}\label{E:parallel-transport}
\nabla_{\alpha} \begin{pmatrix} f^1 \\
\vdots \\
f^n  \end{pmatrix} = 0,
\end{equation}
which is just a linear differential equation with only derivatives in $t^{\alpha}$ appearing. 
Note that this is just an equation of the form 
$$ \dfrac{\partial Y}{\partial t^{\alpha}} = -A_{\alpha}(t) Y, $$
which we are very familiar with by now.
The two expressions are related by letting $Y = (f^1,\ldots,f^n)$ and viewing it as a column vector.

There are some natural questions that come up when thinking about this transport of motion on the base to motion on the fiber.
\begin{problem}[Integrability Problem]
	Is it possible to solve all of our equations \eqref{E:parallel-transport} at once (i.e. simultaneously for $\alpha=1,\ldots,m$)? 
\end{problem}
If this is possible we call the connection \emph{integrable}.
\begin{problem}[Curvature Problem]
	Is moving the direction of $t^{\alpha}$ then moving in the direction of $t^{\beta}$ the same as moving in the direction of $t^{\beta}$ then moving in the direction of $t^{\alpha}$?
\end{problem}
If this is the case we call the connection \emph{flat}.

It turns out that flatness and integrability are really the same thing and that this condition is given $\nabla^2=0$ which is equivalent to vanishing the the curvature tensor.
We will now explain.

The integrability issue is the concern that a solution of $\nabla_{\alpha}Y=0$ may not also be a solution of $\nabla_{\beta}Y=0$.
The first equation is imposes a formula for $\partial_{\alpha} Y$ and the second equation imposes a formula for $\partial_{\beta}Y$. 
It is not guaranteed that $\partial_{\alpha} \partial_{\beta}Y = \partial_{\beta}\partial_{\alpha} Y$. 
When we can do this, the equations (or connection) is integrable. 

As lifting paths is really about general derivations/vector fields.
We now formulate this commutation problem more generally.
First recall that if $\theta_1,\theta_2 \in T_X$ then the commutator
$$[\theta_1,\theta_2] = \theta_1 \theta_2 - \theta_2 \theta_1,$$
is also a derivation.
This is part of the fact that $T_X$ is a sheaf of Lie algebras.
The infinitesimal version of the curvature problem (taking the limit over small paths) leads to a ``curvature zero'' condition.
\begin{problem}
	When does $E$ have a well-defined structure of a $T_X$-module via $\nabla$. 
	In other words, when is it the case that for all $\theta_1,\theta_2 \in T_X$ the following equation holds:
	\begin{equation}\label{E:commutation}
	[\nabla_{\theta_1},\nabla_{\theta_2}] = \nabla_{[\theta_1,\theta_2]}?
	\end{equation}
\end{problem}

The failure of  the commutation relation \eqref{E:commutation} is measured by the curvature tensor which we now define.
\begin{definition}
	The map $R_{\nabla}:T_X\otimes T_X \to \End(E)$ given by 
	$$ R_{\nabla}(\theta_1,\theta_2)(s) = \nabla_{\theta_1}(\nabla_{\theta_2}(s)) - \nabla_{\theta_2}(\nabla_{\theta_1})(s) - \nabla_{[\theta_1,\theta_2]}(s) $$
	is called the \emph{curvature tensor}.
\end{definition}
When the context is clear we will just use $R=R_{\nabla}$ so that we don't have to keep writing the subscript $\nabla$.
In what follows we will soon see that $R$ as an alternative description in terms of  $\nabla^2$.
To see that this even makes sense note that $\nabla^2:E \to \Omega^2_X\otimes E$.
Hence $\nabla^2$ can eat two tangent vectors $\theta_1$ and $\theta_2$ and a section $s$ and spit out another section.
The subsequet theorem will prove that for all $s\in E$ and $\theta_1,\theta_2 \in T_X$ we will have 
$$ R(\theta_1,\theta_2)(s) = \langle \nabla^2(s), \theta_1 \wedge \theta_2 \rangle,$$
where the pairing $\langle -,-\rangle$ is between $\Omega_X^2$ and $T_X\wedge T_X$.

\begin{exercise}
	In local coordinates $t^{\alpha}$ and $s_i$ find an expression for $R_{\nabla}(\partial_{\alpha},\partial_{\beta})(s_j)$ in terms of the Christoffel symbols.
\end{exercise}

We now can give our equations for integrability/flatness.

\begin{theorem}[Integrability Conditions]\label{T:integrability-conditions}
	Let $E$ be a vector bundle of rank $n$ on a complex manifold $X$.
	Let $\nabla$ be a connection on $E$.
	The following are equivalent:
	\begin{enumerate}
		\item \label{I:nabla-squared}Iterating $\nabla$ twice is zero: $\nabla^2=0$.
		\item \label{I:vanishing-curvature} The curvature tensor is identically zero: $R_{\nabla}=0$.
		\item \label{I:connection-one-form} For every set of local coordinate so that $\nabla = d+\omega$ locally with $\omega$ a connection one form with respect to these coordinates we have 
		$$ d(\omega) + \omega \wedge \omega =0.$$
		\item \label{I:potentials} For every set of local coordinate so that $\nabla = d+\omega$ with $\omega = A_{\alpha} dt^{\alpha}$ and $A_{\alpha} \in M_n(\Ocal_X(U))$ one has 
		$$ \dfrac{\partial A_{\alpha}}{\partial t^{\beta}}- \dfrac{\partial A_{\beta}}{\partial t^{\beta}} = -[A_{\alpha},A_{\beta}].$$
	\end{enumerate}
	More generally 
	$$d(\omega) + \omega \wedge \omega= G_{\alpha\beta} \ dt^{\alpha}\wedge dt^{\beta}, \quad G_{\alpha\beta} = \frac{1}{2}\left(  \partial_{\alpha} A_{\beta} - \partial_{\beta} A_{\alpha} + [A_{\alpha},A_{\beta}]\right)$$
	 is a local expressions of the curvature $2$-form.
\end{theorem}
\begin{proof}
	We will first prove \eqref{I:vanishing-curvature} if and only if \eqref{I:potentials}.
	Let $U\subset X$ be an open set with coordinates $t^{1},\ldots,t^m$ and where $E(U)$ is has a basis $s_1,\ldots,s_n$. 
	Since every expression of $R(\theta_1,\theta_2)$ can be expressed in terms of $R(\partial_{\alpha},\partial_{\beta})$, we just need to compute $R(\partial_{\alpha},\partial_{\beta})$.
	The following computation is a Weyl algebra computation (meaning everything is viewed as an operator).
	Importantly, for a matrix $A$ and a derivative $\theta$ we have $\theta A = A\theta + \theta(A)$ there $\theta(A)$ denotes the application of $\theta$ to the entries of $A$ and $\theta A$ means the application of $A$ and an operator followed by $\theta$ as an operator.
	\begin{align*}
	R(\partial_{\alpha},\partial_{\beta}) =& \nabla_{\alpha}\nabla_{\beta} - \nabla_{\beta}\nabla_{\alpha} - \nabla_{[\partial_{\alpha},\partial_{\beta}]} \\
	=&\nabla_{\alpha}\nabla_{\beta} - \nabla_{\beta}\nabla_{\alpha} \\
	=&(\partial_{\alpha}+A_{\alpha})(\partial_{\beta} + A_{\beta}) - (\partial_{\beta}+A_{\beta})(\partial_{\alpha} + A_{\alpha}) \\
	=& (\partial_{\alpha} A_{\beta} +A_{\alpha}\partial_{\beta}) + A_{\alpha}A_{\beta} + \partial_{\alpha} \partial_{\beta}- \left( (\partial_{\beta} A_{\alpha} +A_{\beta}\partial_{\alpha}) + A_{\beta}A_{\alpha} + \partial_{\beta} \partial_{\alpha} \right) \\
	=& [A_{\alpha},A_{\beta}] + A_{\alpha} \partial_{\beta}-A_{\beta}\partial_{\alpha} \\
	& + A_{\beta}\partial_{\alpha} + \partial_{\alpha}(A_{\beta}) \\
	& + A_{\alpha} \partial_{\beta} + \partial_{\beta}(A_{\alpha}) \\
	=& \partial_{\alpha}(A_{\beta}) - \partial_{\beta}(A_{\alpha}) + [A_{\alpha},A_{\beta}].
	\end{align*}

	This implies (in coordinates) 
	$$ R(\partial_{\alpha},\partial_{\beta})=\partial_{\alpha}(A_{\beta}) - \partial_{\beta}(A_{\alpha}) + [A_{\alpha},A_{\beta}].$$
	
	Let's see that \eqref{I:connection-one-form} and \eqref{I:potentials} are equivalent. 
	We recall that locally $\omega=A_{\alpha}dt^{\alpha}$. 
	We will compute $d\omega + \omega\wedge \omega$.
	
	In what follows we are going to use the following trick: if $H_{\alpha\beta}$ is an antisymmetric tensor, e.g. there is some free $R$-modules $V$ with a basis $v^{\alpha}$ such that $H_{\alpha\beta} v^{\alpha}\wedge v^{\beta} \in V\wedge V$ then $H_{\alpha\beta} = -H_{\beta\alpha}$ and 
	$$H_{\alpha\beta} = \frac{1}{2}(H_{\alpha\beta} - H_{\beta\alpha} ).$$
	This will be used when we compute coefficients of differential forms ``coordinate tensor'' is not alternating. 
	You can just antisymmetrize.
	If you don't understand what this means now, it should become apparent in the following computation.
	
	Write $\omega = A_{\alpha} dt^{\alpha}$ hence in local coordinates.
	We compute:
	\begin{align*}
	\omega \wedge \omega &= A_{\alpha} \ dt^{\alpha} \wedge A_{\beta}\ dt^{\beta} 
	= A_{\alpha} A_{\beta} \ dt^{\alpha} \wedge dt^{\beta} =\frac{1}{2} (A_{\alpha} A_{\beta} - A_{\beta} A_{\alpha}) \ dt^{\alpha}\wedge dt^{\beta}\\
	d(\omega) &= d (A_{\beta} \ dt^{\beta}) = \dfrac{\partial A_{\beta}}{\partial t^{\alpha}}\ dt^{\alpha} \wedge dt^{\beta} =  \frac{1}{2}\left(\dfrac{\partial A_{\beta}}{\partial t^{\alpha}}-\dfrac{\partial A_{\alpha}}{\partial t^{\beta}} \right )\ dt^{\alpha}\wedge dt^{\beta} 
	\end{align*}
	which gives our desired identity.
	
	We will now show the computation $\nabla^2=0$ in \eqref{I:nabla-squared} equivalent to $d\omega + \omega\wedge \omega=0$ in \eqref{I:connection-one-form}.
	The key idea is to use the relations for the Weyl algebra of the exterior algebra (Lemma~\ref{L:exterior-weyl-algebra}).
	As elements of $R[d]$ where $R = M_n(\Omega_X^{\bullet})$ we have 
	\begin{align*}
	\nabla^2 =& (d+\omega)(d+\omega) = d^2+\omega\wedge d + d\omega + \omega \wedge \omega \\
	&= \omega \wedge d + (d(\omega) - \omega \wedge d ) + \omega \wedge \omega \\
	&= d(\omega) + \omega \wedge \omega.
	\end{align*}
	This proves the desired equality.
\end{proof}

%See Theorem~\begin{T:frobenius-integrability}.


\begin{exercise}
	Photons are the force carrying particles for the electro-magnetic force field (changes in force are mediated by the emission of light). 
	In this exercise the speed of light will be $c=1$.
	Space time is encoded by a manifold $X$ and the vector bundle is rank one corresponding to the Lie algebra of $U(1)$. 
	In local coordinates (some chart of spacetime) we let $E=(E_1,E_2,E_3)$ denote a electric field and $B=(B_1,B_2,B_3)$ denote a magnetic field. 
	Both $E$ and $B$ are functions of $(t,x,y,z)$.
	They are encoded in the Faraday tensor $F\in \Omega^2_X$ given by 
	$$F= E_1dx \wedge dt + E_2\ dy \wedge dt + E_3 dz \wedge dt + B_1 dy \wedge dz + B_2 dz \wedge dx + B_3 dx \wedge dy.$$
	If we order the variables $(x^0,x^1,x^2,x^3)=(t,x,y,z)$ then Faraday tensor is $F_{\mu\nu} = \partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}$ is  or as an antisymmetric matrix is
	$$(F_{\mu\nu})=\begin{bmatrix}
	0     &  E_1 &  E_2 &  E_3 \\
	-E_1 &  0     & -B_3   &  B_2    \\
	-E_2 &  B_3  &  0     & -B_1  \\
	-E_3 & -B_2   &  B_1  &  0
	\end{bmatrix}.$$
	Since we are in rank one everything commutes and there is nothing really interesting to say about the curvature equations. 
	Maxwell's equations become $dF=0$.
\end{exercise}

I'm tempted to put an exercise about Yang-Mills equations here but haven't done so. 
Perhaps you should just google these now and see all of these curvature tensors appearing everywhere.

\section{Plemelj's Construction}

\section{Riemann-Hilbert Correspondences}\label{S:rhcs}
By \emph{a} Riemann-Hilbert correspondence we will mean a theorem which gives some equivalence between a category of differential equations of some flavor with a category of representations of fundamental groups of some flavor. 
This may not even be a functor but just a bijection of particular sets of some property. 

The general strategy for a functorial version is to have some category of connections or differential equations on some space $X$ which we will call $\Conn(X)$ (e.g. holomorphic connections, Fuchsian connections, holomorphic systems, Fuchsian systesm, holonomic $D$-modules), a category of local systems $\LocSys(X)$ (essentially solutions of the differential equations, perverse sheaves), and a category of representations of the fundamental group $\Repn(X)$. 

The idea is then to pass from differential equations to representations through ``local systems''.
\begin{equation}
\Conn(X) \cong \LocSys(X) \cong \Repn(X) .
\end{equation}

Before going any further we describe what a local system is. 
This conversation continues in section \ref{S:rhc-strat} which the reader should feel free to skip immediately to on the first reading. 


%%%%%%%%%%%%%%%%%%%%%%%
\section{Local Systems and Representations of $\pi_1(X)$}
%%%%%%%%%%%%%%%%%%%%%%%
In this section we are going to introduce the notion of a local system and then prove that the category of local systems on $X$ is equivalent to the category of finite dimensional representations of the fundamental group. 

Along the way we are going to work out what happens with the holomorphic differential equations on $\PP^1\setminus S$ where $S$ is a finite collection of points. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Local Systems}
%%%%%%%%%%%%%%%%%%%%%%%
In what follows we will let $X$ be a topological space. 
For a ring or abelian group $K$ we will let $\underline{K}_X$ be the sheaf associated to the presheaf  constant presheaf defined by 
$$ U\mapsto F(U) = \begin{cases}
K, & U\neq \emptyset ,\\
0, & U = \emptyset 
\end{cases}$$
The presheaf is not a sheaf because it is possible for there to be two disjoint open sets $U_1$ and $U_2$ which means that any element $a \in F(U_1)$ and $b\in F(U_2)$ agree on their restriction but don't lift to a common element in $F(U_1 \cup U_2)$.
This is fixed by allowing for elements ``like'' $a\oplus b \in \underline{K}_X(U_1 \cup U_2)$  so that the sheaf axioms are satisfied. 
On topological spaces (as opposed to general sites) then $\underline{K}_X(U) = \operatorname{Cont}(U, K)$, the collection of continuous map from $U$ to $K$ where $K$ is given the discrete topology.

\begin{definition}
	Let $X$ be a topological space and let $K$ be a field. 
	A \emph{$K$-local system} over $X$ is a sheaf $L$ valued in finite dimensional $K$-vector spaces such that $L \cong \underline{K}_X$ locally. 
\end{definition}
This forms a category and morphisms are morphism of sheaves of $\CC$-linear vector spaces. 

\begin{exercise}\label{EX:locally-constant}
	Let $X$ be a topological space and let $V$ be an $R$-module for some ring $R$. 
	Show that the constant sheaf $\underline{V}_X$ on $X$ is the same thing as the sheaf  $$U\mapsto\Cont(U,V), \quad U \subset X \mbox{ open }$$ 
	where $\Cont$ denotes continuous maps.
	Here $V$ is given the discrete topology. 
\end{exercise}
The above exercise is important. 
Let $X$ be a topological space and let $f:X\to Y$ be a continuous map to a discrete topological space $Y$.
Then $f^{-1}(\lbrace y\rbrace)$ is open for every $y \in Y$. 
This holds for every $y\in Y$ and in particular the map $f:X\to Y$ is \emph{locally constant}. 

The main example of a local systems comes from solutions of differential equations. 
In fact this example is why the definition even exists. 
Consider the first order holomorphic system defined on $U \subset \CC$, given by 
$$ Y' = A(t) Y, \quad A \in M_n(\hol(U)).$$
For every $V \subset U$ open define  
$$ L(V) = \lbrace Y \in \hol(U)^{\oplus n} \colon Y' = A(t) Y \rbrace.$$
We know that at each $t_0 \in U$ we have an isomorphism 
$$ \lbrace Y \in \CC\langle t-t_0 \rangle^{\oplus n} \colon Y'=AY \rbrace \cong \CC^n $$
since solutions form a finite dimernsional $\CC$-vector space. 
By considering representatives of each basis element in $\CC\langle t-t_0 \rangle^{\oplus n}$ we know that there exists some $V \subset U$ containing $t_0$ where 
$$ L(V) \cong \CC^{\oplus n}.$$
This proves that $L$ is a local system. Note in particular that 
$$ L(V) = \Phi_V \cdot \CC^{\oplus n } $$
where $\Phi_V$ is a fundamental matrix valid on $V \subset U$. 

\subsection{Pushfowards and Pullbacks of Local Systems}
I was tempted to add the following exercise without comment:
\begin{exercise}
	The category $\LocSys(X)$ the category of local systems of finite dimensional $\CC$-vector spaces makes sense and for every morphism of topological spaces $f:X\to Y$ one a functor $f^*:\LocSys(X) \to \LocSys(Y)$.
\end{exercise}
In hindsight, this isn't very nice. 

We are going to need to talk about inverse images of pullback of local systems so I want to say a couple words about inverse images of sheaves in general. 
Let $\Gcal$ be a sheaf on a topological space $Y$. 
For a morphism of topological spaces $f:X\to Y$ we want to define the inverse image sheaf $f^{-1}\Gcal$ (sometimes denoted $f^*\Fcal$) which is the left adjoint of $f_*$. 
Here $f_*$ is the direct image sheaf and it turns sheaves $\Fcal$ on $X$ into sheaves $f_*\Fcal$. 
These direct image sheaves are super easy to describe: given $V \subset Y$ we have $(f_*\Fcal)(V) = \Fcal(f^{-1}(V))$. 
That is it.
The sheaves $f^{-1}\Gcal$, are no so simple. 
Harshorne, for example, defines $f^{-1}\Gcal$ to be the sheaf associated to the presheaf
$$ U\mapsto \varinjlim_{V\supset f(U)} \Gcal(U). $$
Yuck! I think this definition sucks and is hard to work with. 

For every sheaf $\Fcal$ on a topological space $X$, I'm going to introduce a topological space $\overline{\Fcal}$ and a morphism of topological spaces $\pi: \overline{\Fcal} \to X$ that is going to make our life easier. 
This space is called the \emph{espace \'etale} and has the important property that sections of $\pi$ over an open set $U$ correspond to elements of $\Fcal(U)$.
By a section over an open set $U$ we mean 
$$ \Gamma_X(\mathcal{F})(U) :=\lbrace s: U \to \overline{\Fcal} \colon \pi s = \id_U \rbrace. $$
That is $\Gamma_X(\mathcal{F}) \cong \Fcal$ as sheaves. 
I should mention that in the description $s$ is just a continuous morphism of topological spaces. 
The situation is pictured in Figure~\ref{F:section}.
\begin{figure}[h]\label{F:section}
	\begin{center}
		\includegraphics[scale=0.75]{section.eps}
	\end{center}
	\caption{A picture of the espace \'etale for a sheaf $\mathcal{F}$. 
		Pictures is a map $s$ over an open set $U$ such that $\pi s=\id_U$.}
\end{figure}

\begin{definition}
	Given a sheaf $\Fcal$ on a topological space $X$ we define the \emph{espace \'etale} of $\Fcal$ to be the topological space $\overline{\Fcal}$ whose underlying set is 
	$$ \overline{\Fcal} = \coprod_{x\in X} \Fcal_x = \lbrace (x,t) \colon t\in \Fcal_x \rbrace, $$ 
	and whose topology is generated in the following way: for each $s \in \Fcal(U)$ for $U\subset X$ open declare
	$$ W(U, s) = \lbrace  (x,s_x) \colon s_x \mbox{ germ of $s$ at $x\in U$ }\rbrace $$
	to be open and take the topology on $\overline{\Fcal}$ to be the smallest topology so that the $W(U,s)$ are open for every $s\in \Fcal(U)$. 
\end{definition}

\begin{exercise}
	Prove that $\Gamma_X(\mathcal{F})$ and $\Fcal$ are isomorphism as sheaves. 
\end{exercise}

We now turn to the point of introducing this construction. 
Given a morphism of topological spaces $f:X\to Y$ and a sheaf $\Gcal$ on $Y$ we define take define $f^{-1}\Gcal$ to by its espace \'etale $\overline{f^{-1}\Gcal}$ which is just the pullback of the $\overline{\Gcal}\to Y$ to $Y$.
That is 
$$ \overline{f^{-1}\Gcal} = \overline{\Gcal} \times_Y X,$$
where the fiber product is taken in the category of topological spaces. 
Similarly, $f_*\Fcal$ for a sheaf $\Fcal$ on $X$ is the pushout 

\begin{exercise}
	Using this definition of $f^{-1}\Gcal$ show that $(f^{-1},f_*)$ are an adjoint pair of functors. 
\end{exercise}



\subsection{Representations Associated to Local Systems}\label{S:locsys-to-repn}
Let $X$ be a topological space which is connected and locally path connected so that $\pi_1(X,x_0)$ makes sense. 
Let $L$ be a local system on $X$ and let $\gamma:[0,1] \to X$ be a continuous path contained in $X$. 
Then $\gamma^*L$ is a local system on $[0,1]$ which is trivial.
In seeing this, Exercise~\ref{EX:locally-constant} is important.
Furthermore every germ $v \in (\pi^*L)_0$ extends uniquely to $(\gamma^*L)([0,1])$ via monodromy.
In particular there is a morphism 
$$ M_{\gamma}: (\gamma^*L)_0 \to (\gamma^*L)_1.$$
Since $(\gamma^*L)_t \cong L_{\gamma(t)}$ for each $t$ if $\gamma$ is a closed path starting and ending at $x_0\in X$ then $(\gamma^*L)_0 \cong L_{\gamma(0)} \cong L_{x_0} \cong L_{\gamma(1)} \cong (\gamma^*L)_1$.
This then means that $M_{\gamma}$ induces an automorphism of $L_{x_0}$.
This defines the representation associated to the pair $(L,x_0)$ consisting of a local system $L$ and a point $x_0 \in X$
\begin{definition}
	The \emph{monodromy representation} associated to the $(L,x_0)$ is 
	$$\rho_{(L,x_0)}: \pi_1(X,x_0) \to \Aut(L_{x_0}), \quad \gamma \mapsto M_{\gamma}, $$
	as described above. 
\end{definition}

\subsection{Representation from Local System = Monodromy Representation}
Recall that the category of $K$-representations of a group $\Pi$ is the category of $K[\Pi]$-modules and when we talk about two representations being isomorphic we talk about them being isomorphic as $K[\Pi]$-modules. 
We are interested in the case when $\Pi = \pi_1(U,t_0)$ for some $U\subset \CC$. 

\begin{theorem}
	Consider a holomorphic system on $U\subset \CC$ of the form 
	$$ Y' = A(t) Y, \quad A(t) \in M_n(\hol(U)). $$
	Let $L_A$ be the local system associated to this system. 
	Let $\rho_A$ be the monodromy representation associated to the fundamental matrix $\Phi(t)$ satisfying $\Phi(t_0)=I_n$. 
	Then $\rho_{L_A,t_0} \cong \rho_A$ as reprentations of $\pi_1(U,t_0)$.
\end{theorem}
\begin{proof}
	Our aim is to compute $\rho_{L_A}$ (we will drop the base point $t_0$ from the notation for convenience). 
	At a point $t_1\in U$ along a curve $\gamma_1$ starting at $t_0$ and ending at $t_1$ we have 
	$$ \Phi_{\gamma_1} \CC^n \cong L_{A,t_1},$$
	where $\Phi_{\gamma} \in \GL_n(\CC\langle t-t_1\rangle)$ is the local fundamental matrix. 
	The representation associated to the local system gives 
	$$\begin{tikzcd}
	\CC^n & \arrow[l, "\Phi ^{-1}"] \Phi \CC^n \arrow[r] & \Phi_{\gamma} \CC^n \arrow[r, "\Phi_{\gamma}^{-1}"] & \CC^n  \\
	& (\gamma^*L_A)_0 \ar[u, equal] & (\gamma^*L_A)_1 \ar[u, equal]& 
	\end{tikzcd}
	$$
	Here $v= \Phi c \mapsto \Phi_{\gamma} c = \Phi M_{\gamma} c$ implies that $\Phi M_{\gamma} \Phi^{-1}$ is the action on the stalk. 
	In triviallized coordinates we have 
	$$ v \mapsto \Phi v \mapsto \Phi M_{\gamma} \Phi^{-1} \Phi v \mapsto \Phi_{\gamma}^{-1} M_{\gamma} v \mapsto M_{\gamma}^{-1} \Phi M_{\gamma} v.$$
\end{proof}

\subsection{Monodromy Representations in Higher Dimension}
In higher dimensions this is how we are going to define the representation associated to a connection. 
That is to every vector bundle $E$ with integrable connection $\nabla^2=0$ we have the associated local system $E^{\nabla}$ defined by 
 $$ E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s)=0 \rbrace, \quad U \subset X \mbox{ open }. $$
The very definition of integrability is exactly so that the system of equations $\nabla(s)=0$ has a local basis of solutions. 
More precisely if $t^1,\ldots,t^m$ are local coordinates for $X$ and $s_1,\ldots,s_n$ are local basis for $E$ then 
 $$ \nabla(s)=0 \quad \iff  \quad \nabla_{\partial_j}(Y) = \dfrac{\partial Y}{\partial t^j} + A_j Y =0, \quad 1\leq j \leq m,$$
where $Y\in \Ocal_X(U)^{\oplus n}$ is a presentation of $s \in E(U)$ under the trivialization given by the local basis and $A_jdt^j$ is the connection 1-form in local coordinates.

This describes the functors
 $$ \Conn(X) \to \LocSys(X) \to \Repn(X).$$
The connection, goes to a local set of solutions (a local system), which by \S~\ref{S:locsys-to-repn} gives rise to a representation. 
Spoiler: for holomorphic vector bundles this will be an equivalence of categories. 
The map $\LocSys(X) \to \Repn(X)$ is always going to be an equivalence of categories. 


\subsection{$\LocSys(X) \cong \Repn(X)$}\label{S:locsys-repn}
In this section we show that for a general topological space the category of representations is equivalent to the category of local systems. 

First some notation: let $\Pi$ be a group. 
We will let $\Mod^{\fin}_{\CC[\Pi]}$ denote the category of $\CC[\Pi]$-modules which are finite dimensional as $\CC$-vector spaces. 
We will often conflate a representation $\rho: \Pi \to \GL(V)$ with its underlying  $\CC[\Pi]$-module, which we also denote by $V$.  

Now we give our theorem.
\begin{theorem}
	Let $X$ be a topological space which admits a fundamental group and let $\Pi = \pi_1(X,x_0)$ for $x_0\in \CC$. 
	The category of local systems on $X$ is equivalent to the category of finite dimensional complex representations of the fundamental group 
	$$ \LocSys(X) \xrightarrow{\sim} \Repn(X), \quad L \mapsto \rho_{L,x_0},$$
	Here $\Repn(X)=\Mod_{\CC[\Pi]}^{\fin}$ of finite dimensional complex representations of the fundamental group (= $\CC[\Pi]$-modules).
\end{theorem}
\begin{proof}[Proof Sketch]
	The quasi-inverse is given by the so-called suspension construction. 
	Let $\Pi = \pi_1(X,x_0)$
	Let $\rho: \Pi \to \GL(V)$ be a finite dimensional complex representation of $\Pi$.
	Let $f: \widetilde{X} \to X$ be the universal cover of $X$. 
	Note that we have an action of $\Pi$ on both $\widetilde{X}$ and on $V$ and hence on the complex manifold $\widetilde{X}\times V$ where the action is given by 
	$$(\gamma, (\widetilde{x},v))\to (\gamma(\widetilde{x}), \rho(\gamma)(v)).$$
	Note that since the action by deck transformations preserves fibers of $f$ we have that $\Pi$ also acts on $f^{-1}(U) \times V$ for every open subset $U \subset X$. 
	
	Define the constant sheaf $ \widetilde{L} = \underline{V}_{\widetilde{X}}.$
	We claim the total space of this constant sheaf is $\widetilde{X}\times V$ provided we give $V$ the discrete topology; that is, for every $\widetilde{U} \subset \widetilde{X}$ there is a bijection (see exercise~\ref{EX:total-space})
	\begin{equation}\label{E:total-space}
	\widetilde{L}(\widetilde{U})  \cong \lbrace \mbox{sections of $\pi_1: \widetilde{X}\times V \to \widetilde{X}$ above $\widetilde{U}$ } \rbrace.
	\end{equation}
	Hence there is an action of  $\Pi$ on $(f_*\widetilde{L})(U)$ for every $U$ subset $X$ and it makes sense to define $L(U)$ by the formula 
	$$ L(U) = (f_*\widetilde{L})(U)^{\Pi}. $$
	We claim that this is the local system with the corresponding monodromy represenation.
\end{proof}

\begin{exercise}\label{EX:total-space}
	\taylor{I need to check this.}
	In this exercise $M$ will be a topological space where fundamental groups make sense.\footnote{path connected, locally path connected}. 
	We will let $\Pi = \pi_1(M,x_0)$ for $x_0\in M$ some base point.  	
 Check \eqref{E:total-space} regarding the description of the total space of the local system.
\end{exercise}

%%%%%%%%%%%%%%%%%%%
\section{Holomorphic Riemann Hilbert Correspondence}
%%%%%%%%%%%%%%%%%%%
Let $X$ be a complex manifold. 
We have three categories.
\begin{align*}
\Conn(X) =& \mbox{ (holomorphic vector bundles on $X$ with integrable connections)} \\
\LocSys(X) =& \mbox{ (local systems on $X$) } \\
\Repn(X) =& \mbox{ (finite dimensional complex representations of $\pi_1(X,x_0)$ )}
\end{align*}
The holomorphic Riemann-Hilbert correspondence states that these categories are all equivalent. 
To estabilish this we will show $\Conn(X) \cong \LocSys(X)$ and that $\LocSys(X) \cong \Repn(X)$. 
In fact this section part was already established for general topological spaces in \S\ref{S:locsys-repn}.

The functor 
$$\Conn(X) \to \LocSys(X), \quad (E,\nabla) \mapsto E^{\nabla}$$ was already described.
Here $E^{\nabla}$ is ``the space of horizontal sections''. 
It doesn't hurt to repeat that 
$$ E^{\nabla}(U) = \lbrace s \in E(U) \colon \nabla(s) =0 \rbrace, \quad U\subset X \mbox{ open }.$$
The key condition here is integrability, which, \emph{by definition} is so that we have equatlity of mixed partials in the system of equations 
$$ \nabla(s)=0 \quad \iff  \quad \nabla_{\partial_j}(Y) = \dfrac{\partial Y}{\partial t^j} + A_j Y =0, \quad 1\leq j \leq m,$$
where $t^1,\ldots,t^m$ are local coordinates for $X$ and $s_1,\ldots,s_n$ are a local basis for $E$ giving rise to $s \mapsto Y$. 

The converse construction is as follows.
$$ \LocSys(X) \to \Conn(X), \quad L \mapsto (\Ocal_X\otimes_{\CC_X} L, \nabla_L),$$
where $\nabla_L(f\otimes v) = df \otimes v$ for $f\otimes v \in \Ocal_X \otimes L$, and then the definition is given by extending linearly.

Readers can check as much detail as they want that this construction is a quasi-inverse of $(E,\nabla)\mapsto E^{\nabla}$. 
The main idea here is that a basis of solutions always gives a matrix $\Phi$ which makes the connection equivalent to the trivial connection (see exercise~\ref{EX:equivalence-to-trivial}).

One of the major points here is that a connection is determined by its space of horizontal solutions. 
This allows us to define connections by declaring what their horizontal space is. 
When studying connections in characteristic $p$ on schemes we can define a canonical connection of the pullback of a vector bundle by the Frobenius $\nabla^{\can}$ by declaring the inverse image under the Frobenius to be horizontal.

\begin{theorem}[Holomorphic Riemann-Hilbert]
	We have the following equivalences of categories for $X$ a complex manifold:
	$$ \Conn(X) \cong \LocSys(X) \cong \Repn(X).$$
\end{theorem}

Later we will modify the category $\Conn(X)$ to get more information about the differential equations at the singularities. 


\chapter{Isomonodromic Deformations}

The Painlev\'e VI equation takes the form
 \begin{align*}\frac{d^2y}{dt^2}&=
 \frac{1}{2}\left(\frac{1}{y}+\frac{1}{y-1}+\frac{1}{y-t}\right)\left( \frac{dy}{dt} \right)^2
 -\left(\frac{1}{t}+\frac{1}{t-1}+\frac{1}{y-t}\right)\frac{dy}{dt} \\&\quad +
 \frac{y(y-1)(y-t)}{t^2(t-1)^2}
 \left(\alpha+\beta\frac{t}{y^2}+\gamma\frac{t-1}{(y-1)^2}+\delta\frac{t(t-1)}{(y-t)^2}\right),\\ 
 \end{align*}
where $\alpha,\beta,\gamma,\delta \in \CC$ are constants.
The aim of this section is to show that this equation is really about isomonodromic deformations of rank two Fuchsian equations with trace free entries on $\PP^1\setminus \lbrace 0,1,\infty, x\rbrace$ where $x$ is a variable point.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Schlesinger's Equations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider the case of a Fuchsian differential equation of rank 2 on $\PP^1$ with singular points at $\lbrace 0,1,\infty,x\rbrace$ for some variable $x\in \PP^1$ not equal to $0,1$ or $\infty$,
$$\begin{cases}
	  \dfrac{dY}{dt} = A(t,x) Y \\
	  A(t,x) = \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x} 
\end{cases}$$
and define $A_{\infty}(x)$ by the equation $A_0+A_1+A_2 +A_{\infty}=0$.
The matrices $A_j(x)$ we will assume depend holomorphically on the variable $x$ in some unspecified domain (we think of $x$ as varying a little bit around some $x_0$).
We will often write $A_j=A_j(x)$ for short.
We note that for a fixed $x=x_0$ the Fuchsian differential equation gives a monodromy representation. 
Also, as we vary $x\in \PP^1\lbrace 0,1,\infty\rbrace$ the fundamental group does not change (see Figure~\ref{F:moving-point}).
\begin{figure}[h]\label{F:moving-point}
	\begin{center}
		\includegraphics[scale=0.8]{moving-point.eps}
	\end{center}
\caption{Varying $x$ in $\PP^1\setminus\lbrace 0,1,\infty,x\rbrace$ does not change the fundamental group.}
\end{figure}
In what follows we will let 
 $$ \Pi=\langle \gamma_0,\gamma_1,\gamma_2 \colon \gamma_0\gamma_1\gamma_2=1\rangle.$$
For $x_0$ and $x_1$ in $\PP^1$ with $x_0\neq x_1$ we have 
 $$\Pi\cong \pi_1( \PP^1\setminus\lbrace 0,1,\infty,x_0\rbrace,t_0) \cong \pi_1(\PP^1\setminus\lbrace 0,1,\infty,x_1\rbrace,t_0).$$
Now fix $x=x_0$ and a fundamental matrix $\Phi_0(t)$ for our system at $x=x_0$ so that $\Phi_0'(t) = A(t,x_0)\Phi_0(t)$. 
This gives a monodromy representation 
$$ \rho_0\colon \Pi \to \GL_2(\CC), \quad \gamma \mapsto M_{\gamma},$$
where $M_{\gamma} \in \GL_2(\CC)$ is the matrix so that $(\Phi_0(t))_{\gamma} = \Phi_0(t) M_{\gamma}$.

\begin{problem}[Isomonodromic Deformations]
	Find a parameter space $X$ and a function 
	$$X\mapsto M_2(\CC)^3, \quad x\mapsto (A_0(x),A_1(x),A_2(x))$$
	so that the fundamental matrix $\Phi(x,t)$ of the system 
	$$ \dfrac{dY}{dt}(x,t) = \left( \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x} \right)Y(x,t) $$
	satisfies
	\begin{enumerate}
		\item (Monodromy at $x_0$) $\Phi(t,x_0) = \Phi_0(t)$ (so that $M_{\gamma}$ define the monodromy representation at that point).
		\item (Isomonodromy) For each $x\in X$, and all $\gamma \in \Pi$, $$\Phi(t,x)_{\gamma} = \Phi(t,x)M_{\gamma},$$
		up to conjugation of the collection of $M_{\gamma}$.
	\end{enumerate}
\end{problem}
 If a function $x\to A(x)$ satisfies the isomonodromy problem we call it an \emph{isomonodromic deformation} of the system at $x=x_0$.

\subsection{Schlesinger's Theorem}
In this section we are going to give some equations that determine when 
\begin{equation}
x\mapsto A(x,t) = \dfrac{A_0(x)}{t} +\dfrac{A_1(x)}{t-1} + \dfrac{A_2(x)}{t-x}.
\end{equation}
gives an isomonodromic deformation.
We will make some simplifying assumptions. 
\begin{enumerate}
	\item Assume that $A_{\infty}(x)$ is a constant diagonal matrix.
	\item (Non-resonance) Assume that for each $x$ the eigenvalues of $A_j(x)$ for $j=0,1,\infty$ do not differ by an integer. 
\end{enumerate}
Under the conditions above we can give equations.
Both of these conditations are used in the secret weapon of this theorem: the logarithmic Riemann-Hilbert correspondence. 
\begin{theorem}
	Assume the assumptions and work in the notaiton of this section.
	The map $x\mapsto A(x)$ is isomonodromic if and only if 
	$$\dfrac{\partial A_0}{\partial x} = \dfrac{[A_0,A_2]}{x}, \quad \dfrac{\partial A_1}{\partial x} = \dfrac{[A_1,A_2]}{x-1}, \quad \dfrac{\partial A_{\infty}}{\partial x}=0.$$
\end{theorem}
The above Theorem is a special case of a more general formula we will give. 
It turns out these equations are equivalent to integrability conditions for a connection on a certain space. 
To state the connection to connections we will change our setup slightly. 
We will work with a rank $n$ system on $\PP^1$ and allow it to have singularities at $m$-points (and for simplicity we will exclusing $\infty$) we will let $X$ be a parameter space of points
 $$ X = \lbrace (x_1,\ldots, x_m) \in \CC^m \colon x_i \neq x_j, \mbox{ for } i\neq j \rbrace .$$
We will use the notation $x=(x_1,\ldots,x_m)$
\begin{theorem}[Schlesinger's Equations]
	Consider the system, 
	 $$ \dfrac{\partial Y}{
	 \partial t}(t,x) = \sum_{j=1}^m \dfrac{A_j(x)}{t-x_j} Y(t,x)$$
 	on $\PP^1\times X$.
 	This system is isomonodromic if and only if 
 	\begin{equation}\label{E:schlesinger}
 	\begin{cases}
 	 \dfrac{\partial A_j}{\partial x_i} = \dfrac{[A_j,A_i]}{x_j-x_i}, &  i \neq j,\\ 
 	  \dfrac{\partial A_j}{\partial x_j} = -\sum_{i\neq j}\dfrac{[A_i,A_j]}{x_i-x_j}.
 	  \end{cases}.
 	 \end{equation}
\end{theorem}
In these notes we will call \eqref{E:schlesinger} \emph{Schlesinger's equations}.
The commutators in the Schlesinger's equation should make the reader suspect that these equations are possibly an integrability/curvature condition on some connection. 
This is indeed the case and we will actually have a log connection on the space $X\times \PP^1$.

\begin{figure}[h]\label{F:connection-space}
	\begin{center}
		\includegraphics[scale=0.75]{connection-space.eps}
	\end{center}
\caption{A picture of the $X\times \PP^1$ and the associated divisor $D$ in the case of two points and a single singular point $x$ varying.}
\end{figure}

\begin{theorem}
	The following are equivalent for a collection of matrix valued functions $A_j(x)$ on $X$.
	\begin{enumerate}
		\item The connection $\nabla=d+\omega$ on $\Ocal_{\PP^1\times X}$ given by 
		 $$ \omega = \sum_{j=1}^m A_j(x) \dfrac{d(t-x_j)}{t-x_j},$$
		 is integrable.
		 \item\label{I:differential-schlesinger} The matrices $A_j$ satisfy
		  $$ dA_j - \sum_{i\neq j} [A_i,A_j] \dfrac{d(x_i-x_j)}{x_i-x_j} = 0, \quad 1\leq j \leq m.$$
		  \item Schlesinger's equations \eqref{E:schlesinger} hold.
	\end{enumerate}
\end{theorem}
\begin{proof}
	 Also \eqref{I:differential-schlesinger} are equivalent to Schlesinger's equations.
	
	The computation of the equivalent between \eqref{I:differential-schlesinger} and the integrability condition can be rough if you try to do too much or move to Einstein notation.
	Also, we should observe that Schlesinger's equation are the literal equations one obtains from integrability but can be derived from the integrability conditions.
	
	The key is to not manipulate your equations too much and think about what you are doing.
	Integrability of the connection is equivalent to (see Theorem~\ref{T:integrability-conditions}) $$d(\omega)+\omega\wedge \omega=0.$$ 
	We compute.
	 $$d(\omega) = \sum_{i=1}^m \frac{dA_i}{t-x_i}\wedge dt - \sum_{i=1}^m \frac{dA_i \wedge dx_i}{t-x_i},$$
	 $$\omega \wedge \omega = \sum_{i,j} \frac{[A_i,A_j]}{t-x_j} \frac{dt\wedge d(x_i-x_j) + dx_i \wedge dx_j}{x_i-x_j}.$$
	 Collecting the terms with $dt$ in them one finds
	 \begin{equation}\label{E:take-residue}
	  \sum_{i} \frac{dA_i}{t-x_i} = \sum_{i,j} \frac{[A_i,A_j]}{t-x_i}\frac{d(x_i-x_j)}{x_i-x_j}
	 \end{equation}
	 The remaining terms give
	 \begin{equation}\label{E:dont-take-residue}
	  \dfrac{dA_i \wedge dx_i}{(t-x_i)} = \sum_{i,j} \frac{[A_i,A_j]}{t-x_i}\frac{dx_i\wedge dx_j}{x_i-x_j}.
	 \end{equation}
	 Taking residues of \eqref{E:take-residue} along $t=x_k$ we find that 
	  $$ dA_k = \sum_{j\neq k} [A_k,A_j] \frac{d(x_k-x_j)}{x_k-x_j}=0.$$
	 This proves \eqref{I:differential-schlesinger}.
	 
	 To see the converse observe that multiplying \eqref{I:differential-schlesinger} by $\frac{1}{t-x_j}$ and summing over $j$ gives \eqref{E:take-residue}.
	 To recover \eqref{E:dont-take-residue} we apply $-\wedge \dfrac{dx_i}{t-x_i}$ to \eqref{I:differential-schlesinger} and sum over $i$.
\end{proof}


One can be a little more general with the points if one likes.  
The following is a description of 

\begin{figure}[h]\label{F:plot-of-s}
	\begin{center}
		\includegraphics[scale=0.75]{plot-of-s.eps}
	\end{center}
\caption{A picture of $S \subset \PP^1\times X$ in the case of four parametrized points.}
\end{figure}

\subsection{The Geometry of the Connection}

\taylor{I need to finish this}

In \cite[VI, \S1, pg 192]{Sabbah2007} Sabbah works with an arbitrary complex manifold $X$ some $S \subset X\times \PP^1$ such that the map $f:S \to X$ induced by the projection onto $X$ has degree $m$. 
This means that over $\mu \in X$ the we have a fiber $S_{\mu}$
$$S_{\mu} = f^{-1}(\mu) = \lbrace (\mu,x_1(\mu)) (\mu,x_2(\mu)),\ldots,(\mu,x_m(\mu))\rbrace.$$ 
By assumptions on $X$ (1-connectedness) the manifold $S$ is trivializable: there exists maps $x_j: X \to \PP^1$ such that $S$ is the union of the graphs of these functions:
 $$ S=S_1 \cup \cdots \cup S_m $$
where $S_j$ is the graph of the map $x_j$.
In what we were doing above the maps $x_j$ were the projections from $X\subset (\PP^1)^m$ onto $\PP^1$.

Note now that the fiber of $(X\times \PP^1)\setminus S$ above $x=\mu\in X$ is $\PP^1\setminus S_{\mu} \cong (\PP^1\setminus S_{\mu}) \times \lbrace \mu \rbrace$.
If we assume that $H_2(X,\ZZ)=0$ or that $\pi_2(X)=0$ then one can use the long exact sequence in homotopy groups to conclude that the inclusion of the fiber 
 $$ \PP^1\setminus S_{\mu} \hookrightarrow (\PP^1\times X)\setminus S $$
induces an isomorphism of fundamental groups.
\begin{lemma}
	
\end{lemma}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Poincar\'{e}-Fuchs]{The Poincar\'{e}-Fuchs Theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
Which ordinary differential equations of the form
	  $$ P(t,y,y') =0, $$
with $P(u,v,w) \in \CC[x,y,z]$ admit meromorphic solutions $y(t)$ which have the property that $y(t)$ has no-movable singularities? 
\end{problem}
The Poincar\'e-Fuchs theorem states that the only equations of this form are 1) Ricatti equations and 2) Weierstrass equations. 
We are going to give two proofs of this. 
The first is foliation theoretic and the second is differential algebraic. 

A basic reference for the foliation theoretic proof is Pan and Sebastian \cite{Pan2004}.
There is also a great \href{https://www.youtube.com/watch?v=DoCCm8zjvXQ&list=PL0E0n75oNCDk5tuV-t2_K56sEfLd0Od8H&index=21}{YouTube Lecture} by Loray on the topic. 

A basic reference for the differential algebraic proof is Matsuda's book \cite{Matsuda1980} and is based of his paper \cite{Matsuda1978}.
I will follow the treatment from Buium's book \cite{Buium1986} where he further develops this theory in higher dimensions.

Both \cite{Pan2004} and \cite{Matsuda1978} give with the stated goal of making Poincar\'e's original proof rigorous. 
See \cite{Matsuda1978} for details.
%There is a recurring theme of mistakes in differential algebra bring corrected. Painlev\'e Gambier, Poincare Matsuda/Pain, Manin/Coleman/Chai,  


%H. Poincar√©, C.R. Acad. Sci. Paris 99 (1885), 75‚Äì77; JFM 16.0250.01] and [H. Poincar√©, Acta Math. 7 (1885), 1‚Äì32; JFM 17.0279.01]



%\section{Differential Equations from the Foliation-Theoretic Perspective}

%\subsection{Exterior Differential Systems}

%\subsection{Comparison to $\Delta$-Schemes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Differential Galois Theory]{Differential Galois Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
	Can the function $\int e^{x^2} dx$ be written in terms of elementary functions?
\end{problem}
In order to address this we need to talk about what an elementary function is. 
We also need to talk about what a special function is. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Poincar\'e Problem]{The Poincar\'e Problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
	Consider differential equations of the form
	 $$ \dfrac{dy}{dx} = \frac{a(x,y)}{b(x,y)}, \quad a(x,y),b(x,y) \in \CC[x,y].$$
	 For which $a(x,y),b(x,y) \in \CC[x,y]$ does the equation admit algebraic solutions?
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Dimension Theory]{Dimension Theory}\label{S:dimension-theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section is motivated by the following problem.
\begin{problem}
	Let $(K,\Delta)$ be a $\Delta$-field. 
	Let $u_1,\ldots,u_n \in K\lbrace x_1,\ldots,x_n\rbrace_{\Delta}$ and consider the system of PDEs
	 $$ u_1=u_2=\cdots=u_n=0. $$
	How many constants of integration does a general solution of this equation need?
\end{problem}

\appendix 
\chapter[Analytic Elements]{Analytics Elements of Differential Equations}

\section{Solutions of Homogeneous Differential Equations}
The following is a useful formula.
\begin{theorem}[Solutions of Homogeneous Differential Equations]\label{T:solutions}
	If $Y'= A(t)Y$ is an ordinary differential equation then 
	 $$ \Phi(t) = \exp( \int_{t_0}^t A(s)^T ds )^T $$
	provides a local fundamental matrix. 
\end{theorem}



\section{Cauchy-Kovalevskya (Cauchy-Kowalevski)}

\backmatter



\bibliographystyle{amsalpha}
\bibliography{diff-alg.bib}

\end{document}
